# Почему появился Kubernetes

**Kubernetes** – это платформа с открытым исходным кодом для размещения контейнеров и определения прикладных API для управления облачной семантикой обеспечения этих контейнеров хранилищами данных, сетевыми услугами, поддержкой безопасности и другими ре- сурсами. Kubernetes обеспечивает непрерывную синхронизацию все- го пространства состояний ваших приложений, в том числе способов доступа к ним из внешнего мира.

**Основыне теримины:**
- CNI (Container Networking Interface) и CSI (Container Storage Inter- face) – сетевой интерфейс контейнеров и интерфейс хранилища для контейнеров соответственно; позволяют подключать к се- тям и хранилищам модули Pod (с контейнерами), работающие в Kubernetes;
- контейнер (Container) – образ Docker или OCI (Open Container Ini- tiative), который обычно запускает приложение;
- плоскость управления (Control plane) – мозг кластера Kubernetes, осуществляющий планирование контейнеров и управляющий всеми объектами Kubernetes (которые иногда называют мастер- объектами);
- набор демонов (DaemonSet) – аналог развертывания (Deployment), но выполняется на каждом узле кластера;
- развертывание (Deployment) – набор модулей, которыми управ- ляет Kubernetes;
- kubectl – инструмент командной строки для взаимодействия с панелью управления Kubernetes;
- kubelet – агент Kubernetes, работающий на узлах кластера. Обе- спечивает поддержку плоскости управления;
- узел (Node) – машина, на которой запущен процесс kubelet;
- OCI (Open Container Initiative) – общий формат образа для созда- ния выполняемых автономных приложений. Также называется образами Docker;
- Pod (модуль) – объект Kubernetes, инкапсулирующий действую- щий контейнер.

Kubernetes дает возможность централизовать управление пространством состояний всех приложений с использованием одного удобного инструмента: kubectl – клиента командной строки, выполняющего вызовы REST API к серверу Kubernetes API.

Kubernetes использует привилегированный контейнер в среде Linux, он может управлять правилами iptables для организации маршрутизации трафика к приложениям, что, собственно, и делает прокси-сервер Kubernetes Service - kube-proxy. Контейнеры оказываются фундаментальным примитивом и для запуска приложений, и для управления инфраструктурой, которые запускают сервисы, необходимые приложениям (специализированные хранилища или брандмауэры с определенными настройками), и, что особенно важно, сами приложения. Kubernetes практически бесспорно считается современным стандартом для организации и запуска контейнеров в любом облачном окружении, на сервере или в центре обработки данных.

### Контейнеры и образы

Docker можно рассматривать как способ запуска контейнеров, где контейнер – это работающий образ OCI. Спецификация OCI – это стандартный способ определения образа, который может быть запущен такой программой, как Docker, и в конечном счете представляет собой архив с различными слоями. Контейнеры добавляют слой изоляции, устраняющий необходимость управления библиотеками на сервере или предварительной загрузки инфраструктуры другими зависимостями приложений. Использование контейнеров немыслимо без автоматизации, и именно этой цели служит Kubernetes.

### Базовая основа Kubernetes

Все сущее в Kubernetes определяется в виде простых текстовых файлов в формате YAML или JSON, и платформа запускает образы OCI декларативным способом. Kubernetes позволяет определить желаемое состоя- ние всех приложений в кластере, их подключение к сети, место работы, используемое хранилище и т. д., делегируя базовую реализацию этих деталей самой платформе Kubernetes.

Традиционные правила инфраструктуры:
- конфигурация портов или IP-маршрутов;
- постоянная доступность хранилища для приложений;
- размещение программного обеспечения на определенных или произвольных серверах;
- обеспечение безопасного доступа приложений друг к другу с использованием, например, RBAC или сетевых правил;
- конфигурация DNS для каждого приложения и глобально.

Все эти компоненты определяются в конфигурационных файлах, представляющих объекты в Kubernetes API. Kubernetes использует эти стандартные блоки и контейнеры, применяет изменения, отслеживает эти изменения и устраняет сбои или нарушения, пока не будет достигнуто желаемое конечное состояние.

### Возможности Kubernetes

Платформы оркестрации контейнеров позволяют разработчикам автоматизировать процесс запуска экземпляров, подготовки хостов, связывания контейнеров для оптимизации процедур оркестрации и продления жизненных циклов приложений. 

Основные возможности платформы оркестрации контейнеров:
- предоставления доступа, не зависящего от используемой облачной технологии, ко всем возможностям сервера API;
- интеграции со всеми основными облачными платформами и гипервизорами в диспетчере контроллеров Kubernetes (Kubernetes Controller Manager, KCM);
- обеспечения отказоустойчивости для хранения и определения состояния всех сервисов, приложений и конфигураций центров обработки данных или других инфраструктур, поддерживаемых Kubernetes;
- управления развертыванием, чтобы минимизировать время простоя отдельных узлов, сервисов или приложений;
- автоматизации масштабирования хостов и приложений с поддержкой постоянного обновления;
- создания внутренних и внешних соединений (известных как типы ClusterIP, NodePort или LoadBalancer Service) с балансировкой нагрузки;
- предоставления возможности планирования запуска приложе- ний на определенном виртуализированном оборудовании на основе его метаданных с помощью маркировки узлов и планировщика Kubernetes;
- обеспечения высокой доступности с помощью DaemonSets и других технологических инфраструктур, в которых приоритет
отдается контейнерам, работающим на всех узлах кластера;
- обнаружения сервисов через службу доменных имен, ранее реализованную как KubeDNS, а совсем недавно – CoreDNS, которая интегрируется с сервером API;
- запуска пакетных процессов (известных как задания), которые используют хранилище и контейнеры так же, как обычные приложения;
- расширения API и создания собственных программ, управляемых API, с помощью пользовательских определений ресурсов и без создания каких-либо сопоставлений портов или подключений;
- проверки сбойных процессов на уровне кластера, включая удаленное выполнение в любом контейнере в любое время с помощью kubectl exec и kubectl describe;
- подключения локального и/или удаленного хранилища к контейнеру и декларативного управления томами хранилища с помощью StorageClass API и PersistentVolumes.

Если вам не нужны высокая доступность, масштабируемость и оркестрация, то, возможно, вам не нужна Kubernetes.

### Компоненты и архитектура Kubernetes
- аппаратная инфраструктура – включает компьютеры, сетевую инфраструктуру, инфраструктуру хранения и реестр контейнеров;
- рабочие узлы Kubernetes – базовая вычислительная единица в кластере Kubernetes;
- плоскость управления Kubernetes – основа Kubernetes. Она включает сервер API, планировщика, диспетчера контроллеров и другие контроллеры.

### Kubernetes API

Администрирование микросервисов и других контейнерных приложений на платформе Kubernetes сводится к объявлению объектов Kubernetes API. Сервер Kubernetes API – kube-apiserver – позволяет выполнять CRUD-операции (Create, Read, Update, Delete – создавать, читать, обновлять, удалять) со всеми объектами и предоставляет интерфейс передачи репрезентативного состояния RESTful (REpresentational State Transfer).

Все объекты Kubernetes API имеют:
- именованную версию API (например, v1 или rbac.authorization. k8s.io/v1);
- тип (например, kind: Deployment);
- раздел метаданных.

Существует около 70 различных типов API, просмотреть их командой **kubectl api-resources**.

**Когда не стоит использовать Kubernetes**
- высокопроизводительные вычисления (High-Performance Comput- ing, HPC) – контейнеры добавляют дополнительные сложности, а наличие нового уровня бьет по производительности.
- унаследованные приложения – некоторые приложения имеют требования к оборудованию, программному обеспечению и задержке, что затрудняет их контейнеризацию.
- миграция – реализации унаследованных систем могут быть на- столько жесткими, что их миграция в Kubernetes не дает особых преимуществ

# Зачем нужны модули Pod?

Pod – это наименьшая атомарная единица, которую можно развернуть в кластере Kubernetes. Определение Pod описывает модуль, способный включать несколько контейнеров, что позволяет Kubernetes создать несколько контейнеров на узле. Многие другие объекты Kubernetes API либо используют Pod напрямую, либо являются объектами API, поддерживающими Pod. Некоторые контроллеры Kubernetes высокого уровня запускают модули Pod и управляют ими.

### Что такое Pod?

Pod – это модуль, содержащий один или несколько образов OCI, которые выполняются в контейнерах на одном узле кластера Kubernetes. Узел Kubernetes – это отдельная единица вычислительной инфраструктуры (сервер), на которой выполняется kubelet.
```yml
apiVersion: v1
kind: Pod
metadata:
spec:
  container:
    - name: busybox
      image: mycontainerregistry.io/foo
```
**kubectl create -f pod.yaml**

Команда kubectl – это двоичный выполняемый файл, реализующий интерфейс командной строки к серверу Kubernetes API.

**kubectl get po** - посмотреть Pod

### Пространства имен в Linux

Пространства имен Linux – это функция ядра Linux, позволяющая разделять процессы. Pod имеет следующие пространства имен Linux:
- одно или несколько пространств имен PID;
- единое сетевое пространство имен;
- пространство имен IPC;
- пространство имен cgroup (управляющая группа);
- пространство имен mnt (монтирование);
- пространство имен user (ID пользователя).

Пространства имен Linux – это компоненты файловой системы ядра Linux, обеспечивающие базовую функциональность для получения образа и создания работающего контейнера.

### Kubernetes, инфраструктура и Pod

Единица вычислительной мощности в Kubernetes представлена объектом узла Node. Некоторые требования к узлу:
-  сервер;
-  установленная операционная система (ОС), Linux или Windows, с необходимыми зависимостями;
- systemd (диспетчер системных служб, в Linux);
- kubelet (агент узла);
- среда выполнения контейнеров (например, Docker);
- сетевой прокси-сервер (kube-proxy), обслуживающий сервисы Kubernetes;
- провайдер сетевого интерфейса контейнеров (Container Network Interface, CNI).

**kubelet** – это двоичная программа, играющая роль агента и взаимодействующая с сервером Kubernetes API посредством поддержки цикла управления. Она работает на каждом узле; без этой программы узел Kubernetes недоступен планировщику и не может считаться частью кластера.

kubelet гарантирует:
- запуск любых модулей Pod, запланированных для выполнения на данном хосте механизмом управления, который следит за распределением модулей Pod между узлами;
- регулярное уведомление сервера API об исправной работе kubelet отправкой контрольных сообщений (Kubernetes 1.17+) с помощью механизма в пространстве имен kube-node-lease кластера;
- своевременное освобождение ресурсов, выделенных для Pod, включая эфемерные хранилища или сетевые устройства.

**kubelet не может выполнять свои обязанности без провайдера CNI и среды выполнения**, доступной через интерфейс среды выполнения контейнеров (Container Runtime Interface, CRI). CNI обслуживает потребности CRI, который затем запускает и останавливает контейнеры. **kubelet использует CRI и CNI для согласования состояния узла с состоянием плоскости управления**.

**Сервис (Service)** – это объект API, определяемый платформой Kubernetes. Двоичный файл сетевого прокси Kubernetes (kube-proxy) создает на каждом узле сервисы ClusterIP и NodePort. Вот некоторые типы сервисов:
- ClusterIP – внутренний балансировщик, распределяющий нагрузку между модулями Pod в кластере Kubernetes;
- NodePort – открытый порт на узле Kubernetes, распределяющий нагрузку между несколькими модулями Pod;
- LoadBalancer – внешний сервис, создающий балансировщик нагрузки, внешний по отношению к кластеру.

### Объект Node

Узлы поддерживают модули Pod, а плоскость управления определяет группу узлов, на которых работают контроллеры, диспетчер контроллеров и планировщик. Получить список узлов кластера - **kubectl get no**

Объект Node, описывающий узел, где размещена плоскость управления Kubernetes - **kubectl get no kind-control-plane -o yaml**

Модули Pod реализуют возможность развертывания образов. Образы развертываются на узле, а их жизненным циклом управляет kubelet. Объекты сервисов управляются сетевым прокси Kubernetes. Сетевой прокси Kubernetes обеспечивает возможность балансировки нагрузки внутри кластера, а также аварийное переключение, обновление, высокую доступность и масштабирование. Комбинация из пространства имен mnt в Linux, агента kubelet и объекта узла Node позволяет подключить диск к Pod.

### Сервер Kubernetes API: kube-apiserver

Сервер Kubernetes API (kube-apiserver) – это HTTP REST-сервер, экспортирующий различные объекты API для кластера Kubernetes, такие как Pod, Node или HorizontalPodAutoscaler. Сервер API предоставляет веб-интерфейс для выполнения операций CRUD с состоянием кластера. Сервер API – единственный компонент в плоскости управления, взаимодействующий с etcd, базой данных Kubernetes. По сути, сервер API предоставляет интерфейс для всех операций, изменяющих состояние кластера Kubernetes. Сервер API не имеет состояния и может работать на нескольких узлах одновременно. Перед серверами API в плоскости управления, состоящей из нескольких узлов, размещается балансировщик нагрузки HTTPS. В состав сервера API вхо- дят контроллеры доступа, обеспечивающие аутентификацию и авторизацию клиентов. Вызов, отправленный командой kubectl, проходит аутентификацию, а затем сервер API сохраняет новый объект развертывания в etcd. Следующий шаг – уведомление планировщика о необходимости запустить модуль Pod на узле.

### Планировщик Kubernetes: kube-scheduler

**Планировщик Kubernetes (kube-scheduler)** предлагает чистую и простую реализацию планирования, идеально подходящую для такой сложной системы, как Kubernetes. При планировании модулей Pod он учитывает несколько факторов, таких как аппаратная конфигурация узла, доступные вычислительные ресурсы и объем памяти, ограничения политик планирования и др. Планировщик также следует правилам соответствия/несоответствия (affinity/anti-affinity), определяющим порядок планирования и размещения модулей Pod. По сути, правила соответствия определяют силу притяжения модулей Pod к узлам, отвечающим требованиям, тогда как правила несоответствия определяют силу отталкивания. Ограничения (Taint), дополняющие правила, позволяют узлам отклонять определенные типы модулей, а это означает, что планировщик может определить, какие модули и на каких узлах не должны находиться. Планировщик выбирает узлы для размещения реплик, а затем планирует развертывание модулей Pod на них. Жизненным циклом модуля Pod управляет агент kubelet. Он действует как мини-планировщик для узла. Как только планировщик Kubernetes обновит NodeName в определении Pod, kubelet развернет этот модуль на своем узле.

### Контроллеры инфраструктуры

Инфраструктура поддержки контроллеров - Диспетчер контроллеров Kubernetes (Kubernetes Controller Manager, KCM) или компонент kube-controller-manager и облачный диспетчер контроллеров (Cloud Controller Manager, CCM).

**Контроллеры** – это программные компоненты, управляющие жизненным циклом модулей Pod. К ним относятся kubelet, облачный диспетчер контроллеров (CCM) и планировщик.

Объекты API PersistentVolume (PV) и PersistentVolumeClaim (PVC) определяют хранилища и воплощаются в реальность диспетчерами KCM и CCM. Одна из ключевых особенностей Kubernetes – возможность работать на множестве платформ: в облаке, на «голом железе» или на ноутбуке. Однако хранилища и другие компоненты различаются на разных платформах. KCM – это набор циклов управления, запускающих различные компоненты, называемые контроллерами, на узле, находящемся в плоскости управления. Это один двоичный файл, но он управляет несколькими циклами и, соответственно, контроллерами.

При развертывании в облачном окружении платформа Kubernetes напрямую взаимодействует с API общедоступного или частного облака, и большинство соответствующих вызовов API выполняет CCM. Цель этого компонента – запускать контроллеры, специфичные для облака, и выполнять вызовы к API облака. Вот список этих контроллеров:
- контроллер узлов – выполняет тот же код, что и KCM;
- контроллер маршрутизации – настраивает маршруты в используемой облачной инфраструктуре;
- контроллер сервисов – создает, обновляет и удаляет балансировщики нагрузки облачного провайдера;
- контроллер томов – создает, подключает и монтирует тома, а также взаимодействует с облачным провайдером, осуществляя управление томами.

Разрабатываются другие интерфейсы для поддержки более модульного и независимого от провайдеров услуг будущего Kubernetes:
- сетевой интерфейс контейнеров (Container Network Interface, CNI) – предоставляет IP-адреса модулям Pod;
- интерфейс среды выполнения контейнеров (Container Runtime Interface, CRI) – определяет и подключает различные механизмы выполнения контейнеров;
- интерфейс хранилища для контейнеров (Container Storage Interface, CSI) – модульный способ поддержки новых типов хранилищ без необходимости изменять код Kubernetes.

kubelet создает модуль Pod, он определяет нужное хранилище и подключает его к контейнеру через пространство имен mnt Linux. После этого приложение может пользоваться хранилищем. При создании сервиса LoadBalancer вместо ClusterIP облачный провайдер Kubernetes «наблюдает» за запросом балансировщика и выполняет его. Контроллер KCM, выполняя цикл наблюдения, обнаруживает потребность в новом балансировщике нагрузки и выполняет вызовы API, необходимые для создания балансировщика в облаке, или вызывает аппаратный балансировщик нагрузки, внешний по отношению к кластеру Kubernetes.

### Масштабирование, высокодоступные приложения и плоскость управления

Выполняя масштабирование, kubectl может увеличивать и уменьшать количество модулей Pod в кластере. Он работает непосредственно с наборами реплик, состояний и другими объектами API, которые используют модули Pod
```
kubectl scale --replicas 300 deployment zeus-front-end-ui
```
Сбои можно разбить на три основные категории: сбой модуля Pod, сбой узла и сбой обновления программного обеспечения.
- сбой модуля Pod. За жизненный цикл модулей Pod отвечает агент kubelet, который выполняет запуск, остановку и перезапуск модулей. Выход модуля Pod из строя определяется по отсутствию контрольных сообщений от него или по аварийному завершению процесса. В этом случае kubelet пытается перезапустить модуль.
- сбой узла. Один из циклов управления в kubelet постоянно сообщает серверу API об исправности узла. Если узел присылает контрольные сообщения недостаточно часто, то контроллер KCM меняет его статус на «вне сети» и планировщик перестает планировать модули Pod для запуска на этом узле. Модули, действовавшие на узле, планируются для удаления, а затем переносятся на другие узлы.
- сбой в обновлении программного обеспечения. Обновление развертываний осуществляется простым изменением версии образа в определении YAML и обычно выполняется одним из трех способов:
  - kubectl edit – принимает объект Kubernetes API на входе и открывает локальный терминал для редактирования объекта API на месте;
  - kubectl apply – принимает файл на входе и отыскивает объект API, соответствующий этому файлу, автоматически заменяя его;
  - kubectl patch – применяет небольшой файл с исправлениями, определяющий различия для объекта.

### Автоматическое масштабирование

Три формы автоматического масштабирования:
- создание большего количества модулей Pod (горизонтальное автоматическое масштабирование с помощью HorizontalPodAutoscaler);
- предоставление модулям Pod большего количества ресурсов (вертикальное автоматическое масштабирование с помощью VerticalPodAutoscaler);
- создание дополнительных узлов (с помощью ClusterAutoscaler).

### Управление затратам

Kubernetes поддерживает возможность плотного размещения модулей Pod, что позволяет запускать узлы с избыточными ресурсами и высокой плотностью модулей. Плотность Pod контролируется с помощью следующих шагов.
- Масштабируйте и профилируйте свои приложения. Приложения должны быть протестированы и тщательно проверены как на предмет потребления памяти, так и центрального процессора. После профилирования можно правильно определить потребности приложения в ресурсах.
- Выберите размер узла. Это позволит упаковать несколько приложений на одном узле. Запуск виртуальных машин разных размеров или серверов без системного программного обеспечения с разной емкостью позволяет сэкономить деньги и развернуть на них больше модулей Pod. При этом вы должны обеспечить достаточно большое количество узлов для поддержания высокой доступности в соответствии с требованиями SLA.
- Сгруппируйте определенные приложения вместе на определенных узлах. Это обеспечит наибольшую плотность. Ограничения и допуски позволяют шаблону Operator (Оператор) группировать модули и управлять их развертыванием.

Еще фактор, который необходимо учитывать, – это шумные соседи. В зависимости от рабочей нагрузки некоторые из настроек могут оказаться неподходящими. Однако вы можете более равномерно распределить «шумные» приложения в своем кластере Kubernetes, используя определения соответствия/несоответствия (affinity/antiaffinity) модулей Pod.

# Создание модулей Pod

Для создания так называемого контейнера вызывается множество примитивов Linux:
- kubelet выясняет, что должен запустить контейнер;
- затем kubelet (обращением к среде выполнения контейнеров) запускает приостановленный контейнер (так называемый контейнер pause), что дает ОС Linux время на создание сети для контейнера. Этот приостановленный контейнер является предшественником фактического приложения, которое будет запущено. Его цель – создать дом для начальной загрузки нового сетевого процесса контейнера и его идентификатора процесса (PID);

### Что такое примитивы Linux?

Примерами таких примитивов могут служить такие инструменты, как iptables, ls, mount и многие другие базовые программы, доступные в большинстве дистрибутивов Linux. Знание основ этих инструментов дает мощный толчок в понимании множества новых плагинов и надстроек в экосистеме Kubernetes:
- сетевой прокси-сервер kube-proxy создает правила iptables, и эти правила часто приходится проверять для устранения проблем с сетью контейнеров в больших кластерах. Получить эти правила можно, запустив команду iptables -L в узле Kubernetes. Провайдеры Container Network Interface (CNI) тоже используют этот сетевой прокси-сервер
- интерфейс Container Storage Interface (CSI) определяет сокет для взаимодействий kubelet с технологиями хранения. С помощью команды mount можно увидеть смонтированные в кластере контейнеры и тома, управляемые платформой Kubernetes, не привлекая kubectl и другие инструменты, не входящие в стандартную конфигурацию ОС.
- команды среды выполнения контейнеров, такие как unshare и mount, используются при создании изолированных процессов.

Примитивы Linux почти всегда представляют свои операции как операции с каким-либо файлом, потому что все, что вам нужно построить с помощью Kubernetes, изначально создавалось для работы в Linux, а Linux изначально разрабатывалась для использования файловой абстракции в качестве примитива управления.

Важной чертой примитивов Linux: их можно комбинировать в операции более высокого уровня. Используя канал (|), можно получить вывод одной команды и передать его для обработки другой команде. Например, проверка работоспособности etcd внутри кластера. Внутри контейнера на узле, где запущены компоненты плоскости управления Kubernetes, можно выполнить следующую команду:
```
ls /var/log/containers/ | grep etcd
```
Можно узнать, где находятся ресурсы конфигурации, связанные с etcd, выполнив примерно такую команду:
```
find /etc | grep etcd; find /var | grep etcd
```
### Предварительные условия для запуска модуля Pod

Для создания модуля Pod мы полагаемся на возможности изоляции, сетевых взаимодействий и управления процессами. Все это может быть реализовано с помощью утилит, уже доступных в ОС Linux. На самом деле некоторые из этих утилит можно считать обязательными, потому что без них kubelet не сможет выполнить действия, необходимые для запуска модуля Pod. 

Программы (или примитивы), на которые мы полагаемся в повседневной работе с кластерами Kubernetes:
- **swapoff** – команда, отключающая подкачку памяти, что является известным предварительным условием для запуска Kubernetes, позволяющим исключить конкуренцию за процессор и память;
- **iptables** – основное требование (обычно) сетевого прокси-сервера, который создает правила iptables для отправки служебного трафика модулям Pod;
- **mount** – эта команда проецирует ресурс в определенное место в файловой системе;
- **systemd** – эта команда обычно запускает kubelet – основной процесс, управляющий всеми контейнерами в кластере;
- **socat** – эта команда позволяет установить двунаправленную связь между процессами; socat обеспечивает правильное функционирование команды kubectl port-forward;
- **nsenter** – инструмент для входа в различные пространства имен процесса и просмотра происходящего (с точки зрения сети, хранилища или процесса). Пространство имен Linux имеет определенные ресурсы, к которым невозможно обратиться из внешнего мира. Например, уникальный IP-адрес модуля Pod в кластере Kubernetes не используется другими модулями, даже на том же узле, потому что каждый модуль работает в отдельном пространстве имен;
- **unshare** – команда, позволяющая запускать дочерние процессы, выполняющиеся изолированно, **unshare** также может изолировать точки монтирования (каталог /) и сетевые пространства имен (IP-адреса), и поэтому ее можно считать прямым аналогом команды docker run в ОС Linux;
- **ps** – программа, отображающая список запущенных процессов. Агент kubelet должен постоянно следить за процессами, чтобы всегда быть в курсе, например, когда они завершаются. С помощью команды ps можно определить, имеются ли в кластере процессы-зомби, не начал ли привилегированный контейнер проявлять неуправляемое поведение.

### Исследование зависимостей модуля Pod от Linux

Жизненный цикл модуля Pod – циклический процесс, отражающий фундаментальный цикл управления, который определяет, что делает сам агент kubelet во время работы. Поскольку контейнеры в модуле Pod могут выйти из строя в любой момент, на этот случай существует цикл управления, возвращающий их к жизни. Можно сказать, что сам Kubernetes – это всего лишь сложно организованный набор циклов управления, которые позволяют автоматически запускать и управлять контейнерами в больших масштабах. Пока kubelet работает, он выполняет непрерывный цикл согласования, в котором проверяются и запускаются модули Pod. После запуска модуля Pod в Kubernetes объект Pod на сервере API хранит полную информацию о состоянии модуля.
```
kubectl get pods -o yaml
```
Одним из параметров, которые Kubernetes определяет для всех своих модулей Pod, является default-token. Он предоставляет модулям сертификат, позволяющий связываться с сервером API и «звонить домой». В дополнение к томам Kubernetes мы передаем нашим модулям информацию о DNS.
```
kubectl exec -t -i core-k8s mount | grep resolv.conf
```
В большинстве окружений Linux то, что мы называем контейнерами, – это просто процессы, созданные с помощью нескольких механизмов изоляции, позволяющих им уживаться с сотнями других процессов в кластере.

### Создание модуля Pod с нуля

Четыре основных аспекта модуля Pod:
- хранилище;
- IP-адрес;
- изолированную сеть;
- идентификатор процесса.

Для начала создадим контейнер в самом прямом смысле – папку, в которой есть именно все, что нужно для запуска командной оболочки Bash, и больше ничего. Это делается с помощью известной команды **chroot**.

Назначение **chroot** – создать изолированную корневую файловую систему для процесса. Делается это в три шага.
- Выбор программы, которую требуется запустить, и где в файловой системе она должна работать.
- Создать среду для запуска процесса. В каталоге lib64 в Linux находится множество программ, которые необходимы даже для запуска простой командной оболочки, такой как Bash. Их нужно загрузить в новую корневую файловую систему.
- Скопировать программу для запуска в chroot-окружение.

Наконец, можно запустить программу, и она будет полностью изолирована от исходной файловой системы: она не сможет видеть или изменять другую информацию в вашей файловой системе.

```bash
#/bin/bash
mkdir /home/namespace/box
mkdir /home/namespace/box/bin   # Создают структуру каталогов для chroot-окружения, необходимую для нашей программы Bash
mkdir /home/namespace/box/lib
mkdir /home/namespace/box/lib64
cp -v /usr/bin/kill /home/namespace/box/bin/   # Копируют все программы из базовой ОС в это окружение
cp -v /usr/bin/ps /home/namespace/box/bin
cp -v /bin/bash /home/namespace/box/bin
cp -v /bin/ls /home/namespace/box/bin
cp -r /lib/* /home/namespace/box/lib/   # Копируют библиотечные зависимости программ в каталоги lib/
cp -r /lib64/* /home/namespace/box/lib64/
mount -t proc proc /home/namespace/box/proc   # Монтирует каталог /proc сюда
chroot /home/namespace/box /bin/bash   # Запускаем изолированный процесс Bash в изолированном каталоге
```
Изолированное окружение chroot – один из основных строительных блоков контейнерной революции, продолжающейся и по сей день, хотя в течение некоторого времени она была известна как «виртуальная машина для бедных».

### Использование mount для передачи данных процессам

Команда mount позволяет взять устройство и отобразить его в любой каталог в вашей ОС. Мы могли бы выполнить простую команду **mount --bind /tmp/ /home/namespace/box/data** и создать каталог /data в предыдущем сценарии chroot. Мы создали нечто похожее на контейнер, к тому же имеющее доступ к хранилищу.

### Защита процесса с помощью unshare

Одной из первых проблем, которые вам, возможно, придется решить при создании рабочей среды контейнеризации, является изоляция. Если выполнить ps -ax из этого процесса, сразу станет понятна важность изоляции; контейнер, имеющий полный доступ к хосту, сможет вывести его из строя, например, остановив процесс kubelet или удалив критически важные для системы файлы. Однако с помощью команды unshare мы можем создать chroot- окружение для запуска Bash в изолированном терминале с понастоящему ограниченным пространством процесса. 

Пример демонстрирует использование команды unshare для такой изоляции:
```
unshare -p -f --mount-proc=/home/namespace/box/proc chroot /home/namespace/box /bin/bash
```
Используя unshare для запуска chroot, мы имеем:
- изолированный процесс;
- изолированную файловую систему;
- возможность изменять определенные файлы в дереве каталогов /tmp.

### Создание сетевого пространства имен

Чтобы запустить ту же программу в новой сети, можно снова использовать команду unshare:
```
unshare -p -n -f --mount-proc=/home/namespace/box/proc chroot /home/namespace/box /bin/bash
```
Если сравнить этот модуль Pod с настоящим, то можно заметить одно важное отличие: отсутствие устройства eth0. В этом разница между контейнером, имеющим сеть и процессом в chroot-окружении - само по себе chroot-окружение бесполезно для запуска контейнерных приложений из-за необходимости в передаче им дополнительных изолированных средств.

### Ограничение потребления процессора с помощью cgroups

**Контрольные группы (control groups, cgroups)** – позволяют выделять больше или меньше процессорного времени и памяти приложениям, работающим в наших кластерах.
```yml
containers:
  resources: limits:
    memory: "200Mi"
  requests:
    memory: "100Mi"
```
### Создание раздела resources

Фактический способ определения этого параметра можно настроить в любом дистрибутиве Kubernetes с помощью флага --cgroup-driver. (Драйверы контрольных групп – это архитектурные компоненты в Linux, которые используются для выделения ресурсов. Обычно в роли драйвера в Linux используется systemd.

Чтобы определить ограничения контрольной группы, нужно выполнить следующие действия:
- создать PID (Внутри окружения - echo $$ - в примере 79455)
- передать ограничения для этого PID в операционную систему.
```bash
mkdir /sys/fs/cgroup/memory/chroot0
echo "10" > /sys/fs/cgroup/memory/chroot0/memory.limit_in_bytes
echo "0" > /sys/fs/cgroup/memory/chroot0/memory.swappiness
echo 79455 > /sys/fs/cgroup/memory/chroot0/tasks
```
Теперь вернемся к терминалу, где был запущен сценарий chroot0.sh. Попытка выполнить простую команду, такую как ls, терпит неудачу - bash: fork: Cannot allocate memory

### Проблема сети

Любому контейнеру Kubernetes могут потребоваться:
- маршрутизация трафика внутри кластера для сетевых взаимодействий между модулями Pod;
- маршрутизация трафика наружу для доступа к другим модулям Pod в интернете;
- балансировка трафика между конечными точками за сервисом со статическим IP-адресом.

Для поддержки всего этого необходимо, чтобы метаданные о модулях Pod публиковались где-то в Kubernetes (эту задачу решает сервер API), а также постоянно наблюдать за их состоянием (эту задачу решает kubelet) и своевременно обновлять эту информацию. Таким образом, модули Pod имеют не только команду запуска контейнера и образ Docker, но также метки и четко определенные спецификации, определяющие порядок публикации их состояния, благодаря чему их можно повторно создавать на лету вместе с набором возможностей, предоставляемых агентом kubelet.

### Как kube-proxy реализует сервисы Kubernetes с помощью iptables

Сервисы Kubernetes определяют контракт API, в котором говорится: «При обращении к некоторому IP-адресу ваш трафик автоматически пересылается одной из многих возможных конечных точек». В большинстве кластеров эти сетевые правила полностью реализуются прокси-сервером kube-proxy, который чаще всего настроен на использование iptables для организации маршрутизации сетевых пакетов. Программа iptables добавляет в ядро правила, которые затем используются для обработки сетевого трафика, и это наиболее распространенный способ реализации сервисов Kubernetes и маршрутизации трафика.

Модулю Pod нужно нечто большее, чем набор правил брандмауэра. Ему нужны по крайней мере:
- возможность принимать трафик в конечной точке сервиса;
- возможность отправлять трафик во внешний мир из своей конечной точки;
- возможность отслеживать текущие соединения TCP

### Использование модуля kube-dns

Модуль Pod kube-dns
- работает в любом кластере Kubernetes
- не имеет особых привилегий и использует обычную сеть модулей Pod, а не сеть хоста;
- отправляет трафик в порт 53, широко известный как стандартный порт DNS;
- уже работает в вашем кластере по умолчанию.

В Kubernetes провайдер CNI предоставляет уникальный IP-адрес и правила маршрутизации для доступа к этому адресу. Мы можем исследовать эти маршруты с помощью команды **ip route**

### Другие проблемы

**Хранение**

Помимо сети, нашему модулю Pod также может потребоваться доступ к различным типам хранилищ. Нужен какой-то способ, с помощью которого можно определить типы хранилищ и фиксировать в журнале неудачные попытки монтирования этих томов хранилища. Такой способ в Kubernetes предоставляют объекты StorageClass, PersistentVolume и PersistentVolumeClaim.

**Планирование**

Наличие планировщика, достаточно умного, чтобы поместить модуль Pod в место, где иерархия контрольных групп соответствует требованиям модуля к ресурсам, – еще одна важнейшая функция, которую берет на себя Kubernetes. **Планирование** – это общая проблема в информатике, поэтому следует отметить, что существуют альтернативные инструменты планирования, такие как Nomad.

**Обновление и повторный запуск**

Управление процессами и/или контрольными группами, связанными с модулем Pod, который может перестать работать, является важной частью организации масштабных контейнерных рабочих нагрузок, особенно в контексте микросервисов, которые должны быть переносимыми и эфемерными. Модель данных приложений в Kubernetes, о которой чаще всего думают с точки зрения объектов развертывания Deployment, приложений с состоянием StatefulSet, заданий Job и наборов демонов DaemonSet, поддерживает возможность надежного обновления.

# Использование контрольных групп для управления процессами в модулях Pod

### Процессы и потоки в Linux

Любой процесс в Linux может создать один или несколько потоков выполнения. Поток выполнения – это абстракция, которую программы могут использовать для создания новых процессов, имеющих общую память. Для примера давайте посмотрим с помощью команды ps -T, сколько независимых потоков планирования используется в Ku- bernetes:
```
ps -ax | grep scheduler
ps -T 631 - команда выводит список потоков выполнения планировщика, использующих общую память.
```
### Контрольные группы для управляющих процессов

Планировщик порождает несколько потомков и часто сам создается платформой Kubernetes, потому что является потомком containerd, – среды выполнения контейнеров, – которую Kubernetes использует в kind. В качестве эксперимента можете попробовать завершить процесс containerd и понаблюдать, как планировщик и его потоки возвращаются к жизни. Это делается самим агентом kubelet, у которого есть каталог /manifests. В этом каталоге определяются процессы, которые всегда должны выполняться, даже до того, как сервер API сможет планировать контейнеры. Собственно, так Kubernetes и устанавливает себя через kubelet. Жизненный цикл установки Kubernetes, который реализует kubeadm:
- в kubelet есть каталог manifests, в который включены сервер API, планировщик и диспетчер контроллеров;
- system запускает kubelet;
- kubelet сообщает процессу containerd о необходимости запустить все процессы, перечисленные в каталоге manifests;
- сразу после запуска сервера API агент kubelet подключается к нему и запускает все контейнеры, которые запросит сервер API.

**Зеркальные модули Pod и сервер API**
В kubelet есть секретное оружие: каталог /etc/kubernetes/manifests. Агент kubelet постоянно сканирует этот каталог, и, когда в нем появляются новые модули Pod, он немедленно создает и запускает их. Поскольку запуск этих модулей Pod производится в обход сервера Kubernetes API, им необходимо зеркалировать себя, чтобы сервер API узнал об их существовании. По этой причине модули Pod, созданные за пределами плоскости управления Kubernetes, называются зеркальными модулями Pod (mirror Pod). Зеркальные модули, как и любые другие, можно увидеть, выполнив команду kubectl get pods -A, но они создаются и управляются агентом kubelet на независимой основе. Это позволяет kubelet в одиночку запустить целый кластер Kubernetes, работающий внутри модулей Pod.

Модули Pod с классом **burstable**, как правило, не имеют жестких ограничений на использование ресурсов. Планировщик – это пример модуля Pod, который может иметь всплески потребления процессора (например, когда необходимо быстро запланировать 10 или 20 модулей Pod для выполнения на узле)

### Реализация контрольных групп для обычного модуля Pod

Модуль Pod планировщика представляет особый случай, потому что работает на всех кластерах и не поддерживает прямой возможности настройки или исследования. Волшебство изоляции в Kubernetes на компьютере с Linux можно рассматривать как обычное иерархическое распределение ресурсов, организованное с помощью простой структуры каталогов.
```
sudo cat /sys/fs/memory/docker/753../kubepods/pod8a58e9/d176../memory.limit_in_bytes
```
### Как kubelet управляет контрольными группами

Заглянув в /sys/fs/cgroup, можно увидеть все контролируемые ресурсы, которые в Linux можно распределить иерархически:
```
ls -d /sys/fs/cgroup/*
```
### Как kubelet управляет ресурсами

В структуре данных allocatable. Выполнив команду **kubectl get nodes -o yaml** на узле Kubernetes можно увидеть:
```
allocatable:
  cpu: "12"
  ephemeral-storage: 982940092Ki hugepages-1Gi: "0" hugepages-2Mi: "0"
  memory: 32575684Ki
  pods: "110"
```
Эти ресурсы представляют суммарный бюджет контрольных групп, доступный для выделения ресурсов модулям Pod. Агент kubelet вычисляет его, определяя общую емкость узла. Затем определяет пропускную способность процессора, необходимую для него самого и базового узла, и вычитает ее из доступного объема ресурсов. Полученные значения используются планировщиком Kubernetes при принятии решения о возможности размещения контейнера на этом конкретном узле. Для планирования используется следующее уравнение:
**Доступно_для_распределения = емкость_узла – ресурсы_для_kube – ресурсы_для_системы**

Вместе с kubelet запускается встроенная родительская логика. Соответствующий параметр настраивается с помощью флага командной строки, в результате чего kubelet становится родительской контрольной группой cgroup верхнего уровня для дочерних контейнеров. Предыдущее уравнение вычисляет общее количество контрольных групп, доступных для kubelet, которое называется бюджетом распределяемых ресурсов.

### Классы QoS: почему они важны и как они работают

Под QoS (качества обслуживания - Quality of Service) понимается доступность ресурсов в любой момент. QoS позволяет идти по тонкой грани, когда многие службы работают неоптимально в часы пик, не жертвуя качеством критически важных сервисов. На практике такими критическими сервисами могут быть системы обработки платежей. Вытеснение модуля Pod во многом зависит от того, насколько он превысит лимит ресурсов. В общем случае:
- приложения с предсказуемым потреблением памяти и процессора подвергаются вытеснению с меньшей вероятностью;
- жадные приложения с большей вероятностью будут вытеснены в периоды пиковых нагрузок, если попытаются использовать больше процессорного времени или памяти, чем выделено Kubernetes, и при условии, что они не принадлежат к классу Guaranteed;
- приложения с классом BestEffort – первые кандидаты на вытеснение и перепланирование в периоды пиковых нагрузок.

### Создание классов QoS путем настройки ресурсов

Burstable, Guaranteed и BestEffort – это три класса QoS, которые создаются в зависимости от определения объекта Pod. Эти параметры могут помочь увеличить количество контейнеров, запускаемых в кластере, часть из которых затем может отключаться в периоды высокой загрузки и вновь запускаться позже. Нет универсальных политик, подходящих под все случаи жизни:
- если все контейнеры отнести к классу гарантированного качества обслуживания (Guaranteed QoS), вам будет трудно обслуживать динамические рабочие нагрузки с изменяющимися потребностями в ресурсах;
- если на серверах не будет контейнеров с классом Guaranteed QoS, то kubelet не сможет поддерживать работу некоторых важных процессов.

Вот основные правила определения класса QoS:
- BestEffort – к этому классу относятся модули Pod, не определяющие требуемые количества памяти или ядер процессора. Они первые кандидаты на вытеснение (и перезапуск на другом узле), когда образуется нехватка ресурсов;
- Burstable – к этому классу относятся модули Pod, определяющие требуемые количества памяти или ядер процессора, но не устанавливающие ограничений для обоих ресурсов. Они вытесняются с меньшей вероятностью, чем модули с классом BestEffort;
- Guaranteed – к этому классу относятся модули Pod, определяющие требуемые количества памяти и ядер процессора и устанавливающие ограничения для обоих ресурсов. Они вытесняются и перемещаются в последнюю очередь.
```
kubectl get pods -n qos -o yaml -  вернет класс в поле status определения модуля Pod - qosClass: Burstable
```
# Интерфейсы CNI и настройка сети в модулях Pod
