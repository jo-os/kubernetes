# Kubernetes

**Дирижирование оркестром контейнеров**

Оркестратор контейнеров: программный компонент, предназначенный для объединения множества разных серверов в кластер Кластер — это своего рода унифицированная вычислительная подложка, которая, с точки зрения пользователя, выглядит как компьютер очень высокой мощности, способный работать с контейнерами.

Термин «оркестратор контейнеров» обычно относится к одному сервису, который занимается планированием и оркестрацией кластера, а также управлением им.
- Оркестрация означает координацию и выстраивание цепочки из разных действий для достижения общей цели. 
- Планирование же означает управление доступными ресурсами и направление рабочих заданий туда, где они могут быть выполнены наиболее эффективно.
- Управление кластером является объединение нескольких физических или виртуальных серверов в унифицированную, надежную, отказоустойчивую и довольно цельную группу

В 2014 году компания Google основала открытый проект под названием Kubernetes (от греческого слова — «рулевой, пилот»).

**Что делает платформу Kubernetes такой ценной**

Kubernetes занимается тем же, чем и самые лучшие системные администраторы: автоматизацией, централизованным ведением журнала, мониторингом, обеспечивает отказоустойчивость. Балансировка нагрузки и автомасштабирование, встроены в ядро Kubernetes, Kubernetes отличается обширной и постоянно растущей экосистемой.

**Kubernetes облегчает развертывание**
- Благодаря тому что Kubernetes выполняет плавающие обновления по умолчанию, приобрели популярность развертывания с нулевым временем простоя
- Kubernetes поддерживает автомасштабирование
- Избыточность и отказоустойчивость встроены в Kubernetes

**Kubernetes не решает все проблемы**
- Kubernetes просто не очень хорошо подходит для некоторых структур (таких как базы данных).
- некоторые задачи не требуют Kubernetes и могут работать на платформах, которые иногда называют бессерверными (функции как сервис (functions as a service, FaaS))

**Облачная ориентированность**

Термин «облачно-ориентированный» (cloud native) становится все более популярным сокращением, которое описывает современные приложения и сервисы, пользующиеся преимуществами облаков, контейнеров и оркестрации

Характеристики облачно-ориентированных систем:
- Автоматизируемость
- Универсальность и гибкость
- Устойчивость и масштабируемость
- Динамичность
- Наблюдаемость
- Распределенность

# Первые шаги с Kubernetes

Запуск приложения
```
kubectl run demo --image=cloudnatived/demo:hello --port=9999 --labels app=demo deployment.apps "demo" created
```
Перенаправление портов
```
kubectl port-forward deploy/demo 9999:8888
```
Просмотр pods
```
kubectl get pods --selector app=demo
```
# Размещение Kubernetes

**Архитектура кластера**

«Мозг» кластера называется **управляющим уровнем**. Управляющий уровень на самом деле состоит из нескольких компонентов.
- kube-apiserver — это внешний сервер для управляющего уровня, который обрабатывает API-запросы
- etcd — база данных, в которой Kubernetes хранит всю информацию о существующих узлах, ресурсах кластера
- kube-scheduler определяет, где будут запущены свежесозданные pod-оболочки.
- kube-controller-manager отвечает за запуск контроллеров ресурсов, таких как развертывания.
- cloud-controller-manager взаимодействует с облачным провайдером (в облачных кластерах), управляя такими ресурсами, как балансировщики нагрузки и дисковые тома.

Участники кластера, которые **выполняют компоненты управляющего уровня**, называются **ведущими узлами**.

Участники кластера, которые **выполняют пользовательские рабочие задания**, называются **рабочими узлами**.

Каждый рабочий узел в кластере Kubernetes отвечает за следующие компоненты.
- kubelet отвечает за управление средой выполнения контейнера, в которой запускаются рабочие задания, запланированные для узла, а также за мониторинг их состояния.
- kube-proxy занимается сетевой магией, которая распределяет запросы между pod-оболочками на разных узлах, а также между pod-оболочками и Интернетом.
- Среда выполнения контейнеров запускает и останавливает контейнеры, а также отвечает за их взаимодействие.

**Высокая доступность**

У правильно сконфигурированного управляющего уровня Kubernetes есть несколько ведущих узлов, что делает его высокодоступным. База данных etcd реплицируется между несколькими узлами и может пережить отказ отдельных копий при условии наличия кворума из более чем половины реплик etcd.

**Отказ управляющего уровня**

Если вы остановите все ведущие узлы в своем кластере, pod-оболочки на рабочих узлах продолжат функционировать — по крайней мере какое-то время. Однако вы не сможете развертывать новые контейнеры или менять какие-либо ресурсы Kubernetes, а такие контроллеры, как развертывания, перестанут действовать.

Вам необходимо запастись достаточным количеством ведущих узлов, чтобы кластер мог поддерживать кворум, даже если какой-то из узлов откажет. Для промышленных кластеров реалистичным минимумом является три узла.

**Отказ рабочего узла**

Отказ любого рабочего узла, не влечет за собой никаких существенных последствий: Kubernetes обнаружит сбой и перераспределит pod-оболочки этого узла. Главное, чтобы работал управляющий уровень.

**Установщики Kubernetes**

**kops** (kubernetes.io/docs/setup/production-environment/tools/kops) — это утилита командной строки для автоматического выделения кластеров, служит инструментом, предназначенным специально для AWS.

**Kubespray** фокусируется на установке Kubernetes на существующие компьютеры, особенно на локальные и физические серверы. Это инструмент для простого развертывания промышленных кластеров. Он предлагает множество возможностей, включая высокую доступность и поддержку нескольких платформ.

**TK8** (github.com/kubernauts/tk8) — утилита командной строки для создания кластеров Kubernetes, которая использует как Terraform (для создания облачных серверов), так и Kubespray (для установки на них Kubernetes). Она написана на Go и поддерживает установку на AWS, OpenStack и «чистые серверы». 

**Kubernetes: трудный путь**

**kubeadm** - Утилита kubeadm (kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm) входит в состав дистрибутива Kubernetes и должна помочь вам устанавливать и обслуживать кластеры в соответствии с лучшими рекомендациями. Многие инструменты и сервисы, используют kubeadm для выполнения административных операций,

**Tarmak** (blog.jetstack.io/blog/introducing-tarmak) — это инструмент для управления жизненным циклом кластеров Kubernetes, который должен упростить и сделать более надежными модификацию и обновления кластерных узлов. Tarmak использует Terraform для выделения узлов кластера и Puppet для управления конфигурацией на самих узлах.

**Rancher Kubernetes Engine** - RKE (github.com/rancher/rke) стремится быть простым и быстрым установщиком Kubernetes.

# Работа с объектами Kubernetes
**Надзор и планирование**

Для каждой программы, за которой нужно следить, Kubernetes создает соответствующий объект Deployment, записывающий некоторую связанную с ней информацию: имя образа контейнера, количество реплик (копий), которые вы хотите выполнять, и любые другие параметры, необходимые для запуска контейнера. В связке с ресурсом Deployment работает некий объект Kubernetes под названием контроллер. Контроллеры отслеживают ресурсы, за которые отвечают, убеждаясь в том, что те присутствуют и выполняются, а если заданное развертывание по какой-либо причине не имеет достаточного количества реплик, дополнительно их создают. На самом деле развертывание не управляет репликами напрямую: вместо этого оно автоматически создает сопутствующий объект под названием ReplicaSet, который сам этим занимается.

**Перезапуск контейнеров**

Большинство приложений Kubernetes должны работать долго и надежно, поэтому
подобное поведение имеет смысл: контейнер может остановиться по разным причинам, и в большинстве случаев реакцией живого оператора будет перезапуск — именно так по умолчанию и ведет себя Kubernetes. Задача развертывания состоит в том, чтобы отслеживать связанные с ним контейнеры и постоянно поддерживать определенное их количество.

**Обращение к развертываниям**

Просмотреть все активные развертывания в вашем текущем пространстве:
```
kubectl get deployments
kubectl describe deployments/demo - подробной информации о конкретном развертывании
```
**Pod-оболочки**

**Pod** — это объект Kubernetes, который представляет группу из одного или нескольких контейнеров

**Объекты ReplicaSet**

**ReplicaSet отвечает** за группу идентичных pod-оболочек (или реплик). Развертывания, в свою очередь, управляют объектами ReplicaSet и контролируют поведение реплик в момент их обновления — например, при выкатывании новой версии вашего приложения.

**Поддержание желаемого состояния**

Контроллеры Kubernetes непрерывно сравнивают желаемое состояние, указанное каждым ресурсом, с реальным состоянием кластера и вносят необходимые корректировки. Этот процесс называют циклом согласования, поскольку он все время повторяется в попытке согласовать текущее состояние с желаемым. Создав развертывание, вы сообщили Kubernetes о том, что pod-оболочка должна работать **всегда**.

**Планировщик Kubernetes**

Развертывание создаст pod-оболочки, а Kubernetes при необходимости их запустит. За эту часть процесса отвечает компонент Kubernetes под названием «планировщик». Задача планировщика — следить за этой очередью, взять из нее следующую запланированную pod-оболочку и найти узел, на котором ее можно запустить. При выборе подходящего узла планировщик будет исходить из нескольких разных критериев, включая ресурсы, запрашиваемые pod-оболочкой. Как только выполнение pod-оболочки было запланировано, утилита kubelet, работающая на соответствующем узле, подхватывает ее и производит запуск ее контейнера.

**Ресурсы являются данными**

Все ресурсы Kubernetes, такие как развертывания и pod-оболочки, представлены записями во внутренней базе данных. Цикл согласования следит за любыми изменениями в записях и предпринимает соответствующие действия. На самом деле команда kubectl run лишь добавляет в базу данных новую запись о развертывании, а система делает все остальное. Но для взаимодействия с Kubernetes вовсе не обязательно использовать команду kubectl run — вы можете создавать манифесты ресурсов.

**Манифесты развертываний**

deployment.yaml
```yml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: demo
  labels:
    app: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo
template:
  metadata:
    labels:
      app: demo
spec:
  containers:
    - name: demo
    image: cloudnatived/demo:hello
    ports:
    - containerPort: 8888
```
**Использование команды kubectl apply**

Передавать YAML-манифесты вашему кластеру, используя команду kubectl apply.
```
kubectl apply -f k8s/deployment.yaml
```
Для подключения к pod-оболочке с помощью веб-браузера надо создать сервис — ресурс Kubernetes, который позволяет подключаться к развернутым pod-оболочкам.

**Ресурсы типа «сервис»**

Ресурс типа «сервис» предоставляет один несменяемый IP-адрес или такое же доменное имя, которые автоматически перена- правляют на любую подходящую pod-оболочку. Сервис можно считать веб-прокси или балансировщиком нагрузки, который направляет запросы к группе внутренних pod-оболочек.

service.yaml
```yaml
apiVersion: v1
kind: Service
metadata:
  name: demo
  labels:
    app: demo
spec:
ports:
  - port: 9999
  protocol: TCP
    targetPort: 8888
  selector:
    app: demo
  type: ClusterIP
```
Параметр selector говорит сервису, как перенаправлять запросы к конкретным pod-оболочкам. Сервис предоставляет запросам единую точку входа в эти pod-оболочки.
```
kubectl apply -f k8s/service.yaml
kubectl port-forward service/demo 9999
```
**Обращение к кластеру с помощью kubectl**

С помощью kubectl get можно запрашивать pod-оболочки и развертывания. Эту команду также можно использовать для просмотра узлов вашего кластера:
```
kubectl get nodes
```
Вывести ресурсы всех типов, выполните команду - kubectl get all

Получить исчерпывающую информацию об отдельной pod-оболочке (или любом другом ресурсе), выполните kubectl describe:
```
kubectl describe pod/demo-dev-6c96484c48-69vss
```
## Helm: диспетчер пакетов для Kubernetes

Один из популярных диспетчеров пакетов для Kubernetes называется Helm. helm для установки и конфигурации приложений
(собственных или чужих) и создавать пакеты, которые полностью описывают все необходимые для работы приложения ресурсы, включая их зависимости и настройки. Пакеты в Helm называются чартами (charts).

Установка Helm - Следуйте инструкциям по установке Helm (helm.sh/docs/using_helm/#installing-helm) для вашей операционной системы.

Установка чарта Helm
```
helm install --name demo ./k8s/demo
```
Helm создал ресурс Deployment и Service, команда helm install создает объект Kubernetes под названием «выпуск» (release).

**Чарты, репозитории и выпуски**
- Чарт — пакет Helm с определениями всех ресурсов, необходимых для выполнения приложения в Kubernetes.
- Репозиторий — место, где можно делиться своими чартами и загружать чужие.
- Выпуск — конкретный экземпляр чарта, запущенный в кластере Kubernetes

У каждого выпуска есть уникальное имя, которое можно указать в команде helm install с помощью флага **-name**

**Вывод списка выпусков Helm**

Проверить, какие выпуски запущены на данный момент:
```
helm list
```
Вывести состояние конкретного выпуска, передайте его имя команде **helm status**.

## Управление ресурсами

**Запросы ресурсов**

Запрос ресурса в Kubernetes определяет минимальный объем этого ресурса, который необходим для работы pod-оболочки. Например, запрос 100m (100 миллипроцессоров) и 250Mi (250 МиБ памяти) означает, что pod-оболочка не может быть назначена узлу с меньшим количеством доступных ресурсов. Если в кластере нет ни одного узла с достаточной мощностью, pod-оболочка будет оставаться в состоянии pending, пока такой узел не появится.
```yml
resources:
  requests:
    memory: "10Mi"
    cpu: "100m"
```
**Лимит на ресурс** определяет максимальное количество этого ресурса, которое pod-оболочке позволено использовать. Если pod-оболочка попытается занять больше выделенного ей лимита на процессор, производительность будет снижена. Pod-оболочка, пытающаяся использовать больше, чем указано в лимите на память, будет принудительно остановлена, и ее выполнение, если это возможно, снова окажется запланировано.
```yml
resources:
  limits:
    memory: "20Mi"
    cpu: "250m"
```
Kubernetes допускает **отрицательный баланс ресурсов**, когда сумма всех лимитов у контейнеров одного узла превышает общее количество ресурсов, которыми узел обладает. Общее количество потребляемых ресурсов начнет приближаться к максимальной мощности узла и Kubernetes начнет удалять контейнеры более агрессивно. В условиях нехватки ресурсов могут быть остановлены даже те контейнеры, которые исчерпали запрошенные ресурсы, а не лимиты. При прочих равных, когда возникает необходимость в удалении pod-оболочек, Kubernetes начинает с тех, которые больше всего превысили запрошенные ресурсы.

**Управление жизненным циклом контейнера**

Контейнеризированные приложения довольно часто входят в ступор: их процесс все еще выполняется, но больше не обслуживает никакие запросы. Kubernetes нужен какой-то способ для обнаружения подобных ситуаций, чтобы решить проблему за счет перезапуска контейнера.

**Проверки работоспособности**

Если приложение отвечает с помощью HTTP-кода вида 2xx или 3xx, Kubernetes считает его активным.
Для контейнера с HTTP-сервером определение проверки работоспособности обычно выглядит примерно так:
```yml
livenessProbe:
  httpGet:
    path: /healthz
    port: 8888
  initialDelaySeconds: 3
  periodSeconds: 3
```
**Задержка и частота проверки**

Поле initialDelaySeconds позволяет указать время ожидания перед первой проверкой работоспособности, чтобы избежать убийственного цикла (loop of the death). Поле periodSeconds определяет, как часто следует выполнять проверку работоспособности: в данном примере это делается каждые три секунды.

**Проверки готовности**

Проверки готовности и работоспособности имеют общее происхождение, но разную семантику. Если ваше приложение не начинает прослушивать HTTP, пока не будет готово к обработке запросов, проверки готовности и работоспособности могут выглядеть одинаково:
```yml
readinessProbe:
  httpGet:
    path: /healthz
    port: 8888
  initialDelaySeconds: 3
  periodSeconds: 3
```
Контейнер, не прошедший проверку готовности, удаляется из любых сервисов, совпавших с заданной pod-оболочкой. Это похоже на удаление неисправного узла из пула балансировщика нагрузки: к pod-оболочке не будет направляться трафик, пока она опять не начнет успешно проходить проверку готовности.

**Проверки готовности на основе файла** - Проверка готовности такого рода может быть полезной. Например, если нужно временно выключить контейнер с целью отладки, к нему можно подключиться и удалить файл /tmp/healthy.

**minReadySeconds** - для контейнера можно установить поле minReadySeconds. Контейнеры или pod-оболочки не будут считаться готовыми, пока с момента успешной проверки готовности не пройдет minReadySeconds секунд (по умолчанию 0).

**PodDisruptionBudget** - Иногда Kubernetes нужно остановить ваши pod-оболочки, даже если они в полном порядке и готовы к работе (этот процесс называется выселением). Возможно, узел, на котором они размещены, очищается перед обновлением и pod-оболочки необходимо переместить на другой узел. Ресурс PodDisruptionBudget позволяет указать, сколько pod-оболочек заданного приложения допустимо к потере в любой момент времени.

**minAvailable** - для определения минимального количества рабочих pod.

**maxUnavailable** - это относится лишь к так называемому добровольному выселению, то есть инициированному системой Kubernetes. Если, к примеру, узел испытывает аппаратные неполадки или удаляется, его pod-оболочки выселяются принудительно, даже если это нарушает параметры PodDisruptionBudget.

**Использование пространств имен** 

Механизмом контроля за потреблением ресурсов в вашем кластере является использование пространств имен. Пространство имен в Kubernetes предоставляет способ разделения кластера на отдельные части.

Чтобы увидеть, какие пространства имен существуют в вашем кластере:
```
kubectl get namespaces
```
**Работа с пространствами имен**
Ваша команда будет использовать то пространство имен, которое вы укажете с помощью флага --namespace (сокращенно -n).
```
kubectl get pods --namespace prod
```
В Kubernetes пространства имен можно создавать с помощью ресурса Namespace:
```yml
apiVersion: v1
kind: Namespace
metadata:
  name: demo
```
Пространство имен можно использовать в качестве временного виртуального кластера, который, став ненужным, удаляется.

**Квоты на ресурсы**

Вы можете ограничить потребление процессорного времени, памяти и для заданных пространств имен
```yml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: demo-resourcequota
spec:
  hard:
    pods: "100"
```
Применение этого манифеста к конкретному пространству имен (например, к demo) устанавливает жесткий лимит на запуск не более чем 100 pod-оболочек в этом пространстве.
```
kubectl create namespace demo
kubectl apply --namespace demo -f k8s/resourcequota.yaml
```
Проверить, активирован ли ресурс ResourceQuotas в конкретном пространстве имен:
```
kubectl get resourcequotas -n demo
```
**Оптимизация стоимости кластера**

**Оптимизация узлов**

Более крупные узлы могут быть более рентабельными, поскольку доля их ресурсов, доступных для ваших рабочих заданий, оказывается выше. Но, с другой стороны, потеря отдельного узла будет иметь более заметные последствия для доступной мощности вашего кластера. У мелких узлов также более высокая доля заблокированных ресурсов — порций памяти и процессорного времени, которые не используются и которые слишком малы для того, чтобы pod-оболочка могла их занять. Узлы лучше делать достаточно большими для выполнения как минимум пяти ваших типичных pod-оболочек. По умолчанию в Kubernetes действует лимит 110 pod-оболочек на узел, можете поднять его с помощью параметра --max-pods утилиты kubelet. Стремитесь к показателю 10–100 pod-оболочек на узел.

**Оптимизация хранилища** - Консоль вашего облачного провайдера или сервиса Kubernetes обычно умеет по-
казывать количество IOPS, используемое вашими узлами.

**Избавление от неиспользуемых ресурсов** - По мере увеличения вашего кластера Kubernetes вы будете обнаруживать множество неиспользуемых или потерянных ресурсов.

**Использование метаданных владельца** - Чтобы минимизировать неиспользуемые ресурсы, на уровне организации стоит
ввести правило, согласно которому каждый ресурс должен иметь информацию о своем владельце. Для этого можно использовать аннотации Kubernetes.
```yml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: my-brilliant-app
  annotations:
    example.com/owner: "Customer Apps Team"   - информацию о владельце
```
Метаданные владельца должны указывать на человека или команду, с которыми можно связаться при возникновении вопросов относительно конкретного ресурса.

**Поиск малоиспользуемых ресурсов**

Каждая pod-оболочка должна предоставлять в виде показателя количество полученных запросов. Вы также можете проверить показатели загруженности процессора и памяти у каждой pod-оболочки в вашей веб-консоли, чтобы найти наименее загруженные.

**Удаление завершенных запланированных заданий**

Запланированные задания в Kubernetes — это pod-оболочки, которые выполняются единожды и не перезапускаюся после завершения работы. Однако в базе данных Kubernetes остаются Job-объекты, и если у вас наберется значительное количество завершенных заданий, это может повлиять на производительность API. Для удаления таких объектов предусмотрен
удобный инструмент **kube-job-cleaner**

**Использование принадлежности к узлам для управления планированием**

В Kubernetes можно использовать концепцию принадлежности узлов (node affinities), чтобы pod-оболочки, отказ которых недопустим, не размещались на прерываемых узлах. Чтобы планировщик Kubernetes никогда не разместил pod-оболочку на одном из таких узлов, добавьте следующий фрагмент в ее спецификацию.
```yml
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
          - key: cloud.google.com/gke-preemptible
            operator: DoesNotExist
```
Принадлежность requiredDuringScheduling... является обязательной: pod-оболочка, в которой она указана, никогда не будет запущена на узле, соответствующем выражению селектора (это называется жесткой принадлежностью

Мягкая принадлежность с противоположным смыслом
```yml
preferredDuringSchedulingIgnoredDuringExecution:
- preference:
  matchExpressions:
  - key: cloud.google.com/gke-preemptible
    operator: Exists
weight: 100
```
Означает следующее: «Если можешь — пожалуйста, размещай данную pod-оболочку на прерываемом узле; если нет — ничего страшного.

**Балансировка вашей рабочей нагрузки**

Суть проблемы в том, что планировщик перемещает pod-оболочки с одного узла на другой, только если они по какой-либо причине перезапускаются. К тому же задача планировщика — распределить рабочую нагрузку равномерно между узлами — иногда конфликтует с поддержанием высокой доступности отдельных сервисов. Одним из решений является использование инструмента под названием **Descheduler**. Для Descheduler можно задавать различные стратегии и линии поведения. Например, одна из его политик ищет малонагруженные узлы и переносит на них pod-оболочки с других узлов. Другая политика ищет pod-оболочки, две и более реплики которых размещены на одном узле, и выселяет их.

## Работа с кластерами

**Максимальный кластер**

1.12 официально поддерживает кластеры до 5000 узлов. В документации к Kubernetes говорится о том, что поддерживаемая конфигурация кластера не должна превышать 5000 узлов, 150 000 pod-оболочек, 300 000 контейнеров, а на одном узле не должно быть больше 100 pod-оболочек.

**Федеративные кластеры** - Федерация позволяет синхронизировать два и более кластера, запуская на них идентичные задания.

**Типы облачных серверов** - Ведущие узлы в небольших кластерах (до пяти узлов) должны иметь как минимум один виртуальный процессор (vCPU) и 3–4 ГиБ памяти. 

**Обратное масштабирование**

В принципе, у Kubernetes нет никаких проблем и с обратным масштабированием. Вы можете приказать системе очистить узлы, подлежащие удалению, и она постепенно выключит pod-оболочки на этих узлах или куда-нибудь их переместит.

Большинство инструментов для управления кластером выполняют очищение узлов автоматически, но также можно использовать команду **kubectl drain**, чтобы сделать это вручную. Если у кластера достаточно свободной мощности для перераспределения «обреченных» pod-оболочек, вы сможете удалить их сразу после успешного очищения узлов. Очистка позволяет pod-оболочкам корректно завершить свою работу, убрать за собой, а также, если необходимо, сохранить какое-либо состояние.

**Автомасштабирование** - автомасштабирование — автоматическое увеличение или уменьшение количества серверов в группе на основе какого-то показателя или графика.

**Проверка на соответствие** - платформа Kubernetes содержит набор тестов, позволяющий подтвердить, что кластер соответствует спецификации, то есть удовлетворяет основному набору требований для заданной версии Kubernetes. Соответствие стандартам — это лишь начальное условие, которое должно выполняться любым промышленным кластером. Однако в Kubernetes существует много распространенных проблем с конфигурацией и рабочей нагрузкой:
- использование слишком больших образов контейнеров может привести к потере значительного количества времени и ресурсов кластера;
- развертывания с единственной репликой pod-оболочки не являются высокодоступными;
- выполнение процессов в контейнерах от имени администратора представляет потенциальный риск для безопасности

Стандартным инструментом для выполнения проверок является система **Sonobuoy** от Heptio

**K8Guard** - Утилита K8Guard, разработанная компанией Target, умеет искать распространенные проблемы в кластерах

**Copper** (copper.sh) — это инструмент для проверки манифестов Kubernetes перед их развертыванием; он отмечает распространенные проблемы или применяет отдельные политики.

**kube-bench** — это инструмент для аудита кластеров Kubernetes с помощью набора тестов производительности, разработанных Центром интернет-безопасности.

**Ведение журнала аудита для Kubernetes** - Если включить ведение этого журнала, все запросы к API кластера будут записываться с пометкой о том, когда и кто их сделал.

**Хаотическое тестирование** - Такой вид автоматического, произвольного вмешательства в промышленные сервисы иногда называют тестом обезьяны в честь инструмента Chaos Monkey («хаотическая обезьяна»), разработанного компанией Netflix для тестирования своей инфраструктуры.

**chaoskube** (github.com/linki/chaoskube) случайным образом удаляет pod-оболочки вашего кластера.

**kube-monkey** (github.com/asobti/kube-monkey) запускается в заранее установленное время и формирует график развертываний, которые будут объектами тестирования на протяжении всего дня.

**PowerfulSeal** (github.com/bloomberg/powerfulseal) — это открытый инструмент для хаотического тестирования Kubernetes, который работает в двух режимах: интерактивном и автономном.

## Продвинутые инструменты для работы с Kubernetes

**kubectl** - главный инструмент для взаимодействия с Kubernetes

**Псевдонимы командной оболочки** - Чтобы сделать свою жизнь проще, большинство пользователей Kubernetes первым
делом создают псевдоним командной оболочки для kubectl.
```
alias k=kubectl
 - k get pods
alias kg=kubectl get
alias kgdep=kubectl get deployment
alias ksys=kubectl --namespace=kube-system
alias kd=kubectl describe
```
**Использование коротких флагов**

--namespace можно сократить до -n
```
kubectl get pods -n kube-system
```
Для работы с ресурсами, которые соответствуют какому-то набору меток. Для этого предусмотрен флаг --selector можно сократить до -l.
```
kubectl get pods -l environment=staging
```
**Сокращение названий типов ресурсов**
```
kubectl get po
kubectl get deploy
kubectl get svc
kubectl get ns
```
**Справка по ресурсам Kubernetes**
```
kubectl -h: - полный обзор доступных команд
kubectl get -h - подробное описание команды
```
kubectl предоставляет справку и для объектов Kubernetes, таких как развертывания или pod-оболочки. Команда kubectl explain выводит документацию для заданного типа ресурсов:
```
kubectl explain pods
```
**Отображение более подробного вывода**

С помощью флага -o wide можно получить дополнительную информацию
```
kubectl get pods -o wide
kubectl get nodes -o wide
```
По умолчанию команда kubectl get возвращает данные в виде обычного текста, но вы можете вывести их в формате JSON:
```
kubectl get pods -n kube-system -o json
```
Вывод можно отфильтровать с помощью других инструментов, таких как утилита jq.
```
kubectl get pods -n kube-system -o json | jq '.items[].metadata.name'
```
**Наблюдение за объектами**

У kubectl есть флаг --watch
```
kubectl get pods --watch
```
**Описание объектов**
Получить по-настоящему подробную информацию об объектах Kubernetes, можно воспользоваться командой kubectl describe:
```
kubectl describe pods demo-d94cffc44-gvgzm
```
Раздел Events особенно полезен для отладки контейнеров, которые не работают должным образом, поскольку в нем записан каждый этап жизненного цикла контейнера вместе с любыми возникшими ошибками.

**Работа с ресурсами. Императивные команды kubectl**

Большинство ресурсов можно создавать явным образом с помощью команды kubectl create:
```
kubectl create namespace my-new-namespace
kubectl delete namespace my-new-namespace - kubectl delete и удалит ресурс
kubectl edit deployments my-deployment - просмотреть и модифицировать ресурсы
```
Если вы допустили какие-либо ошибки kubectl об этом сообщит и откроет файл, чтобы вы могли исправить проблему.

**Когда не следует использовать императивные команды**

Они могут быть крайне полезными для быстрого тестирования или проверки новых идей, но их основная проблема в том, что у вас нет единого источника истины. Как только вы выполните императивную команду, состояние вашего кластера перестанет быть синхронизированным с файлами манифестов, хранящимися в системе контроля версий. Проверить изменения можно с помощью команды **kubectl diff**

**Генерация манифестов ресурсов**

Императивные команды могут сэкономить немало времени при создании с нуля YAML-файлов для Kubernetes. Вы можете сгенерировать YAML-манифест с помощью kubectl:
```
kubectl run demo --image=cloudnatived/demo:hello --dry-run -o yaml
```
Флаг --dry-run говорит kubectl о том, что вместо создания самого ресурса kubectl следует вывести его манифест. Флаг -o yaml позволяет отобразить манифест ресурса в формате YAML. Можете сохранить этот вывод в файл, отредактировать его и применить для создания ресурса в кластере:
```
kubectl run demo --image=cloudnatived/demo:hello --dry-run -o yaml > deployment.yaml
```
**Экспорт ресурсов**

kubectl может помочь с созданием манифестов не только для новых, но и для уже существующих ресурсов. Чтобы это сделать, укажите для команды kubectl get флаг --export:
```
kubectl get deployments newdemo -o yaml --export >deployment.yaml
```
**Сравнение ресурсов**

Прежде чем применять манифесты Kubernetes с помощью команды kubectl apply, было бы крайне полезно увидеть, что же на самом деле изменится в кластере. Для этого предусмотрена команда kubectl diff:
```
kubectl diff -f deployment.yaml
```
Можно убедиться, что вносимые вами изменения приведут именно к тем результатам, которых вы ожидали. Вы также будете предупреждены, если состояние активного ресурса рассинхронизировано с YAML-манифестом

**Работа с контейнерами**

**Просмотр журнальных записей контейнера** - одним из самых полезных источников информации являются его журнальные записи. С точки зрения Kubernetes журналом считается все, что контейнер записывает в потоки вывода сообщений об ошибках.

Исследование журнальных сообщений отдельных контейнеров по-прежнему очень полезно для отладки - **kubectl logs**
```
kubectl logs -n kube-system --tail=20 kube-dns-autoscaler-69c5cbdcdd-94h7f
```
Чтобы следить за контейнером и направлять его журнальный вывод в терминал, используйте флаг --follow
```
kubectl logs --namespace kube-system --tail=10 --follow etcd-docker-for-desktop
```
**Подключение к контейнеру** - позволит наблюдать вывод контейнера напрямую - команда **kubectl attach**
```
kubectl attach demo-54f4458547-fcx2n
```
**kubespy** - может наблюдать за отдельными ресурсами кластера и показывать вам, что с ними со временем происходит

**Перенаправление порта контейнера** - kubectl port-forward - с ее помощью также можно перенаправить порт контейнера
```
kubectl port-forward demo-54f4458547-vm88z 9999:8888
```
**Выполнение команд внутри контейнеров** - С помощью **kubectl exec** в любом контейнере можно запустить любую команду, включая командную оболочку
```
kubectl exec -it alpine-7fd44fc4bf-7gl4n /bin/sh
```
Если pod-оболочка содержит больше одного контейнера, kubectl exec по умолчанию выполняет команду только в первом из них. Но вы также можете указать нужный вам контейнер с помощью флага -c:
```
kubectl exec -it -c container2 POD_NAME /bin/sh
```
**Запуск контейнеров с целью отладки**
```
kubectl run NAME --image=IMAGE --rm --it --restart=Never --command --...
```
- --rm. Этот флаг говорит Kubernetes о необходимости удалить образ контейнера после завершения его работы, чтобы тот не занимал место в локальном хранилище узла.
- --it. Запускает контейнер интерактивно (i — interactively) в локальном терминале (t — terminal), чтобы вы могли просматривать его вывод и при необходимости отправлять ему информацию о нажатых клавишах.
- --restart=Never. Говорит Kubernetes не перезапускать контейнер каждый раз, когда тот завершает работу (поведение по умолчанию). Мы можем отключить стандартную политику перезапуска, так как нам нужно запустить контейнер лишь один раз.
- --command--. Вместо точки входа контейнера по умолчанию указывает команду, которую нужно выполнить. Все, что идет за --, будет передано контейнеру в виде командной строки, включая аргументы.

**Использование команд BusyBox**

**busybox** - является особенно полезным ввиду наличия в нем большого количества самых востребованных в Unix команд, таких как cat, echo, find, grep и kill. Чтобы получить интерактивную командную оболочку в своем кластере:
```
kubectl run busybox --image=busybox:1.28 --rm --it --restart=Never /bin/sh
```
**Добавление BusyBox в ваш контейнер**

Самый простой способ сделать отладку контейнера легкой и сохранить при этом его очень маленький размер — скопировать в него исполняемый файл busybox во время сборки.
```
COPY --from=busybox:1.28 /bin/busybox /bin/busybox
контейнер остался очень маленьким, но теперь вы можете получить в нем командную оболочку, запустив:
kubectl exec -it POD_NAME /bin/busybox sh
```
**Контексты и пространства имен**

kubectl предлагает концепцию контекстов. Контекст — это сочетание кластера, пользователя и пространства имен. Когда вы запускаете команды kubectl, они всегда выполняются в текущем контексте.
```
kubectl config get-contexts
CURRENT    NAME                  CLUSTER          AUTHINFO        NAMESPACE
           gke                   gke_test_us-w    gke_test_us     myapp
*          docker-for-desktop    docker-for-d     docker-for-d
```
Это контексты, о которых kubectl в настоящее время знает. Каждый контекст имеет имя и ссылается на определенный кластер, имя пользователя, который в нем аутентифицирован, и пространство имен внутри этого кластера. Текущий контекст помечен звездочкой * в первом столбце. Если мы сейчас запустим команду kubectl, она будет выполнена в пространстве имен по умолчанию кластера Docker Desktop.

С помощью команды kubectl config use-context можно переключиться на другой контекст:
```
kubectl config use-context gke
```
Контексты — это своего рода закладки, позволяющие легко переходить в определенное пространство имен определенного кластера. Чтобы создать новый контекст, выполните **kubectl config set-context**:
```
kubectl config set-context myapp --cluster=gke --namespace=myapp
```
Вы забыли, в каком контексте находитесь, команда kubectl config currentcontext напомнит:
```
kubectl config current-context
```
Для более быстрого переключения между контекстами kubectl можно использовать инструменты **kubectx** и **kubens**
```
kubectx docker-for-desktop - переключать контексты
kubectx -       - быстрый переход к предыдущему контексту
kubens kube-system - переключаться между пространствами имен
kubens -
```
## Работа с контейнерами

**Контейнеры и pod-оболочки** - Pod-оболочка — это единица планирования в Kubernetes. Pod-объект представляет собой контейнер или группу контейнеров, с его помощью в Kubernetes работают все остальные компоненты.

**Контейнер** — это стандартизированный пакет, который содержит фрагмент программного обеспечения вместе с зависимостями контейнера, его конфигурацией, данными. С точки зрения операционной системы контейнер представляет собой изолированный процесс, который находится в своем собственном пространстве имен. Практика показывает, что лучше поручить контейнеру какую-то одну функцию. У контейнера также есть точка входа — команда, которая запускается при старте. Обычно она приводит к созданию единого процесса для выполнения команды, хотя какие-то приложения запускают несколько вспомогательных или рабочих подпроцессов.

**Pod-объект** представляет собой группу контейнеров, которые должны взаимодействовать и обмениваться данными; планировать, запускать и останавливать их нужно вместе, и находиться они должны на одном и том же физическом компьютере.

**Безопасность контейнеров**

Запускать процессы от имени пользователя root, когда этого не требуется, — плохая идея. Она противоречит принципу минимальных привилегий. Они должны работать от имени обычного пользователя.
```yml
containers:
- name: demo
  image: cloudnatived/demo:hello
  securityContext:
    runAsUser: 1000
```
**Блокирование контейнеров с администраторскими привилегиями**
```yml
securityContext:
  runAsNonRoot: true
```
**Настройка файловой системы только для чтения**
```yml
securityContext:
  readOnlyRootFilesystem: true
```
**Отключение повышения привилегий**
```yml
securityContext:
  allowPrivilegeEscalation: false
```
**Мандаты**

Механизм мандатов в Linux является шагом вперед. Он четко определяет, что программа может делать: загружать модули ядра, напрямую выполнять сетевые операции ввода/вывода, обращаться к системным устройствам
```yml
securityContext:
  capabilities:
    drop: ["CHOWN", "NET_RAW", "SETPCAP"]
    add: ["NET_ADMIN"]
```
Некоторые параметры контекста безопасности можно устанавливать и на уровне pod-оболочки
```yml
spec:
  securityContext:
  runAsUser: 1000
  runAsNonRoot: false
  allowPrivilegeEscalation: false
```
Можно создать для приложения отдельную служебную учетную запись, привязать ее к необходимым ролям и прописать в конфигурации pod-оболочки. Для этого укажите в поле serviceAccountName спецификации pod-оболочки имя служебной учетной записи:
```yml
spec:
  serviceAccountName: deploy-tool
```
**Политики перезапуска** - по умолчанию используется политика перезапуска Always, но вы можете указать вместо нее OnFailure или Never.
```yml
spec:
  restartPolicy: OnFailure
```
**imagePullSecrets** - если вы используете приватный реестр? Как передать Kubernetes учетные данные для аутентификации в таком реестре? Сделать это можно с помощью поля imagePullSecrets в pod-оболочке. Для начала учетные данные необходимо сохранить в объекте Secret, если объект Secret называется registry-creds:
```yml
spec:
imagePullSecrets:
- name: registry-creds
```
## Управление pod-оболочками
**Метки** — это пары типа «ключ — значение», которые назначаются объектам вроде pod-оболочек. Они предназначены для определения значимых для пользователей идентификационных атрибутов объектов, но при этом непосредственно не передают семантику основной системе.

pod-оболочкам (и другим ресурсам Kubernetes) можно назначать метки. Метки существуют для маркирования ресурсов с помощью информации, имеющей смысл для нас, но не для Kubernetes.
```
metadata:
  labels:
    app: demo
```
Настоящую пользу они приносят в сочетании с селекторами.

**Селектор** — выражение, соответствующее метке (или набору меток). Он предоставляет способ указать группу ресурсов по их меткам.
```
selector:
  app: demo
```
Метки применяются не только для соединения сервисов и pod-оболочек, вы можете использовать их напрямую в сочетании с флагом --selector, когда обращаетесь к кластеру с помощью команды kubectl get:
```
kubectl get pods --all-namespaces --selector app=demo
```
Узнать, какие метки назначены вашим pod-оболочкам
```
kubectl get pods --show-labels
```
В большинстве случаев вам будет достаточно простых селекторов наподобие app: demo (их еще называют селекторами равенства). Чтобы сделать селектор более точным, можно объединить несколько разных меток:
```
kubectl get pods -l app=demo,environment=production
```
Команда вернет только те pod-оболочки, у которых есть сразу две метки: app: demo и environment: production.
```
selector:
  app: demo
  environment: production
```
Более сложных ресурсов вроде развертываний существуют и другие варианты. Один из них — селектор неравенства:
```
kubectl get pods -l app!=demo
```
Значения меток, входящих в последовательность:
```
kubectl get pods -l environment in (staging, production)
```
Выбирать по меткам, которые не входят в заданную последовательность:
```
kubectl get pods -l environment notin (production)
```
**Метки** идентифицируют ресурсы. Имена меток не могут быть длиннее 63 символов. Метки должны начинаться только с алфавитно-цифровых символов, могут содержать тире, подчеркивания и точки.

**Аннотации** не предназначены для идентификационной информации и применяются инструментами и сервисами за пределами Kubernetes.

**Принадлежность к узлам**

Kubernetes используются следующие термины:
- requiredDuringSchedulingIgnoredDuringExecution (жесткая);
- preferredDuringSchedulingIgnoredDuringExecution (мягкая).

required (обязательная) относится к жесткой принадлежности (правило должно соблюдаться при планировании pod-оболочки), a preferred (предпочтительная) — к мягкой (было бы здорово, если бы правило соблюдалось, но это не столь важно).

**Жесткая принадлежность** - Принадлежность описывает типы узлов, на которых вы хотите запустить свою pod-оболочку. Вы можете указать несколько правил для платформы Kubernetes, по которым она должна выбирать узлы. Каждое из них выражается в виде поля nodeSelectorTerms.

**Мягкая принадлежность** - Мягкая принадлежность выражается аналогичным образом, за исключением того, что каждому правилу назначается вес в виде числа от 1 до 100, который определяет влияние правила на результат.

**Принадлежность и непринадлежность pod-оболочек**
- Некоторые Pod-объекты работают лучше, будучи размещенными на одном узле
- Некоторым pod-оболочкам лучше не встречаться

**Ограничения и допуски**

Ограничения (taints) позволяют узлу отвергнуть группу pod-оболочек, руководствуясь свойствами в своей спецификации. Ограничения, к примеру, можно использовать для создания выделенных узлов, зарезервированных только для конкретного вида pod-оболочек. Чтобы добавить ограничение для отдельного узла, используйте команду kubectl taint:
```
kubectl taint nodes docker-for-desktop dedicated=true:NoSchedule
```
Это создаст ограничение под названием dedicated=true на узле docker-for-desktop. Результат будет тот же, что и у NoSchedule: там можно будет размещать только pod-оболочки с таким же ограничением. Чтобы **вывести** ограничения, установленные для определенного узла, используйте команду **kubectl describe node....** Чтобы **убрать** ограничение на узле, повторите команду **kubectl taint**, но укажите в конце после его имени **знак «минус»**:
```
kubectl taint nodes docker-for-desktop dedicated:NoSchedule-
```
Допуски (tolerations) — это свойства pod-оболочки, описывающие ограничения, с которыми те совместимы. Например, чтобы pod-оболочка допускала ограничение dedicated=true, добавьте следующий код в ее спецификацию:
```yml
spec:
  tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
```
Данной pod-оболочке разрешается работать на узлах с ограничением dedicated=true, которое имеет тот же эффект, что и NoSchedule. Если в результате ограничения pod-оболочка вообще не может быть развернута, она будет оставаться в состоянии Pending

**Объекты DaemonSet**

**DaemonSet** - специальный контроллер - запускает одну копию, например - данный агент отвечает лишь за соединение с сервисом ведения журнала и передачу ему журнальных записей. Используйте DaemonSet, когда вам нужно запустить по одной копии pod-оболочки на каждом узле кластера.

**Объект StatefulSet**

StatefulSet является разновидностью контроллера pod-оболочек. Его отличительная черта — возможность запускать и останавливать pod-оболочки в определенной последовательности. Если вы, к примеру, создадите объект StatefulSet с именем redis, первая запущенная pod-оболочка будет называться redis-0: Kubernetes подождет, пока она будет готова к работе, и только потом запустит следующую, redis-1. Чтобы запустить следующую реплику в StatefulSet, Kubernetes необходимо убедиться в том, что предыдущая уже готова к работе. При уничтожении StatefulSet реплики останавливаются в обратном порядке, завершая свою работу по очереди. Чтобы вы могли обращаться к pod-оболочкам по предсказуемым доменным именам, таким как redis-1, вам, кроме всего прочего, нужно создать сервис с типом clusterIP, равным None (так называемый неуправляемый сервис). В случае с управляемым сервисом вы получаете единую DNS-запись (такую как redis), которая распределяет нагрузку между всеми внутренними pod-оболочками. Если сервис неуправляемый, вы получаете такое же единое доменное имя, но вместе с этим каждой пронумерованной pod-оболочке выдается отдельная DNS-запись вида redis-0, redis-1, redis-2

**Запланированные задания**

Полезной разновидностью pod-контроллера в Kubernetes является запланированное задание (Job-объект). Запланированные задания запускают Pod-объекты конкретно столько раз, сколько вам нужно. После этого задание считается выполненным. Выполнение задания управляется двумя полями: completions и parallelism. Первое, completions, определяет, сколько раз заданная pod-оболочка должна успешно отработать, прежде чем задание можно будет считать выполненным. По умолчанию значение равно 1, что означает однократное выполнение. Поле parallelism указывает, сколько pod-оболочек должно работать одновременно. Значение по умолчанию равно 1.

**Задания Cronjob**

В манифесте Cronjob есть два важных поля, на которые стоит обратить внимание: spec.schedule и spec.jobTemplate. Поле schedule определяет, когда задание нужно запускать: в нем используется тот же формат, что и в Unix-утилите cron. Поле jobTemplate описывает шаблон для задания, которое нужно запустить.

**Горизонтальное автомасштабирование pod-оболочек**

Horizontal Pod Autoscaler (HPA) наблюдает за указанным развертыванием, отслеживая определенные показатели, и соответственно увеличивает или уменьшает количество реплик. Одним из самых распространенных показателей автомасштабирования является загруженность процессора.

- spec.scaleTargetRef указывает развертывание, которое нужно масштабировать;
- spec.minReplicas и spec.maxReplicas задают границы масштабирования;
- spec.metrics определяет показатели, которые будут использоваться для масштабирования.

**Ресурсы Ingress**

Ingress — это своего рода балансировщик нагрузки, размещенный перед сервисом. Он передает сервису запросы, поступающие от клиентов. Сервисы хорошо подходят для маршрутизации внутреннего трафика вашего кластера. Объекты Ingress, в свою очередь, предназначены для перенаправления внешних запросов к вашему кластеру и подходящему микросервису. Ingress может направлять трафик к разным сервисам, исходя из установленных вами правил.

Cобственный контроллер Ingress внутри своего кластера:
- nginx-ingress - NGINX уже давно стал популярным средством балансировки нагрузки, еще до появления Kubernetes. Контроллер nginx-ingress предоставляет Kubernetes большинство возможностей NGINX.
- Contour - использует внутри себя другой инструмент под названием Envoy, чтобы проксировать запросы между клиентами и pod-оболочками.
- Traefik - Это легковесный прокси, который может автоматически управлять TLS-сертификатами для ваших ресурсов Ingress.

**Istio** — пример технологии, часто называемой межсервисным взаимодействием. Она берет на себя маршрутизацию и шифрование сетевого трафика между сервисами, добавляя такие важные функции, как сбор показателей, ведение журнала и балансировка нагрузки

## Конфигурация и объекты Secret

Kubernetes предоставляет несколько разных способов управления конфигурацией. Во-первых, вы можете передавать значения в приложение через переменные среды, указанные в спецификации pod-оболочки. Во-вторых, конфигурационные данные можно хранить непосредственно в Kubernetes, используя объекты ConfigMap и Secret.

**Объекты ConfigMap** - ConfigMap — это основной объект для хранения конфигурационных данных в Kubernetes. Его можно представить в виде именованного набора пар «ключ — значение», в котором хранится конфигурация. С помощью ConfigMap вы можете предоставлять эти данные приложению, внедряя их в окружение pod-оболочки или создавая в ней соответствующий файл.

**Создание ConfigMap**

Один из вариантов — записать данные в манифесте ConfigMap в том виде, в котором они указаны в YAML:
```yml
apiVersion: v1
data:
  config.yaml: |
    autoSaveInterval: 60
    batchSize: 128
    protocols:
      - http
      - https
kind: ConfigMap
metadata:
  name: demo-config
  namespace: demo
```
Более простой способ: переложить эту работу на kubectl и создать ресурс напрямую из YAML-файла:
```
kubectl create configmap demo-config --namespace=demo --from-file=config.yaml
```
Чтобы экспортировать файл манифеста, соответствующий этому ресурсу ConfigMap, примените такую команду:
```
kubectl get configmap/demo-config --namespace=demo --export -o yaml >demo-config.yaml
```
**Конфиденциальные данные в Kubernetes**

Kubernetes предлагает объект специального типа, предназначенный для хранения конфиденциальных данных: **Secret**.
```yml
apiVersion: v1
kind: Secret
metadata:
  name: demo-secret
stringData:
  magicWord: xyzzy
```
Объект Secret можно сделать доступным в контейнере в виде переменных среды или файла на его диске.
```yml
env:
  - name: GREETING
    valueFrom:
    secretKeyRef:
      name: demo-secret
      key: magicWord
```
**Чтение объектов Secret**
```
kubectl describe secret/demo-secret
```
Данные не отображаются. Объекты Secret в Kubernetes имеют тип Opaque: это означает, что их содержимое не показывается в выводе kubectl describe, журнальных записях и терминале. Чтобы просмотреть закодированную версию конфиденциальных данных в формате YAML, воспользуйтесь командой kubectl get:
```
kubectl get secret/demo-secret -o yaml
```
Получим объект Secret, представленный в кодировке base64. Base64 — это схема кодирова ния произвольных двоичных данных в виде строки символов. Объекты Secret всегда хранятся в формате base64. Расшифровать можно:
```
echo "eHl6enk=" | base64 --decode
```
Зашифровать:
```
echo xyzzy | base64
```
**Доступ к объектам Secret**

Кто может читать и редактировать объекты Secret? Это определяется RBAC — механизмом контроля доступа.

Начиная с версии 1.7, Kubernetes поддерживает пассивное шифрование данных. Это означает, что конфиденциальная информация внутри etcd хранится на диске в зашифрованном виде и не может быть прочитана даже тем, кто имеет прямой доступ к базе данных. Для ее расшифровки нужен ключ, который есть только у сервера API Kubernetes.

Проверить, работает ли пассивное шифрование в вашем кластере, можно таким образом:
```
kubectl describe pod -n kube-system -l component=kube-apiserver |grep encryption
```
**Шифрование конфиденциальных данных с помощью Sops**

Sops (от англ. secrets operations) — это система шифрования/дешифрования, которая поддерживает YAML, JSON и двоичные файлы.

## Безопасность и резервное копирование

**Управление доступом в кластере**

К кластерам Kubernetes обычно обращаются две категории людей: администраторы и разработчики, и их должностные обязанности часто требуют разных прав доступа и привилегий. Кроме того, у вас может быть несколько сред развертывания: например, промышленная и тестовая. В ситуации, когда одна команда не должна иметь доступ к процессу разработки и развертывания другой, каждой из них можно выделить по кластеру с разными учетными данными. Это, безусловно, наиболее безопасный подход, но использование дополнительных кластеров имеет свои недостатки.

**Введение в управление доступом на основе ролей**

Другой способ управлять доступом — контролировать список тех, кто может выполнять определенные операции внутри кластера. Для этого Kubernetes предлагает систему управления доступом на основе ролей (Role-Based Access Control, или RBAC). Система RBAC предназначена для выдачи определенных прав доступа определенным пользователям. Включена ли у вас поддержка RBAC):
```
kubectl describe pod -n kube-system -l component=kube-apiserver
```
Если флаг --authorization-mode не содержит значение RBAC, значит, RBAC в вашем кластере не работает. Без RBAC любой, кто имеет доступ к кластеру, может делать что угодно: может даже запустить произвольный код или удалить рабочие задания.

**Понимание ролей**

Всякий раз, подключаясь к Kubernetes, вы делаете это от имени определенного пользователя. У всех пользователей могут быть разные права доступа. Это определяется ролями Kubernetes. Роль описывает определенный набор прав доступа. Например, роль cluster-admin, предназначенная для администраторов, имеет право на чтение и изменение любого ресурса в кластере. Для сравнения, роль view выводит списки объектов в заданном пространстве имен и просматривает их по отдельности, но не может их изменять. Пример манифеста ClusterRole, который выдает доступ к конфиденциальным данным в любом пространстве имен:
```yml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: secret-reader
rules:
  - apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]
```
**Привязка ролей к пользователям** - Это соединить пользователя и роль. Манифест RoleBinding, который назначает пользователю daisy роль edit, но лишь в пространстве имен demo:
```yml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: daisy-edit
  namespace: demo
subjects:
- kind: User
  name: daisy
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: edit
  apiGroup: rbac.authorization.k8s.io
```
В Kubernetes права доступа являются добавочными: вначале пользователи не имеют никаких прав и добавляют их с помощью объектов Role и RoleBinding. **Если права уже выданы, их нельзя забрать.** Какими правами доступа обладает та или иная роль, используйте команду kubectl describe: 
```
kubectl describe clusterrole/edit
```
Если вашему приложению зачем-то нужен доступ к Kubernetes API (например, если речь об инструменте мониторинга, которому необходим список pod-оболочек), создайте для него отдельную служебную запись, свяжите ее с необходимой ролью (например, view) с помощью RoleBinding и ограничьте определенными пространствами имен.

Роль **edit** - Пользователи с такой ролью могут создавать и удалять ресурсы в своем пространстве имен, однако им запрещено создавать новые роли и выдавать права доступа другим пользователям.

У людей, которым не нужно развертывать приложения, по умолчанию должна быть только роль **view**.

**Сканирование безопасности**

**Clair** (https://github.com/coreos/clair) — это открытый сканер контейнеров, разработанный в рамках проекта CoreOS. Он статически анализирует образы контейнеров перед их запуском, чтобы убедиться в том, что они не содержат никакого ПО или своих версий с известными уязвимостями.

**Aqua** Security Platform от компании Aqua (www.aquasec.com/products/aqua-cloud-nativesecurity-platform) — это коммерческое решение с полным набором услуг. Оно по-зволяет организациям сканировать контейнеры на уязвимости, вредоносный код и подозрительную активность, а также

**Anchore Engine** (github.com/anchore/anchore-engine) — это инструмент с открытым исходным кодом не только для сканирования образов контейнеров на известные уязвимости, но и для идентификации всего, что присутствует в контейнере

**Резервное копирование**

**Velero** — это бесплатный и открытый инструмент (ранее известный как Ark), умеющий сохранять и восстанавливать состояние вашего кластера и постоянные данные.

**Мониторинг состояния кластера**

**Состояние управляющего уровня**

Команда **kubectl get componentstatuses** (сокращенно kubectl get cs) выводит сведения о работоспособности компонентов управляющего уровня — планировщика, диспетчера контроллеров и etcd:
```
kubectl get componentstatuses
```
**Состояние узла**

Еще одна полезная команда — kubectl get nodes — выводит список всех узлов в вашем кластере, включая их состояние и версию Kubernetes:
```
kubectl get nodes
kubectl get pods --all-namespaces - список всех pod
```
Если контейнер откажет, Kubernetes периодически будет пытаться его перезапустить: начнет через 10 секунд, с каждой попыткой увеличивая интервал вдвое, пока тот не достигнет пяти минут. Это называется экспоненциальной выдержкой, отсюда и состояние CrashLoopBackOff.

**Загруженность процессора и памяти**
```
kubectl top nodes
kubectl top pods -n kube-system
```
**Kubernetes Dashboard** - это пользовательский веб-интерфейс для кластеров Kubernetes

**Weave Scope** — это отличный инструмент для визуализации и мониторинга, который показывает вам узлы, контейнеры и процессы в режиме реального времени.

**kube-ops-view** - визуализирует то, что происходит в вашем кластере, и показывает, какие в нем есть узлы, сколько ресурсов процессора и памяти занято, сколько pod-оболочек и с каким состоянием выполняется на каждом из узлов

**node-problem-detector** — дополнение для Kubernetes, способное обнаруживать проблемы на уровне узлов и сообщать о них. Речь идет об аппаратных сбоях, таких как ошибки работы процессора или памяти, повреждение файловой системы и зависание среды выполнения контейнера.

## Развертывание приложений Kubernetes

Нужна возможность отделять необработанные файлы манифестов от определенных параметров и переменных, которые могут потребовать модификации с вашей стороны или со стороны любых других пользователей. В идеале все это можно было бы сделать доступным в стандартном формате.

Каждый чарт имеет стандартную структуру. Прежде всего, он находится внутри каталога с таким же именем:
```
demo
├── Chart.yaml
├── production-values.yaml
├── staging-values.yaml
├── templates
│ ├── deployment.yaml
│ └── service.yaml
└── values.yaml
```
Chart.yaml, который определяет имя и версию чарта (имя и версия - обязательны):
```yml
name: demo
sources:
  - https://github.com/cloudnativedevops/demo
version: 1.0.1
```
values.yaml, содержащий параметры, которые автор чарта позволяет менять пользователям, ограничен лишь форматом YAML и не имеет никакой предопределенной структуры.:
```yml
environment: development
container:
  name: demo
  port: 8888
  image: cloudnatived/demo
  tag: hello
replicas: 1
```
Шаблоны/templates Helm - Это такие же файлы манифестов для развертывания и сервиса,только теперь они являются шаблонами. Вместо того чтобы напрямую что-то упоминать — например, имя контейнера, — шаблоны содержат заглушки, которые Helm заменит настоящими значениями из values.yaml.

**Задание зависимостей** - в файле requirements.yaml:
```yml
dependencies:
  - name: redis
    version: 1.2.3
```
**helm dependency update** - Helm загрузит чарты и подготовит их к установке вместе с вашим собственным приложением.

**Продвинутые инструменты управления манифестами**

**ksonnet** (ksonnet.io) позволяет писать манифесты Kubernetes на языке под названием Jsonnet, который является расширенной версией JSON. Jsonnet расширяет возможности JSON: позволяет работать с переменными, циклами, условными выражениями, выполнять арифметические операции, обрабатывать ошибки и т. д. Самое важное то, что ksonnet вводит концепцию прототипов: заранее подготовленных наборов ресурсов Kubernetes, с помощью которых можно «штамповать» часто используемые шаблоны манифестов.

**kapitan** (github.com/deepmind/kapitan) — это еще один инструмент на основе Jsonnet, предназначенный для использования общих конфигурационных значений в разных приложениях или даже кластерах.

**kustomize** (github.com/kubernetes-sigs/kustomize) — это еще один инструмент для работы с манифестами, который использует стандартный формат YAML вместо шаблонов или альтернативных языков наподобие Jsonnet. Вы начинаете с базового манифеста, на который накладываются дополнительные слои, чтобы подогнать его под разные среды или конфигурации.

**kompose** (github.com/kubernetes/kompose) — инструмент для преобразования dockercompose.yml в манифесты Kubernetes. Он позволяет мигрировать из Docker Compose без необходимости писать с нуля собственные манифесты Kubernetes или чарты Helm.

**kubeval** лучше использовать в рамках процесса непрерывного развертывания, чтобы манифесты проверялись всякий раз, когда вы вносите в них изменения. kubeval также позволяет, к примеру, перед обновлением до последней версии Kubernetes проверить, требуют ли ваши манифесты какой-либо модификации для работы с ней.

## Процесс разработки

**Стратегии развертывания**

При наличии нескольких реплик их лучше обновлять по очереди, чтобы не допускать прерывания в обслуживании: это так называемое развертывание без простоя. В Kubernetes вы можете выбрать наиболее подходящую вам стратегию. RollingUpdate исключает простаивание и обновляет pod-оболочки одну за другой, тогда как Recreate позволяет работать быстрее и со всеми pod-оболочками одновременно. В Kubernetes стратегия развертывания приложения определяется в манифесте ресурса Deployment. По умолчанию используется RollingUpdate, поэтому, если все оставить как есть, вы получите именно ее. Поменять на Recreate можно так:
```
apiVersion: extensions/v1beta1
kind: Deployment
spec:
  replicas: 1
  strategy:
    type: Recreate
```
**Плавающие обновления RollingUpdate** - Если выбрать RollingUpdate, pod-оболочки будут обновляться по очереди до тех
пор, пока все реплики не перейдут на новую версию.

**Стратегия Recreate** - В режиме Recreate все запущенные реплики удаляются одновременно, а потом создаются новые.
Иногда, пока выкатывается обновление, у вас может оказаться больше или меньше pod-оболочек, чем указано в поле replicas. Такое поведение регулируется двумя важными параметрами: maxSurge и maxUnavailable:
- maxSurge задает максимальное количество лишних pod-оболочек. Например, если у вас есть десять реплик, а параметр maxSurge равен 30 %, система не допустит, чтобы число одновременно запущенных Pod-объектов превысило 13;
- maxUnavailable задает максимальное количество недоступных pod-оболочек. Если номинальное число реплик равно десяти, а параметр maxUnavailable равен 20 %, Kubernetes никогда не позволит, чтобы количество доступных Pod- объектов опустилось ниже восьми.
```yml
apiVersion: extensions/v1beta1
kind: Deployment
spec:
  replicas: 10
  strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 20%
    maxUnavailable: 3
```
**Сине-зеленые развертывания** - Когда происходит сине-зеленое обновление, заменяются не отдельные Pod-объекты, а целое развертывание: создается новый объект Deployment с самостоятельным стеком pod-оболочек версии v2, который запускается рядом с существующим развертыванием версии v1.

**Rainbow-развертывания** - Иногда приходится поддерживать одновременно три активные версии приложения и больше. Такой подход иногда называют rainbow-развертыванием: при каждом обновлении вы получаете набор pod-оболочек другого цвета. Дальше вы можете останавливать старые версии по мере того, как они закрывают свои соединения.

**Канареечные развертывания** - Если pod-оболочки «выживают», выкатывание можно продолжать до полного завершения. Но если проблема все же возникает, ее масштаб строго ограничен.

## Наблюдаемость и мониторинг

Когда речь идет о мониторинге в контексте DevOps, в основном имеется в виду автоматизированный мониторинг. Автоматизированный мониторинг — это проверка доступности и поведения сайта или сервиса каким-то программным способом, обычно по расписанию и с поддержкой автоматического оповещения инженеров в случае возникновения проблемы

Более продуманная проверка могла бы искать на странице определенный текст, такой как Cloude Native DevOps. Это позволило бы обнаружить проблему с неправильно сконфигурированным, но рабочим веб-сервером.

Какими бы сложными ни были эти проверки, все они относятся к одной категории: мониторинг методом черного ящика. Как можно догадаться из названия, такой мониторинг наблюдает только за внешним поведением системы, не пытаясь выяснить, что происходит внутри.

Но у подобных проверок есть свои недостатки.
- Они могут обнаружить только предсказуемые сбои (например, сайт перестал отвечать).
- Их можно применять только к тем частям системы, которые доступны извне.
- Они являются пассивными и реактивными: сообщают о проблеме только после того, как что-то случилось.
- Они могут ответить на вопрос «Что сломалось?», но не могут объяснить почему, а это более важно.

**Что означает «работает»**

В системном администрировании устойчивость и доступность приложений привыкли измерять временем доступности и выражать это в процентах. Время доступности 99,9 % называют тремя девятками: это значит, что система простаивает примерно девять часов в год.

Если для пользователей сервис не работает, ваши внутренние показатели не имеют значения — сервис недоступен. Приложение может не удовлетворять пользователей и по множеству других причин, даже если формально оно работает. Возьмем очевидный пример: что, если на загрузку вашего сайта уходит десять секунд?

Мониторинг методом черного ящика является хорошим первым шагом на пути к достижению наблюдаемости, но важно понимать, что на этом нельзя останавливаться. Стоит поискать лучший метод.

**Ведение журнала**

Большинство приложений генерируют разного рода журнальные записи, как правило, с временными метками, которые указывают на то, когда и в каком порядке записи были сохранены.  Столкнувшись с ошибкой, приложение обычно записывает этот факт в журнал и добавляет сведения, которые могут помочь системным администраторам разобраться в причинах появления ошибки. Журнальные записи отвечают только на те вопросы и обнаруживают только те проблемы, которые можно предсказать заранее. Извлечение информации из журналов может оказаться непростой задачей, так как каждое приложение ведет их в своем формате. Поскольку информации, которая записывается в журнал, должно быть достаточно для диагностики всевозможного рода проблем, обычно она имеет довольно высокий уровень шума. Если же в журнал попадают только ошибки, вам будет сложно понять, как выглядит нормальная работа. Журнальные записи не очень хорошо масштабируются в соответствии с изменением трафика. Если каждый пользовательский запрос будет генерировать строчку в журнале, которую надо отправить агрегатору, на это может уйти большая часть пропускной способности вашей сети. Журналы не могут дать вам всю информацию, необходимую для обеспечения настоящей наблюдаемости. Для этого нужно пойти дальше и обратиться к чему-то значительно более мощному.

**Введение в показатели*

Более сложный способ сбора информации о сервисах заключается в использовании показателей. Как понятно из названия, показатель — это числовая мера чего-либо. В зависимости от приложения вам могут пригодиться следующие показатели:
- количество запросов, которые обрабатываются в данный момент;
- количество запросов, обрабатываемых в минуту (секунду, час);
- количество ошибок, возникающих при обработке запросов;
- среднее время обслуживания запросов (или пиковое время, или 99-й перцентиль).

Полезно иметь сведения не только о приложениях, но и о таких аспектах работы вашей инфраструктуры, как:
- процессорные ресурсы, занятые отдельными процессами или контейнерами;
- активность дискового ввода/вывода на узлах и серверах;
- входящий и исходящий сетевой трафик компьютеров, кластеров или баланси- ровщиков нагрузки.

Показатели выводят мониторинг на новый уровень, за пределы логики вида «работает/не работает». По аналогии со спидометром в машине или температурной шкалой в термометре они дают числовую информацию о том, что происходит. Показатели также могут помочь с поиском причин возникновения проблем. Показатели бывают и прогностическими: обычно проблемы не возникают в один миг. Прежде чем проблема станет заметной вам или вашим пользователям, на ее скорое появление может указать всплеск какого-либо показателя. Показатели дают возможность разработчикам приложения извлекать информацию о скрытых аспектах системы, основываясь на своем понимании того, как она на самом деле работает

**Трассировка**

Еще одной полезной методикой мониторинга является трассировка. Трассировка отслеживает отдельно взятый пользовательский запрос на протяжении всего его жизненного цикла. Если трассировать отдельный (желательно репрезентативный) запрос от момента открытия пользовательского соединения до момента его закрытия, вы получите картину того, из чего состоит задержка выполнения на каждом этапе путешествия запроса по системе.

**Наблюдаемость**

Поскольку термин «мониторинг» может иметь разное значение для разных людей — от старых добрых проверок методом черного ящика до сочетания показателей, ведения журнала и трассировки — все эти методики в совокупности все чаще называют наблюдаемостью. Наблюдаемость — это совокупность методов и процес- сов, помогающих узнать, насколько хорошо система оснащена и насколько просто можно определить, что происходит внутри нее. Мониторинг определяет, работает ли система, тогда как наблюдаемость предлагает задаться вопросом, почему она не работает.

В более общем смысле наблюдаемость заключается в понимании: понимании того, что делает ваша система и как она это делает. Например, если выкатить изменение, призванное улучшить производительность конкретной функции на 10 %, то наблюдаемость покажет, удалось ли этого достичь. Наблюдаемость также имеет прямое отношение к данным. Вам следует знать, какие данные генерировать, что собирать, как это агрегировать (если есть необходимость), на каких результатах нужно сосредоточиться и как их запрашивать/отображать.

Процесс наблюдаемости имеет огромные преимущества. Благодаря ему источник данных можно добавить, просто подключив к вашему конвейеру. Аналогично новый сервис для визуализации или оповещений становится с точки зрения конвейера лишь еще одним потребителем. Применение процесса наблюдаемости требует стандартного формата показателей и в идеале структурированного ведения журналов приложений с помощью JSON или любого другого формата сериализованных данных.

## Показатели в Kubernetes

Показатели — это числовая мера определенных вещей. Одним из видов ценной информации, которую нам могут дать
показатели, является снимок того, что происходит в конкретный момент. Но мы можем пойти дальше - нас может заинтересовать изменение этого значения во времени. Если сведения о потреблении памяти собираются регулярно, вы можете построить из них временной ряд. Большинство показателей, имеют хронологический характер. Кроме того, они являются числовыми. C показателями можно производить математические и статистические операции.

С учетом этого показатели в основном бывают двух разновидностей: счетчики и измерители. Счетчики могут только увеличиваться (или сбрасываться в ноль) и подходят для измерения таких характеристик, как количество обработанных запросов или число полученных ошибок. Измерители же могут меняться в обе стороны и подходят для постоянно колеблющихся величин (например, для потребления памяти) или для выражения соотношений других значений.

По показателям можно определить, что проблема есть. Например, если частота ошибок внезапно увеличивается. Показатели также могут сказать вам о том, насколько хорошо все работает: например, сколько пользователей ваше приложение может выдержать одновременно.

**Сервисы: шаблон RED**

Большинство людей, использующих Kubernetes, запускают разного рода вебсервисы. Что следует знать о такой системе?
- Одним из очевидных ответов является **количество запросов**, которые вы получаете.
- Еще следует обращать внимание на **количество** запросов, которые по разным причинам оказались **неудачными**, — то есть на число ошибок.
- И третьим полезным показателем является **продолжительность каждого запроса**.

Шаблон RED (**Requests-Errors-Duration** — «запросы — ошибки — продолжительность») — это классическая концепция наблюдаемости, которую начали применять еще на самых ранних этапах развития онлайн-сервисов

**Ресурсы: шаблон USE**

USE означает Utilization, Saturation и Errors (использование, степень загруженности и ошибки). Этот шаблон предназначен для ресурсов — компонентов физических серверов, таких как процессор и диски, или сетевых интерфейсов и каналов. Любой из них может оказаться узким местом
- **Эффективность использования**. Среднее время использования ресурса для обслуживания запросов или доля емкости ресурса, которая сейчас занята.
- **Насыщенность**. Степень перегруженности ресурса или длина очереди запросов, которые ждут, когда ресурс станет доступным. Например, если выполнения на процессоре ждут десять процессов, степень загруженности процессора будет равна десяти.
**Ошибки**. Сколько раз завершилась неудачей операция с этим ресурсом. Например, у диска с неисправными секторами количество ошибок может равняться 25 неудачным операциям чтения.

Все, что отклоняется от нормы, заслуживает вашего внимания. Метод USE — это простая стратегия, с помощью которой вы можете произвести полную проверку работоспособности системы и найти типичные узкие места и ошибки.

**Бизнес-показатели**

Приложения и сервисы тоже могут сгенерировать полезные бизнес-показатели. Например, если у компании есть продукт, доступный в виде подписки, полезной будет следующая информация о его подписчиках.
- Анализ воронки продаж (сколько людей заходит на целевую страницу, сколько щелчков кнопкой мыши нужно, чтобы дойти до страницы регистрации, сколько посетителей завершили транзакцию и т. д.).
- Соотношение регистраций и отмен (коэффициент оттока).
- Доход от каждого клиента.
- Эффективность страниц помощи и поддержки.
- Трафик к странице состояния системы (всплеск которого часто наблюдается во время перебоев в работе или ухудшения качества обслуживания).

**Показатели Kubernetes**

На самом низком уровне находится инструмент под названием cAdvisor, который наблюдает за использованием ресурсов и собирает статистику производительности для контейнеров, запущенных на каждом узле кластера: например, сколько ресурсов процессора, памяти и дискового пространства использует контейнер. cAdvisor является частью Kubelet. Вы также можете мониторить и саму платформу Kubernetes, используя инструмент под названием kube-state-metrics. Он следит за Kubernetes API и генерирует информацию о таких логических объектах, как узлы, pod-оболочки и развертывания

**Показатели работоспособности кластера**
- количество узлов;
- состояние работоспособности узла;
- среднее количество pod-оболочек на одном узле и в целом;
- среднее выделение/использование ресурсов на одном узле и в целом

**Показатели развертывания**
- количество развертываний;
- количество сконфигурированных реплик на каждом развертывании;
- количество недоступных реплик на каждом развертывании

**Показатели контейнера**
- среднее количество контейнеров/pod-оболочек на одном узле и в целом;
- соотношение потребленных ресурсов и запросов/лимитов контейнера
- работоспособность/готовность контейнеров;
- количество перезапусков контейнеров/pod-оболочек;
- входящий/исходящий трафик и ошибки для каждого контейнера.

**Показатели приложения** (например если ваш сервис потребляет сообщения из очереди, обрабатывает и предпринимает на их основе какие-то действия)
- количество полученных сообщений;
- количество успешно обработанных сообщений;
- количество некорректных или ошибочных сообщений;
- время, необходимое для обработки каждого сообщения и реакции на него;
- количество успешных действий, которые были сгенерированы;
- количество неудачных действий.

если ваше приложение в основном ориентировано на запросы
- сколько запросов получено;
- сколько ошибок возвращено;
- продолжительность (время обработки каждого запроса).

**Показатели среды выполнения**
- количество процессов/потоков/горутин;
- использование кучи и стека;
- использование памяти вне кучи;
- пулы буферов сетевого ввода/вывода;
- периоды работы и бездействия сборщика мусора (для языков, где таковой имеется);
- количество используемых файловых дескрипторов или сетевых сокетов

Данные — это не то же самое, что информация. Чтобы получить полезную информацию из собранных необработанных данных, их нужно агрегировать, обработать и проанализировать — это значит собрать статистику. Одной из важных проблем является то, что такая величина легко искажается из-за выбросов: одного или нескольких крайних значений, которые сильно влияют на средний показатель. Для усреднения показателей лучше подходит медиана.

Обсуждая показатели для наблюдения за системами, ориентированными на запросы, мы обычно заинтересованы в том, чтобы узнать, каков наихудший показатель времени отклика для пользователей, а не ее среднее значение. Чтобы получить подобную информацию, можно разбить данные на перцентили. Время отклика с 90-м перцентилем (часто обозначают как P90) — значение, которое выше, чем у 90 % ваших пользователей.

**Обычно нас интересуют наихудшие показатели** - Поскольку люди чаще всего заостряют свое внимание на медленных веб-запросах, данные P99, скорее всего, дадут более реалистичную картину латентности, с которой пользователи имеют дело. Представьте, к примеру, высоконагруженный сайт с миллионом просмотров страниц в день. Если время отклика P99 составляет десять секунд, следовательно, 10 000 запросов к страницам обрабатывались дольше десять секунд. А это означает много недовольных пользователей. Перцентиль и так является усреднением, и, усредняя средние значения, мы попадаем в известную статистическую ловушку. Результат может отличаться от реального среднего - лучше следить за отклонениями от нормы, чем за самой нормой.

Очень полезно иметь информацию о том, что может сломаться. Каждый раз, когда происходит какое-то происшествие или перебой в работе, обращайте внимание на показатели или их комбинации, которые могли бы заранее предупредить об этой проблеме.

**Уведомления о показателях**

Уведомления указывают на какое-то неожиданное отклонение от стабильного рабочего состояния. Как уже упоминалось, крупномасштабные распределенные системы никогда не являются полностью рабочими, они почти непрерывно находятся в состоянии частичной деградации качества обслуживания. Уведомление должно означать одну очень простую вещь: прямо сейчас требуются действия со стороны человека. Если действовать не нужно, не нужны и уведомления. Если что-то следует предпринять, но не сейчас, уведомление можно послать по электронной почте или в чате. Если проблема имеет реальное или потенциальное воздействие на рабочий процесс и меры должны быть предприняты незамедлительно живым человеком, уведомления желательно получать. Если уведомление появилось, не во всех случаях его стоит кому-то отсылать: только в ситуациях, когда проблемы являются неотложными, важными или требующими принятия мер.

**Prometheus**

Стандартным решением де-факто для работы с показателями в облачно-ориентированном мире является использование Prometheus. Prometheus — это система мониторинга и набор инструментов для создания уведомлений с открытым исходным кодом на основе хронологических показателей. Prometheus можно установить в кластер Kubernetes одной командой, используя стандартный чарт Helm. После этого она начнет автоматически собирать показатели вашего кластера и любых приложений на ваш выбор.
