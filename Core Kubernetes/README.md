# Почему появился Kubernetes

**Kubernetes** – это платформа с открытым исходным кодом для размещения контейнеров и определения прикладных API для управления облачной семантикой обеспечения этих контейнеров хранилищами данных, сетевыми услугами, поддержкой безопасности и другими ресурсами. Kubernetes обеспечивает непрерывную синхронизацию всего пространства состояний ваших приложений, в том числе способов доступа к ним из внешнего мира.

**Основыне теримины:**
- CNI (Container Networking Interface) и CSI (Container Storage Interface) – сетевой интерфейс контейнеров и интерфейс хранилища для контейнеров соответственно; позволяют подключать к сетям и хранилищам модули Pod (с контейнерами), работающие в Kubernetes;
- контейнер (Container) – образ Docker или OCI (Open Container Initiative), который обычно запускает приложение;
- плоскость управления (Control plane) – мозг кластера Kubernetes, осуществляющий планирование контейнеров и управляющий всеми объектами Kubernetes (которые иногда называют мастеробъектами);
- набор демонов (DaemonSet) – аналог развертывания (Deployment), но выполняется на каждом узле кластера;
- развертывание (Deployment) – набор модулей, которыми управ- ляет Kubernetes;
- kubectl – инструмент командной строки для взаимодействия с панелью управления Kubernetes;
- kubelet – агент Kubernetes, работающий на узлах кластера. Обеспечивает поддержку плоскости управления;
- узел (Node) – машина, на которой запущен процесс kubelet;
- OCI (Open Container Initiative) – общий формат образа для создания выполняемых автономных приложений. Также называется образами Docker;
- Pod (модуль) – объект Kubernetes, инкапсулирующий действующий контейнер.

Kubernetes дает возможность централизовать управление пространством состояний всех приложений с использованием одного удобного инструмента: kubectl – клиента командной строки, выполняющего вызовы REST API к серверу Kubernetes API.

Kubernetes использует привилегированный контейнер в среде Linux, он может управлять правилами iptables для организации маршрутизации трафика к приложениям, что, собственно, и делает прокси-сервер Kubernetes Service - kube-proxy. Контейнеры оказываются фундаментальным примитивом и для запуска приложений, и для управления инфраструктурой, которые запускают сервисы, необходимые приложениям (специализированные хранилища или брандмауэры с определенными настройками), и, что особенно важно, сами приложения. Kubernetes практически бесспорно считается современным стандартом для организации и запуска контейнеров в любом облачном окружении, на сервере или в центре обработки данных.

### Контейнеры и образы

Docker можно рассматривать как способ запуска контейнеров, где контейнер – это работающий образ OCI. Спецификация OCI – это стандартный способ определения образа, который может быть запущен такой программой, как Docker, и в конечном счете представляет собой архив с различными слоями. Контейнеры добавляют слой изоляции, устраняющий необходимость управления библиотеками на сервере или предварительной загрузки инфраструктуры другими зависимостями приложений. Использование контейнеров немыслимо без автоматизации, и именно этой цели служит Kubernetes.

### Базовая основа Kubernetes

Все сущее в Kubernetes определяется в виде простых текстовых файлов в формате YAML или JSON, и платформа запускает образы OCI декларативным способом. Kubernetes позволяет определить желаемое состоя- ние всех приложений в кластере, их подключение к сети, место работы, используемое хранилище и т. д., делегируя базовую реализацию этих деталей самой платформе Kubernetes.

Традиционные правила инфраструктуры:
- конфигурация портов или IP-маршрутов;
- постоянная доступность хранилища для приложений;
- размещение программного обеспечения на определенных или произвольных серверах;
- обеспечение безопасного доступа приложений друг к другу с использованием, например, RBAC или сетевых правил;
- конфигурация DNS для каждого приложения и глобально.

Все эти компоненты определяются в конфигурационных файлах, представляющих объекты в Kubernetes API. Kubernetes использует эти стандартные блоки и контейнеры, применяет изменения, отслеживает эти изменения и устраняет сбои или нарушения, пока не будет достигнуто желаемое конечное состояние.

### Возможности Kubernetes

Платформы оркестрации контейнеров позволяют разработчикам автоматизировать процесс запуска экземпляров, подготовки хостов, связывания контейнеров для оптимизации процедур оркестрации и продления жизненных циклов приложений. 

Основные возможности платформы оркестрации контейнеров:
- предоставления доступа, не зависящего от используемой облачной технологии, ко всем возможностям сервера API;
- интеграции со всеми основными облачными платформами и гипервизорами в диспетчере контроллеров Kubernetes (Kubernetes Controller Manager, KCM);
- обеспечения отказоустойчивости для хранения и определения состояния всех сервисов, приложений и конфигураций центров обработки данных или других инфраструктур, поддерживаемых Kubernetes;
- управления развертыванием, чтобы минимизировать время простоя отдельных узлов, сервисов или приложений;
- автоматизации масштабирования хостов и приложений с поддержкой постоянного обновления;
- создания внутренних и внешних соединений (известных как типы ClusterIP, NodePort или LoadBalancer Service) с балансировкой нагрузки;
- предоставления возможности планирования запуска приложе- ний на определенном виртуализированном оборудовании на основе его метаданных с помощью маркировки узлов и планировщика Kubernetes;
- обеспечения высокой доступности с помощью DaemonSets и других технологических инфраструктур, в которых приоритет
отдается контейнерам, работающим на всех узлах кластера;
- обнаружения сервисов через службу доменных имен, ранее реализованную как KubeDNS, а совсем недавно – CoreDNS, которая интегрируется с сервером API;
- запуска пакетных процессов (известных как задания), которые используют хранилище и контейнеры так же, как обычные приложения;
- расширения API и создания собственных программ, управляемых API, с помощью пользовательских определений ресурсов и без создания каких-либо сопоставлений портов или подключений;
- проверки сбойных процессов на уровне кластера, включая удаленное выполнение в любом контейнере в любое время с помощью kubectl exec и kubectl describe;
- подключения локального и/или удаленного хранилища к контейнеру и декларативного управления томами хранилища с помощью StorageClass API и PersistentVolumes.

Если вам не нужны высокая доступность, масштабируемость и оркестрация, то, возможно, вам не нужна Kubernetes.

### Компоненты и архитектура Kubernetes
- аппаратная инфраструктура – включает компьютеры, сетевую инфраструктуру, инфраструктуру хранения и реестр контейнеров;
- рабочие узлы Kubernetes – базовая вычислительная единица в кластере Kubernetes;
- плоскость управления Kubernetes – основа Kubernetes. Она включает сервер API, планировщика, диспетчера контроллеров и другие контроллеры.

### Kubernetes API

Администрирование микросервисов и других контейнерных приложений на платформе Kubernetes сводится к объявлению объектов Kubernetes API. Сервер Kubernetes API – kube-apiserver – позволяет выполнять CRUD-операции (Create, Read, Update, Delete – создавать, читать, обновлять, удалять) со всеми объектами и предоставляет интерфейс передачи репрезентативного состояния RESTful (REpresentational State Transfer).

Все объекты Kubernetes API имеют:
- именованную версию API (например, v1 или rbac.authorization. k8s.io/v1);
- тип (например, kind: Deployment);
- раздел метаданных.

Существует около 70 различных типов API, просмотреть их командой **kubectl api-resources**.

**Когда не стоит использовать Kubernetes**
- высокопроизводительные вычисления (High-Performance Comput- ing, HPC) – контейнеры добавляют дополнительные сложности, а наличие нового уровня бьет по производительности.
- унаследованные приложения – некоторые приложения имеют требования к оборудованию, программному обеспечению и задержке, что затрудняет их контейнеризацию.
- миграция – реализации унаследованных систем могут быть на- столько жесткими, что их миграция в Kubernetes не дает особых преимуществ

# Зачем нужны модули Pod?

Pod – это наименьшая атомарная единица, которую можно развернуть в кластере Kubernetes. Определение Pod описывает модуль, способный включать несколько контейнеров, что позволяет Kubernetes создать несколько контейнеров на узле. Многие другие объекты Kubernetes API либо используют Pod напрямую, либо являются объектами API, поддерживающими Pod. Некоторые контроллеры Kubernetes высокого уровня запускают модули Pod и управляют ими.

### Что такое Pod?

Pod – это модуль, содержащий один или несколько образов OCI, которые выполняются в контейнерах на одном узле кластера Kubernetes. Узел Kubernetes – это отдельная единица вычислительной инфраструктуры (сервер), на которой выполняется kubelet.
```yml
apiVersion: v1
kind: Pod
metadata:
spec:
  container:
    - name: busybox
      image: mycontainerregistry.io/foo
```
**kubectl create -f pod.yaml**

Команда kubectl – это двоичный выполняемый файл, реализующий интерфейс командной строки к серверу Kubernetes API.

**kubectl get po** - посмотреть Pod

### Пространства имен в Linux

Пространства имен Linux – это функция ядра Linux, позволяющая разделять процессы. Pod имеет следующие пространства имен Linux:
- одно или несколько пространств имен PID;
- единое сетевое пространство имен;
- пространство имен IPC;
- пространство имен cgroup (управляющая группа);
- пространство имен mnt (монтирование);
- пространство имен user (ID пользователя).

Пространства имен Linux – это компоненты файловой системы ядра Linux, обеспечивающие базовую функциональность для получения образа и создания работающего контейнера.

### Kubernetes, инфраструктура и Pod

Единица вычислительной мощности в Kubernetes представлена объектом узла Node. Некоторые требования к узлу:
-  сервер;
-  установленная операционная система (ОС), Linux или Windows, с необходимыми зависимостями;
- systemd (диспетчер системных служб, в Linux);
- kubelet (агент узла);
- среда выполнения контейнеров (например, Docker);
- сетевой прокси-сервер (kube-proxy), обслуживающий сервисы Kubernetes;
- провайдер сетевого интерфейса контейнеров (Container Network Interface, CNI).

**kubelet** – это двоичная программа, играющая роль агента и взаимодействующая с сервером Kubernetes API посредством поддержки цикла управления. Она работает на каждом узле; без этой программы узел Kubernetes недоступен планировщику и не может считаться частью кластера.

kubelet гарантирует:
- запуск любых модулей Pod, запланированных для выполнения на данном хосте механизмом управления, который следит за распределением модулей Pod между узлами;
- регулярное уведомление сервера API об исправной работе kubelet отправкой контрольных сообщений (Kubernetes 1.17+) с помощью механизма в пространстве имен kube-node-lease кластера;
- своевременное освобождение ресурсов, выделенных для Pod, включая эфемерные хранилища или сетевые устройства.

**kubelet не может выполнять свои обязанности без провайдера CNI и среды выполнения**, доступной через интерфейс среды выполнения контейнеров (Container Runtime Interface, CRI). CNI обслуживает потребности CRI, который затем запускает и останавливает контейнеры. **kubelet использует CRI и CNI для согласования состояния узла с состоянием плоскости управления**.

**Сервис (Service)** – это объект API, определяемый платформой Kubernetes. Двоичный файл сетевого прокси Kubernetes (kube-proxy) создает на каждом узле сервисы ClusterIP и NodePort. Вот некоторые типы сервисов:
- ClusterIP – внутренний балансировщик, распределяющий нагрузку между модулями Pod в кластере Kubernetes;
- NodePort – открытый порт на узле Kubernetes, распределяющий нагрузку между несколькими модулями Pod;
- LoadBalancer – внешний сервис, создающий балансировщик нагрузки, внешний по отношению к кластеру.

### Объект Node

Узлы поддерживают модули Pod, а плоскость управления определяет группу узлов, на которых работают контроллеры, диспетчер контроллеров и планировщик. Получить список узлов кластера - **kubectl get no**

Объект Node, описывающий узел, где размещена плоскость управления Kubernetes - **kubectl get no kind-control-plane -o yaml**

Модули Pod реализуют возможность развертывания образов. Образы развертываются на узле, а их жизненным циклом управляет kubelet. Объекты сервисов управляются сетевым прокси Kubernetes. Сетевой прокси Kubernetes обеспечивает возможность балансировки нагрузки внутри кластера, а также аварийное переключение, обновление, высокую доступность и масштабирование. Комбинация из пространства имен mnt в Linux, агента kubelet и объекта узла Node позволяет подключить диск к Pod.

### Сервер Kubernetes API: kube-apiserver

Сервер Kubernetes API (kube-apiserver) – это HTTP REST-сервер, экспортирующий различные объекты API для кластера Kubernetes, такие как Pod, Node или HorizontalPodAutoscaler. Сервер API предоставляет веб-интерфейс для выполнения операций CRUD с состоянием кластера. Сервер API – единственный компонент в плоскости управления, взаимодействующий с etcd, базой данных Kubernetes. По сути, сервер API предоставляет интерфейс для всех операций, изменяющих состояние кластера Kubernetes. Сервер API не имеет состояния и может работать на нескольких узлах одновременно. Перед серверами API в плоскости управления, состоящей из нескольких узлов, размещается балансировщик нагрузки HTTPS. В состав сервера API вхо- дят контроллеры доступа, обеспечивающие аутентификацию и авторизацию клиентов. Вызов, отправленный командой kubectl, проходит аутентификацию, а затем сервер API сохраняет новый объект развертывания в etcd. Следующий шаг – уведомление планировщика о необходимости запустить модуль Pod на узле.

### Планировщик Kubernetes: kube-scheduler

**Планировщик Kubernetes (kube-scheduler)** предлагает чистую и простую реализацию планирования, идеально подходящую для такой сложной системы, как Kubernetes. При планировании модулей Pod он учитывает несколько факторов, таких как аппаратная конфигурация узла, доступные вычислительные ресурсы и объем памяти, ограничения политик планирования и др. Планировщик также следует правилам соответствия/несоответствия (affinity/anti-affinity), определяющим порядок планирования и размещения модулей Pod. По сути, правила соответствия определяют силу притяжения модулей Pod к узлам, отвечающим требованиям, тогда как правила несоответствия определяют силу отталкивания. Ограничения (Taint), дополняющие правила, позволяют узлам отклонять определенные типы модулей, а это означает, что планировщик может определить, какие модули и на каких узлах не должны находиться. Планировщик выбирает узлы для размещения реплик, а затем планирует развертывание модулей Pod на них. Жизненным циклом модуля Pod управляет агент kubelet. Он действует как мини-планировщик для узла. Как только планировщик Kubernetes обновит NodeName в определении Pod, kubelet развернет этот модуль на своем узле.

### Контроллеры инфраструктуры

Инфраструктура поддержки контроллеров - Диспетчер контроллеров Kubernetes (Kubernetes Controller Manager, KCM) или компонент kube-controller-manager и облачный диспетчер контроллеров (Cloud Controller Manager, CCM).

**Контроллеры** – это программные компоненты, управляющие жизненным циклом модулей Pod. К ним относятся kubelet, облачный диспетчер контроллеров (CCM) и планировщик.

Объекты API PersistentVolume (PV) и PersistentVolumeClaim (PVC) определяют хранилища и воплощаются в реальность диспетчерами KCM и CCM. Одна из ключевых особенностей Kubernetes – возможность работать на множестве платформ: в облаке, на «голом железе» или на ноутбуке. Однако хранилища и другие компоненты различаются на разных платформах. KCM – это набор циклов управления, запускающих различные компоненты, называемые контроллерами, на узле, находящемся в плоскости управления. Это один двоичный файл, но он управляет несколькими циклами и, соответственно, контроллерами.

При развертывании в облачном окружении платформа Kubernetes напрямую взаимодействует с API общедоступного или частного облака, и большинство соответствующих вызовов API выполняет CCM. Цель этого компонента – запускать контроллеры, специфичные для облака, и выполнять вызовы к API облака. Вот список этих контроллеров:
- контроллер узлов – выполняет тот же код, что и KCM;
- контроллер маршрутизации – настраивает маршруты в используемой облачной инфраструктуре;
- контроллер сервисов – создает, обновляет и удаляет балансировщики нагрузки облачного провайдера;
- контроллер томов – создает, подключает и монтирует тома, а также взаимодействует с облачным провайдером, осуществляя управление томами.

Разрабатываются другие интерфейсы для поддержки более модульного и независимого от провайдеров услуг будущего Kubernetes:
- сетевой интерфейс контейнеров (Container Network Interface, CNI) – предоставляет IP-адреса модулям Pod;
- интерфейс среды выполнения контейнеров (Container Runtime Interface, CRI) – определяет и подключает различные механизмы выполнения контейнеров;
- интерфейс хранилища для контейнеров (Container Storage Interface, CSI) – модульный способ поддержки новых типов хранилищ без необходимости изменять код Kubernetes.

kubelet создает модуль Pod, он определяет нужное хранилище и подключает его к контейнеру через пространство имен mnt Linux. После этого приложение может пользоваться хранилищем. При создании сервиса LoadBalancer вместо ClusterIP облачный провайдер Kubernetes «наблюдает» за запросом балансировщика и выполняет его. Контроллер KCM, выполняя цикл наблюдения, обнаруживает потребность в новом балансировщике нагрузки и выполняет вызовы API, необходимые для создания балансировщика в облаке, или вызывает аппаратный балансировщик нагрузки, внешний по отношению к кластеру Kubernetes.

### Масштабирование, высокодоступные приложения и плоскость управления

Выполняя масштабирование, kubectl может увеличивать и уменьшать количество модулей Pod в кластере. Он работает непосредственно с наборами реплик, состояний и другими объектами API, которые используют модули Pod
```
kubectl scale --replicas 300 deployment zeus-front-end-ui
```
Сбои можно разбить на три основные категории: сбой модуля Pod, сбой узла и сбой обновления программного обеспечения.
- сбой модуля Pod. За жизненный цикл модулей Pod отвечает агент kubelet, который выполняет запуск, остановку и перезапуск модулей. Выход модуля Pod из строя определяется по отсутствию контрольных сообщений от него или по аварийному завершению процесса. В этом случае kubelet пытается перезапустить модуль.
- сбой узла. Один из циклов управления в kubelet постоянно сообщает серверу API об исправности узла. Если узел присылает контрольные сообщения недостаточно часто, то контроллер KCM меняет его статус на «вне сети» и планировщик перестает планировать модули Pod для запуска на этом узле. Модули, действовавшие на узле, планируются для удаления, а затем переносятся на другие узлы.
- сбой в обновлении программного обеспечения. Обновление развертываний осуществляется простым изменением версии образа в определении YAML и обычно выполняется одним из трех способов:
  - kubectl edit – принимает объект Kubernetes API на входе и открывает локальный терминал для редактирования объекта API на месте;
  - kubectl apply – принимает файл на входе и отыскивает объект API, соответствующий этому файлу, автоматически заменяя его;
  - kubectl patch – применяет небольшой файл с исправлениями, определяющий различия для объекта.

### Автоматическое масштабирование

Три формы автоматического масштабирования:
- создание большего количества модулей Pod (горизонтальное автоматическое масштабирование с помощью HorizontalPodAutoscaler);
- предоставление модулям Pod большего количества ресурсов (вертикальное автоматическое масштабирование с помощью VerticalPodAutoscaler);
- создание дополнительных узлов (с помощью ClusterAutoscaler).

### Управление затратам

Kubernetes поддерживает возможность плотного размещения модулей Pod, что позволяет запускать узлы с избыточными ресурсами и высокой плотностью модулей. Плотность Pod контролируется с помощью следующих шагов.
- Масштабируйте и профилируйте свои приложения. Приложения должны быть протестированы и тщательно проверены как на предмет потребления памяти, так и центрального процессора. После профилирования можно правильно определить потребности приложения в ресурсах.
- Выберите размер узла. Это позволит упаковать несколько приложений на одном узле. Запуск виртуальных машин разных размеров или серверов без системного программного обеспечения с разной емкостью позволяет сэкономить деньги и развернуть на них больше модулей Pod. При этом вы должны обеспечить достаточно большое количество узлов для поддержания высокой доступности в соответствии с требованиями SLA.
- Сгруппируйте определенные приложения вместе на определенных узлах. Это обеспечит наибольшую плотность. Ограничения и допуски позволяют шаблону Operator (Оператор) группировать модули и управлять их развертыванием.

Еще фактор, который необходимо учитывать, – это шумные соседи. В зависимости от рабочей нагрузки некоторые из настроек могут оказаться неподходящими. Однако вы можете более равномерно распределить «шумные» приложения в своем кластере Kubernetes, используя определения соответствия/несоответствия (affinity/antiaffinity) модулей Pod.

# Создание модулей Pod

Для создания так называемого контейнера вызывается множество примитивов Linux:
- kubelet выясняет, что должен запустить контейнер;
- затем kubelet (обращением к среде выполнения контейнеров) запускает приостановленный контейнер (так называемый контейнер pause), что дает ОС Linux время на создание сети для контейнера. Этот приостановленный контейнер является предшественником фактического приложения, которое будет запущено. Его цель – создать дом для начальной загрузки нового сетевого процесса контейнера и его идентификатора процесса (PID);

### Что такое примитивы Linux?

Примерами таких примитивов могут служить такие инструменты, как iptables, ls, mount и многие другие базовые программы, доступные в большинстве дистрибутивов Linux. Знание основ этих инструментов дает мощный толчок в понимании множества новых плагинов и надстроек в экосистеме Kubernetes:
- сетевой прокси-сервер kube-proxy создает правила iptables, и эти правила часто приходится проверять для устранения проблем с сетью контейнеров в больших кластерах. Получить эти правила можно, запустив команду iptables -L в узле Kubernetes. Провайдеры Container Network Interface (CNI) тоже используют этот сетевой прокси-сервер
- интерфейс Container Storage Interface (CSI) определяет сокет для взаимодействий kubelet с технологиями хранения. С помощью команды mount можно увидеть смонтированные в кластере контейнеры и тома, управляемые платформой Kubernetes, не привлекая kubectl и другие инструменты, не входящие в стандартную конфигурацию ОС.
- команды среды выполнения контейнеров, такие как unshare и mount, используются при создании изолированных процессов.

Примитивы Linux почти всегда представляют свои операции как операции с каким-либо файлом, потому что все, что вам нужно построить с помощью Kubernetes, изначально создавалось для работы в Linux, а Linux изначально разрабатывалась для использования файловой абстракции в качестве примитива управления.

Важной чертой примитивов Linux: их можно комбинировать в операции более высокого уровня. Используя канал (|), можно получить вывод одной команды и передать его для обработки другой команде. Например, проверка работоспособности etcd внутри кластера. Внутри контейнера на узле, где запущены компоненты плоскости управления Kubernetes, можно выполнить следующую команду:
```
ls /var/log/containers/ | grep etcd
```
Можно узнать, где находятся ресурсы конфигурации, связанные с etcd, выполнив примерно такую команду:
```
find /etc | grep etcd; find /var | grep etcd
```
### Предварительные условия для запуска модуля Pod

Для создания модуля Pod мы полагаемся на возможности изоляции, сетевых взаимодействий и управления процессами. Все это может быть реализовано с помощью утилит, уже доступных в ОС Linux. На самом деле некоторые из этих утилит можно считать обязательными, потому что без них kubelet не сможет выполнить действия, необходимые для запуска модуля Pod. 

Программы (или примитивы), на которые мы полагаемся в повседневной работе с кластерами Kubernetes:
- **swapoff** – команда, отключающая подкачку памяти, что является известным предварительным условием для запуска Kubernetes, позволяющим исключить конкуренцию за процессор и память;
- **iptables** – основное требование (обычно) сетевого прокси-сервера, который создает правила iptables для отправки служебного трафика модулям Pod;
- **mount** – эта команда проецирует ресурс в определенное место в файловой системе;
- **systemd** – эта команда обычно запускает kubelet – основной процесс, управляющий всеми контейнерами в кластере;
- **socat** – эта команда позволяет установить двунаправленную связь между процессами; socat обеспечивает правильное функционирование команды kubectl port-forward;
- **nsenter** – инструмент для входа в различные пространства имен процесса и просмотра происходящего (с точки зрения сети, хранилища или процесса). Пространство имен Linux имеет определенные ресурсы, к которым невозможно обратиться из внешнего мира. Например, уникальный IP-адрес модуля Pod в кластере Kubernetes не используется другими модулями, даже на том же узле, потому что каждый модуль работает в отдельном пространстве имен;
- **unshare** – команда, позволяющая запускать дочерние процессы, выполняющиеся изолированно, **unshare** также может изолировать точки монтирования (каталог /) и сетевые пространства имен (IP-адреса), и поэтому ее можно считать прямым аналогом команды docker run в ОС Linux;
- **ps** – программа, отображающая список запущенных процессов. Агент kubelet должен постоянно следить за процессами, чтобы всегда быть в курсе, например, когда они завершаются. С помощью команды ps можно определить, имеются ли в кластере процессы-зомби, не начал ли привилегированный контейнер проявлять неуправляемое поведение.

### Исследование зависимостей модуля Pod от Linux

Жизненный цикл модуля Pod – циклический процесс, отражающий фундаментальный цикл управления, который определяет, что делает сам агент kubelet во время работы. Поскольку контейнеры в модуле Pod могут выйти из строя в любой момент, на этот случай существует цикл управления, возвращающий их к жизни. Можно сказать, что сам Kubernetes – это всего лишь сложно организованный набор циклов управления, которые позволяют автоматически запускать и управлять контейнерами в больших масштабах. Пока kubelet работает, он выполняет непрерывный цикл согласования, в котором проверяются и запускаются модули Pod. После запуска модуля Pod в Kubernetes объект Pod на сервере API хранит полную информацию о состоянии модуля.
```
kubectl get pods -o yaml
```
Одним из параметров, которые Kubernetes определяет для всех своих модулей Pod, является default-token. Он предоставляет модулям сертификат, позволяющий связываться с сервером API и «звонить домой». В дополнение к томам Kubernetes мы передаем нашим модулям информацию о DNS.
```
kubectl exec -t -i core-k8s mount | grep resolv.conf
```
В большинстве окружений Linux то, что мы называем контейнерами, – это просто процессы, созданные с помощью нескольких механизмов изоляции, позволяющих им уживаться с сотнями других процессов в кластере.

### Создание модуля Pod с нуля

Четыре основных аспекта модуля Pod:
- хранилище;
- IP-адрес;
- изолированную сеть;
- идентификатор процесса.

Для начала создадим контейнер в самом прямом смысле – папку, в которой есть именно все, что нужно для запуска командной оболочки Bash, и больше ничего. Это делается с помощью известной команды **chroot**.

Назначение **chroot** – создать изолированную корневую файловую систему для процесса. Делается это в три шага.
- Выбор программы, которую требуется запустить, и где в файловой системе она должна работать.
- Создать среду для запуска процесса. В каталоге lib64 в Linux находится множество программ, которые необходимы даже для запуска простой командной оболочки, такой как Bash. Их нужно загрузить в новую корневую файловую систему.
- Скопировать программу для запуска в chroot-окружение.

Наконец, можно запустить программу, и она будет полностью изолирована от исходной файловой системы: она не сможет видеть или изменять другую информацию в вашей файловой системе.

```bash
#/bin/bash
mkdir /home/namespace/box
mkdir /home/namespace/box/bin   # Создают структуру каталогов для chroot-окружения, необходимую для нашей программы Bash
mkdir /home/namespace/box/lib
mkdir /home/namespace/box/lib64
cp -v /usr/bin/kill /home/namespace/box/bin/   # Копируют все программы из базовой ОС в это окружение
cp -v /usr/bin/ps /home/namespace/box/bin
cp -v /bin/bash /home/namespace/box/bin
cp -v /bin/ls /home/namespace/box/bin
cp -r /lib/* /home/namespace/box/lib/   # Копируют библиотечные зависимости программ в каталоги lib/
cp -r /lib64/* /home/namespace/box/lib64/
mount -t proc proc /home/namespace/box/proc   # Монтирует каталог /proc сюда
chroot /home/namespace/box /bin/bash   # Запускаем изолированный процесс Bash в изолированном каталоге
```
Изолированное окружение chroot – один из основных строительных блоков контейнерной революции, продолжающейся и по сей день, хотя в течение некоторого времени она была известна как «виртуальная машина для бедных».

### Использование mount для передачи данных процессам

Команда mount позволяет взять устройство и отобразить его в любой каталог в вашей ОС. Мы могли бы выполнить простую команду **mount --bind /tmp/ /home/namespace/box/data** и создать каталог /data в предыдущем сценарии chroot. Мы создали нечто похожее на контейнер, к тому же имеющее доступ к хранилищу.

### Защита процесса с помощью unshare

Одной из первых проблем, которые вам, возможно, придется решить при создании рабочей среды контейнеризации, является изоляция. Если выполнить ps -ax из этого процесса, сразу станет понятна важность изоляции; контейнер, имеющий полный доступ к хосту, сможет вывести его из строя, например, остановив процесс kubelet или удалив критически важные для системы файлы. Однако с помощью команды unshare мы можем создать chroot- окружение для запуска Bash в изолированном терминале с понастоящему ограниченным пространством процесса. 

Пример демонстрирует использование команды unshare для такой изоляции:
```
unshare -p -f --mount-proc=/home/namespace/box/proc chroot /home/namespace/box /bin/bash
```
Используя unshare для запуска chroot, мы имеем:
- изолированный процесс;
- изолированную файловую систему;
- возможность изменять определенные файлы в дереве каталогов /tmp.

### Создание сетевого пространства имен

Чтобы запустить ту же программу в новой сети, можно снова использовать команду unshare:
```
unshare -p -n -f --mount-proc=/home/namespace/box/proc chroot /home/namespace/box /bin/bash
```
Если сравнить этот модуль Pod с настоящим, то можно заметить одно важное отличие: отсутствие устройства eth0. В этом разница между контейнером, имеющим сеть и процессом в chroot-окружении - само по себе chroot-окружение бесполезно для запуска контейнерных приложений из-за необходимости в передаче им дополнительных изолированных средств.

### Ограничение потребления процессора с помощью cgroups

**Контрольные группы (control groups, cgroups)** – позволяют выделять больше или меньше процессорного времени и памяти приложениям, работающим в наших кластерах.
```yml
containers:
  resources: limits:
    memory: "200Mi"
  requests:
    memory: "100Mi"
```
### Создание раздела resources

Фактический способ определения этого параметра можно настроить в любом дистрибутиве Kubernetes с помощью флага --cgroup-driver. (Драйверы контрольных групп – это архитектурные компоненты в Linux, которые используются для выделения ресурсов. Обычно в роли драйвера в Linux используется systemd.

Чтобы определить ограничения контрольной группы, нужно выполнить следующие действия:
- создать PID (Внутри окружения - echo $$ - в примере 79455)
- передать ограничения для этого PID в операционную систему.
```bash
mkdir /sys/fs/cgroup/memory/chroot0
echo "10" > /sys/fs/cgroup/memory/chroot0/memory.limit_in_bytes
echo "0" > /sys/fs/cgroup/memory/chroot0/memory.swappiness
echo 79455 > /sys/fs/cgroup/memory/chroot0/tasks
```
Теперь вернемся к терминалу, где был запущен сценарий chroot0.sh. Попытка выполнить простую команду, такую как ls, терпит неудачу - bash: fork: Cannot allocate memory

### Проблема сети

Любому контейнеру Kubernetes могут потребоваться:
- маршрутизация трафика внутри кластера для сетевых взаимодействий между модулями Pod;
- маршрутизация трафика наружу для доступа к другим модулям Pod в интернете;
- балансировка трафика между конечными точками за сервисом со статическим IP-адресом.

Для поддержки всего этого необходимо, чтобы метаданные о модулях Pod публиковались где-то в Kubernetes (эту задачу решает сервер API), а также постоянно наблюдать за их состоянием (эту задачу решает kubelet) и своевременно обновлять эту информацию. Таким образом, модули Pod имеют не только команду запуска контейнера и образ Docker, но также метки и четко определенные спецификации, определяющие порядок публикации их состояния, благодаря чему их можно повторно создавать на лету вместе с набором возможностей, предоставляемых агентом kubelet.

### Как kube-proxy реализует сервисы Kubernetes с помощью iptables

Сервисы Kubernetes определяют контракт API, в котором говорится: «При обращении к некоторому IP-адресу ваш трафик автоматически пересылается одной из многих возможных конечных точек». В большинстве кластеров эти сетевые правила полностью реализуются прокси-сервером kube-proxy, который чаще всего настроен на использование iptables для организации маршрутизации сетевых пакетов. Программа iptables добавляет в ядро правила, которые затем используются для обработки сетевого трафика, и это наиболее распространенный способ реализации сервисов Kubernetes и маршрутизации трафика.

Модулю Pod нужно нечто большее, чем набор правил брандмауэра. Ему нужны по крайней мере:
- возможность принимать трафик в конечной точке сервиса;
- возможность отправлять трафик во внешний мир из своей конечной точки;
- возможность отслеживать текущие соединения TCP

### Использование модуля kube-dns

Модуль Pod kube-dns
- работает в любом кластере Kubernetes
- не имеет особых привилегий и использует обычную сеть модулей Pod, а не сеть хоста;
- отправляет трафик в порт 53, широко известный как стандартный порт DNS;
- уже работает в вашем кластере по умолчанию.

В Kubernetes провайдер CNI предоставляет уникальный IP-адрес и правила маршрутизации для доступа к этому адресу. Мы можем исследовать эти маршруты с помощью команды **ip route**

### Другие проблемы

**Хранение**

Помимо сети, нашему модулю Pod также может потребоваться доступ к различным типам хранилищ. Нужен какой-то способ, с помощью которого можно определить типы хранилищ и фиксировать в журнале неудачные попытки монтирования этих томов хранилища. Такой способ в Kubernetes предоставляют объекты StorageClass, PersistentVolume и PersistentVolumeClaim.

**Планирование**

Наличие планировщика, достаточно умного, чтобы поместить модуль Pod в место, где иерархия контрольных групп соответствует требованиям модуля к ресурсам, – еще одна важнейшая функция, которую берет на себя Kubernetes. **Планирование** – это общая проблема в информатике, поэтому следует отметить, что существуют альтернативные инструменты планирования, такие как Nomad.

**Обновление и повторный запуск**

Управление процессами и/или контрольными группами, связанными с модулем Pod, который может перестать работать, является важной частью организации масштабных контейнерных рабочих нагрузок, особенно в контексте микросервисов, которые должны быть переносимыми и эфемерными. Модель данных приложений в Kubernetes, о которой чаще всего думают с точки зрения объектов развертывания Deployment, приложений с состоянием StatefulSet, заданий Job и наборов демонов DaemonSet, поддерживает возможность надежного обновления.

# Использование контрольных групп для управления процессами в модулях Pod

### Процессы и потоки в Linux

Любой процесс в Linux может создать один или несколько потоков выполнения. Поток выполнения – это абстракция, которую программы могут использовать для создания новых процессов, имеющих общую память. Для примера давайте посмотрим с помощью команды ps -T, сколько независимых потоков планирования используется в Ku- bernetes:
```
ps -ax | grep scheduler
ps -T 631 - команда выводит список потоков выполнения планировщика, использующих общую память.
```
### Контрольные группы для управляющих процессов

Планировщик порождает несколько потомков и часто сам создается платформой Kubernetes, потому что является потомком containerd, – среды выполнения контейнеров, – которую Kubernetes использует в kind. В качестве эксперимента можете попробовать завершить процесс containerd и понаблюдать, как планировщик и его потоки возвращаются к жизни. Это делается самим агентом kubelet, у которого есть каталог /manifests. В этом каталоге определяются процессы, которые всегда должны выполняться, даже до того, как сервер API сможет планировать контейнеры. Собственно, так Kubernetes и устанавливает себя через kubelet. Жизненный цикл установки Kubernetes, который реализует kubeadm:
- в kubelet есть каталог manifests, в который включены сервер API, планировщик и диспетчер контроллеров;
- system запускает kubelet;
- kubelet сообщает процессу containerd о необходимости запустить все процессы, перечисленные в каталоге manifests;
- сразу после запуска сервера API агент kubelet подключается к нему и запускает все контейнеры, которые запросит сервер API.

**Зеркальные модули Pod и сервер API**
В kubelet есть секретное оружие: каталог /etc/kubernetes/manifests. Агент kubelet постоянно сканирует этот каталог, и, когда в нем появляются новые модули Pod, он немедленно создает и запускает их. Поскольку запуск этих модулей Pod производится в обход сервера Kubernetes API, им необходимо зеркалировать себя, чтобы сервер API узнал об их существовании. По этой причине модули Pod, созданные за пределами плоскости управления Kubernetes, называются зеркальными модулями Pod (mirror Pod). Зеркальные модули, как и любые другие, можно увидеть, выполнив команду kubectl get pods -A, но они создаются и управляются агентом kubelet на независимой основе. Это позволяет kubelet в одиночку запустить целый кластер Kubernetes, работающий внутри модулей Pod.

Модули Pod с классом **burstable**, как правило, не имеют жестких ограничений на использование ресурсов. Планировщик – это пример модуля Pod, который может иметь всплески потребления процессора (например, когда необходимо быстро запланировать 10 или 20 модулей Pod для выполнения на узле)

### Реализация контрольных групп для обычного модуля Pod

Модуль Pod планировщика представляет особый случай, потому что работает на всех кластерах и не поддерживает прямой возможности настройки или исследования. Волшебство изоляции в Kubernetes на компьютере с Linux можно рассматривать как обычное иерархическое распределение ресурсов, организованное с помощью простой структуры каталогов.
```
sudo cat /sys/fs/memory/docker/753../kubepods/pod8a58e9/d176../memory.limit_in_bytes
```
### Как kubelet управляет контрольными группами

Заглянув в /sys/fs/cgroup, можно увидеть все контролируемые ресурсы, которые в Linux можно распределить иерархически:
```
ls -d /sys/fs/cgroup/*
```
### Как kubelet управляет ресурсами

В структуре данных allocatable. Выполнив команду **kubectl get nodes -o yaml** на узле Kubernetes можно увидеть:
```
allocatable:
  cpu: "12"
  ephemeral-storage: 982940092Ki hugepages-1Gi: "0" hugepages-2Mi: "0"
  memory: 32575684Ki
  pods: "110"
```
Эти ресурсы представляют суммарный бюджет контрольных групп, доступный для выделения ресурсов модулям Pod. Агент kubelet вычисляет его, определяя общую емкость узла. Затем определяет пропускную способность процессора, необходимую для него самого и базового узла, и вычитает ее из доступного объема ресурсов. Полученные значения используются планировщиком Kubernetes при принятии решения о возможности размещения контейнера на этом конкретном узле. Для планирования используется следующее уравнение:
**Доступно_для_распределения = емкость_узла – ресурсы_для_kube – ресурсы_для_системы**

Вместе с kubelet запускается встроенная родительская логика. Соответствующий параметр настраивается с помощью флага командной строки, в результате чего kubelet становится родительской контрольной группой cgroup верхнего уровня для дочерних контейнеров. Предыдущее уравнение вычисляет общее количество контрольных групп, доступных для kubelet, которое называется бюджетом распределяемых ресурсов.

### Классы QoS: почему они важны и как они работают

Под QoS (качества обслуживания - Quality of Service) понимается доступность ресурсов в любой момент. QoS позволяет идти по тонкой грани, когда многие службы работают неоптимально в часы пик, не жертвуя качеством критически важных сервисов. На практике такими критическими сервисами могут быть системы обработки платежей. Вытеснение модуля Pod во многом зависит от того, насколько он превысит лимит ресурсов. В общем случае:
- приложения с предсказуемым потреблением памяти и процессора подвергаются вытеснению с меньшей вероятностью;
- жадные приложения с большей вероятностью будут вытеснены в периоды пиковых нагрузок, если попытаются использовать больше процессорного времени или памяти, чем выделено Kubernetes, и при условии, что они не принадлежат к классу Guaranteed;
- приложения с классом BestEffort – первые кандидаты на вытеснение и перепланирование в периоды пиковых нагрузок.

### Создание классов QoS путем настройки ресурсов

Burstable, Guaranteed и BestEffort – это три класса QoS, которые создаются в зависимости от определения объекта Pod. Эти параметры могут помочь увеличить количество контейнеров, запускаемых в кластере, часть из которых затем может отключаться в периоды высокой загрузки и вновь запускаться позже. Нет универсальных политик, подходящих под все случаи жизни:
- если все контейнеры отнести к классу гарантированного качества обслуживания (Guaranteed QoS), вам будет трудно обслуживать динамические рабочие нагрузки с изменяющимися потребностями в ресурсах;
- если на серверах не будет контейнеров с классом Guaranteed QoS, то kubelet не сможет поддерживать работу некоторых важных процессов.

Вот основные правила определения класса QoS:
- BestEffort – к этому классу относятся модули Pod, не определяющие требуемые количества памяти или ядер процессора. Они первые кандидаты на вытеснение (и перезапуск на другом узле), когда образуется нехватка ресурсов;
- Burstable – к этому классу относятся модули Pod, определяющие требуемые количества памяти или ядер процессора, но не устанавливающие ограничений для обоих ресурсов. Они вытесняются с меньшей вероятностью, чем модули с классом BestEffort;
- Guaranteed – к этому классу относятся модули Pod, определяющие требуемые количества памяти и ядер процессора и устанавливающие ограничения для обоих ресурсов. Они вытесняются и перемещаются в последнюю очередь.
```
kubectl get pods -n qos -o yaml -  вернет класс в поле status определения модуля Pod - qosClass: Burstable
```
# Интерфейсы CNI и настройка сети в модулях Pod

### Зачем нужны программно-определяемые сети в Kubernetes

Главная проблема контейнерных сетей, часто насчитывающих сотни модулей Pod с экземплярами одного и того же сервиса, состоит в последовательной маршрутизации трафика в кластер и из него, чтобы этот трафик всегда попадал в нужное место, даже если модули Pod перемещаются между узлами. Для решения этой проблемы Kubernetes предоставляет два основных сетевых инструмента:
- прокси-сервер сервисов – обеспечивает балансировку нагрузки модулей Pod с постоянными IP-адресами и маршрутизирует объекты Kubernetes Service;
- CNI – гарантирует возрождение модулей Pod в плоской сети, к которой легко получить доступ из кластера.

В основе этого решения лежит Kubernetes Service с типом ClusterIP – объект, маршрутизируемый внутри кластера Kubernetes, но недоступный за его пределами.

### Реализация Kubernetes SDN на стороне сервиса: kube-proxy

Существует три основных типа объектов Service: ClusterIP, NodePort и LoadBalancer. Они определяют модули Pod для подключения с помощью меток. Если бы эти сервисы имели тип NodePort, то они могли бы получить IP-адреса любых узлов в нашем кластере. Если бы они имели тип LoadBalancer, то наше облако предоставило бы внешний IP-адрес. 
- kube-proxy использует технологию низкоуровневой маршрутизации, такую как iptables или IPVS, для передачи трафика от сервисов в модули Pod и обратно;
- сервис типа LoadBalancer получает трафик с внешнего IP-адреса и пересылает его на внутренний IP-адрес.

**NodePort** – это тип объектов Service в Kubernetes, доступных на всех пор- тах за пределами внутренней сети модуля Pod. Они позволяют построить самый простой балансировщик нагрузки.

Сервисы NodePort основаны на сервисах ClusterIP. Сервисы **ClusterIP** имеют внутренний IP-адрес, не пересекающийся (обычно) с сетью модуля Pod, которая синхронизирована с сервером API.

### Плоскость данных в kube-proxy

Прокси-сервер kube-proxy должен иметь возможность обрабатывать входящий и исходящий TCP-трафик модулей Pod с сервисами. В Linux для проксирования обычно используется iptables. В кластерах kind прокси kube-proxy по умолчанию использует iptables.

Однако kube-proxy – это лишь один из компонентов, управляющих трафиком в Kubernetes SDN. Маршрутизацию трафика в Kubernetes можно представить в виде трех отдельных уровней:
- внешние балансировщики нагрузки или входные/шлюзовые маршрутизаторы – направляют трафик в кластер Kubernetes;
- kube-proxy – управляет маршрутизацией трафика между сервисами в модулях Pod. kube-proxy просто управляет статическими правилами маршрутизации, которые реализуются ядром или другой технологией в плоскости данных, такой как iptables;
- провайдеры CNI – направляют трафик в модули Pod и от них, независимо от способа обращения, – через конечную точку службы или напрямую.

### Провайдеры CNI

Провайдеры CNI (CNI providers) реализуют спецификацию CNI, определяющую контракт, который позволяет окружениям выполнения контейнеров запрашивать рабочий IP-адрес для процесса при запуске. Краткий перечень основных локальных провайдеров CNI:
- Calico – провайдер CNI на основе протокола пограничного шлюза (Border Gateway Protocol, BGP), который реализует плоскость данных, создавая новые правила маршрутизации BGP.
- Antrea – провайдер CNI плоскости данных OVS, использующий мост для маршрутизации всего трафика модулей Pod.
- Flannel – провайдер CNI на основе моста; в настоящее время вы- шел из употребления.
- Google, EC2 и NCP – облачные провайдеры CNI, использующие патентованное программное обеспечение для маршрутизации трафика с применением облачных технологий.
- Cilium – провайдер CNI на основе технологии XDP, использую- щий современные Linux API без привлечения механизмов ядра для управления трафиком.
- KindNet – плагин CNI, который по умолчанию используется в кластерах kind. Предназначен исключительно для использования в простых кластерах с одной подсетью.

### Входные контроллеры

Входные контроллеры (ingress controllers) позволяют направить в кластер весь трафик через один IP-адрес

# Хранилища в модулях Pod и CSI

Возьмем для примера конкретную задачу – реализация возможности хранения файлов в модуле Pod – и решим ее. Файл должен сохраняться в период от остановки до повторного запуска контейнера и должен быть доступен новым узлам в кластере. Kubernetes позволяет представить эту модель данных, предлагая такие понятия, как том хранилища PersistentVolume (PV), запрос на хранилище PersistentVolumeClaim (PVC) и класс хранилища StorageClass:
- тома хранилищ PV дают возможность управлять дисковыми томами в среде Kubernetes;
- запросы на хранилище PVC определяют требования приложений (модулей Pod) к этим томам и обрабатываются Kubernetes API;
- класс хранилища StorageClass дает возможность получать тома, не зная, как они реализованы. Это позволяет определять запросы PVC, не зная точно, какой тип тома PersistentVolume используется за кулисами.

Классы хранилищ StorageClass дают приложениям возможность запрашивать тома или типы хранилищ, отвечающие требованиям конечного пользователя, декларативно, что позволяет определять классы, удовлетворяющие различным потребностям, таким как:
- сложные требования к уровню обслуживания (что хранить, как долго хранить и что не хранить);
- высокие требования к производительности (приложения пакетной обработки или приложения с малой задержкой);
- безопасность в многопользовательских окружениях (доступ пользователей к определенным томам).

### Небольшое отступление: виртуальная файловая система (VFS) в Linux

Фактически сама файловая система является абстракцией сложной схемы, соединяющей приложения с простым набором API-интерфейсов, доступ к файлу подобен доступу к любому другому API. Управление файлами в Linux реализовано в форме набора простых команд, в том числе:
- read() – читает несколько байтов из открытого файла;
- write() – записывает несколько байтов в открытый файл;
- open() – создает и/или открывает файл для чтения и записи;
- stat() – возвращает некоторые основные сведения о файле;
- chmod() – меняет права доступа к файлу для пользователя или группы.
Все эти команды передаются так называемой виртуальной файловой системе (Virtual Filesystem, VFS), которая в большинстве случаев является оберткой вокруг BIOS системы. В облаке и в случае с FUSE (Filesystem in Userspace – файловая система в пространстве пользователя) Linux VFS является оберткой вокруг механизмов, которые в конечном счете ведут к сетевым вызовам.

### Три вида хранилищ для Kubernetes
- хранилище Docker/containerd/CRI – файловая система с копированием при записи, используется контейнерами. Контейнерам требуются специальные файловые системы во время выполнения, потому что запись выполняется на уровне VFS. Обычно в окружении Kubernetes используются такие файловые системы, как btrfs, overlay или overlay2;
- хранилище в инфраструктуре Kubernetes – тома hostPath или Secret, которые используются на отдельных узлах для локального обмена информацией
- хранилище для приложений – тома хранилища, которые модули Pod используют в кластере Kubernetes. Если модуль Pod должен записать данные на диск, то ему необходимо смонтировать том хранилища, для чего в объявление Pod добавляются соответствующие определения.

### Создание PVC в кластере kind
Как создать каталог, который модуль Pod мог бы использовать для хранения некоторых файлов:
- PV (PersistentVolume – том хранилища) создается провайдером динамических хранилищ, работающим в нашем кластере kind. Это контейнер, который создает хранилище для Pod, выполняя запрос PVC;
- PVC (PersistentVolumeClaim – запрос на хранилище) недоступен до готовности тома PersistentVolume, потому что планировщик должен убедиться, что сможет смонтировать хранилище в про- странстве имен модуля Pod перед его запуском;
- kubelet не запустит Pod, пока VFS не смонтирует доступный для записи PVC в пространство имен файловой системы Pod.

Проверить, какие провайдеры хранилищ доступны в кластере Kubernetes, можно командой **kubectl get sc**

Чтобы показать, как модули Pod поддерживают хранение общих данных для контейнеров и как смонтировать несколько точек хранения с разной семантикой, мы запустим Pod с двумя контейнерами и двумя томами. В итоге:
- контейнеры в Pod смогут совместно использовать общую информацию;
- постоянное хранилище может быть создано в kind по запросу с помощью динамического поставщика hostPath;
- любой контейнер сможет использовать несколько томов, смонтированных в модуле Pod.
```yml
apiVersion: v1
kind: PersistentVolumeClaim metadata:
  name: dynamic1
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100k
```

Получить список динамически созданных томов **kubectl get pv**

Когда определен класс standard или default, PVC, для которого не определен класс хранилища, автоматически настраивается на получение PVC по умолчанию, если он существует. На самом деле это происходит с помощью контроллера доступа (admission controller), который предварительно модифицирует новые модули Pod, поступающие на сервер API, добавляя к ним метку класса хранилища default. При наличии этой метки инструмент подготовки томов (provisioner), работающий в кластере, автоматически обнаруживает запрос модуля Pod на получение хранилища и немедленно создает том.

### Интерфейс контейнерного хранилища (CSI)
Kubernetes CSI определяет интерфейс, чтобы поставщики решений для организации хранилищ могли легко подключаться к любому кластеру Kubernetes и предоставлять приложениям широкий спектр возможностей хранения данных. Цель определения CSI – упростить управление решениями хране- ния данных с точки зрения производителя.

Возникла потребность в стандарте, определяющем возможность создания, монтирования и управления жизненным циклом внешних томов. По аналогии с интерфейсом CNI, рассматривавшимся выше, стандарт CSI гарантирует нормальную работу DaemonSet на всех узлах, монтирующих тома. CSI позволяет легко заменить хранилище одного типа другим и даже одновременно запустить несколько хранилищ разных типов.

Проблема внутренних провайдеров характерна не только для хранилищ. Интерфейсы CRI, CNI и CSI появились из внешнего кода, который долгое время включался в ядро Kubernetes. В первых версиях в ядро Kubernetes входили такие инструменты, как Docker, Flannel и многие другие файловые системы. Они постепенно удаляются, и CSI является лишь одним из ярких примеров, как после создания надлежащих интерфейсов код может исключаться из ядра и перемещаться во внешние плагины.

### CSI как спецификация, работающая внутри Kubernetes

Спецификация CSI определяет общий набор функций, позволяет определить службу поддержки хранилища без указания конкретной реализации. Определяемые им операции делятся на три основные категории: сервисы идентификации, сервисы контроллеров и сервисы узлов.
- сервисы идентификации – позволяют плагину идентифицировать себя (предоставить метаданные о себе), чтобы плоскость управления Kubernetes могла подтвердить, что плагин хранилища определенного типа запущен и доступен для данного типа томов;
- сервисы узлов – позволяют агенту kubelet взаимодействовать с локальным сервисом, выполняющим операции, специфичные для провайдера хранилища.
- сервисы контроллеров – реализуют обработку событий создания, удаления и других, связанных с управлением жизненным циклом тома хранилища. Чтобы NodeService имел какую-либо ценность, используемая система хранения должна сначала создать том, который в нужный момент сможет подключить kubelet. Сервисы контроллеров играют связующую роль, соединяя Kubernetes с поставщиком решения хранения данных.

### CSI: как работает драйвер хранилища
Плагин хранилища CSI разбивает действия, выполняемые для подключения хранилища к Pod, на три этапа: регистрацию драйвера хранилища, запрос тома и его публикацию. Регистрация драйвера хранилища выполняется через Kubernetes API. Фреймворку сообщается, как обращаться с этим конкретным драйвером и когда агенту kubelet будет доступен определенный тип хранилища. Получив запрос на создание тома, механизм поддержки хранилищ приступает к созданию тома, вызывая функцию CreateVolume, представленную выше. На самом деле CreateVolume вызывается (обычно) отдельным сервисом, известным как внешний инструмент подготовки (external provisioner), который, скорее всего, никак не связан с DaemonSet. Скорее, это стандартный Pod, наблюдающий за сервером Kubernetes API и отвечающий на запросы создания томов, который вызывает API поставщика решения хранилищ. Этот сервис просматривает созданные объекты PVC, а затем вызывает функцию CreateVolume зарегистрированного драйвера CSI. В момент публикации том подключается (монтируется) к модулю Pod. Эта операция выполняется драйвером хранилища CSI, который обычно находится на каждом узле кластера. Публикация тома – это просто необычное название операции монтирования тома в место, указанное агентом kubelet, чтобы Pod cмог записать в него данные. kubelet отвечает за запуск контейнера в Pod с правильными пространствами имен монтирования для доступа к этому каталогу.

### Привязка точек монтирования
В Linux конкретная операция, делающая каталог доступным для модуля Pod называется привязкой точки монтирования, в любой среде хранения, предоставляемой CSI, Kubernetes получает несколько запущенных сервисов, которые координируют взаимодействия вызовов API для достижения конечной цели – монтирования внешних томов в модули Pod. Драйверы CSI – это набор контейнеров, часто поддерживаемых поставщиками, поэтому сам kubelet должен поддерживать возможность монтирования изнутри контейнера. Это известно как распространение монтирования (mount propagation) и является важной частью низкоуровневых требований к Linux, соответствие которым необходимо для правильной работы Kubernetes.

**Контроллер** – это мозг любого драйвера CSI, пересылающий запросы к хранилищу внутренним провайдерам, такими как vSAN, EBS и т. д. Интерфейс, который он реализует, должен позволять создавать, удалять и публиковать тома на лету для использования нашими модулями Pod.

**Интерфейс узла** отвечает за взаимодействие с агентом kubelet и монтирование хранилища в модули Pod.

# Реализация и моделирование хранилищ
Готовясь запустить в кластере приложение, которому требуется хранилище, вы должны задать себе следующие вопросы:
- должно ли хранилище гарантировать сохранность данных? Для организации хранилища с гарантиями сохранности часто приходится использовать такие решения, как NAS, NFS или что-то вроде GlusterFS. Все они имеют свои достоинства и недостатки, которые вы должны оценить и взвесить;
- должно ли хранилище работать быстро? Является ли скорость ввода/вывода критически важным параметром? Если важна высокая скорость, то часто хорошим выбором оказывается хранилище emptyDir, работающее в памяти, или хранилище специального типа с подходящим контроллером;
- какой объем хранилища потребуется каждому контейнеру и сколько контейнеров планируется запустить? При большом количестве контейнеров может потребоваться контроллер хранилища;
- нужен ли выделенный диск для обеспечения безопасности? Если да, то вашим потребностям могут лучше соответствовать локальные тома;
- используются ли рабочие нагрузки искусственного интеллекта (ИИ) с кешами для моделей и обучающих данных? Для них могут потребоваться быстродействующие тома, остающиеся в памяти в течение нескольких часов;
- потребности в хранилищах укладываются в диапазон от 1 до 10 Гбайт? Если да, то в большинстве случаев можно с успехом использовать локальное хранилище или emptyDir;
- используется ли что-то вроде распределенной файловой системы Hadoop (Hadoop Distributed File System, HDFS) или Cassandra, выполняющей репликацию и резервное копирование данных? Если да, то в таком случае вы сможете использовать только тома на локальном диске, но это усложняет восстановление;
- допускается ли приостановка хранилища? Если да, то, возможно, вам подойдет модель хранения объектов поверх распределенных томов, например, с применением таких технологий, как NFS или GlusterFS

### Микрокосм в экосистеме Kubernetes: динамическое хранилище
Определив, что приложению необходимо хранилище, можно взглянуть, какие примитивы, предоставляет Kubernetes. Существует довольно много хранилищ, преследующих разные цели. Это связано с тем, что хранилище, в отличие от сети, является чрезвычайно ограниченным и дорогостоящим ресурсом из-за физических ограничений и различных юридических и процедурных аспектов хранения данных на предприятиях. Пользователи запрашивают хранилище, администраторы определяют хранилище с помощью классов хранилищ, а инструменты подготовки CSI обычно отвечают за подготовку хранилища, доступного пользователю для записи.

### Оперативное управление хранилищем: динамическое выделение ресурсов
Возможность оперативного управления хранилищем в кластере означает также необходимость поддержки оперативного выделения томов. Эта поддержка называется динамической подготовкой (dynamic provisioning), или динамическим выделением ресурсов.

### Локальное хранилище в сравнении с emptyDir
Том emptyDir хорошо известен большинству новичков в Kubernetes. Он предлагает самый простой способ смонтировать каталог в Pod и практически не требует затрат на безопасность или ресурсы, за которыми необходимо внимательно следить. Сравнительные характеристики локальных томов и томов emptyDir. Эти тома имеют совершенно разный жизненный цикл, несмотря на то что все данные хранятся локально. На- пример, в случае аварии локальный том можно использовать для восстановления данных из работающей базы данных, тогда как emptyDir не поддерживает этого. Использование сторонних томов и контрол- леров для PVC – это третий вариант применения, который обычно подразумевает, что хранилище может быть переносимым и монтироваться на новых узлах в случае миграции модуля Pod. Тома emptyDir не имеют конкретного варианта использования и служат своего рода швейцарским армейским ножом. Обычно они используются, когда двум контейнерам требуется небольшое пространство для временного хранения данных. Возможно, вам интересно, зачем использовать тип emptyDir вместо простого монтирования реального PersistentVolume непосредственно в два контейнера. Тому есть несколько причин:
- тома PersistentVolume обычно обходятся довольно дорого. Им требуется контроллер распределенного хранилища, который пре- доставит том определенного объема, и этот объем может быть ограничен. Если нет нужды хранить данные для модуля Pod, то и нет смысла тратить ресурсы;
- производительность томов PersistentVolume может быть на порядок ниже производительности томов emptyDir. Это связано с тем, что часто сохраняемые данные должны передаваться по сети и сохраняться на диске. Тома emptyDir, напротив, могут использовать временное файловое хранилище (tmpfs) и находиться в оперативной памяти, которая по определению работает намного быстрее;
- тома PersistentVolume по своей природе менее безопасны, чем emptyDir. Данные, хранящиеся в томе PersistentVolume, могут сохраняться и читаться в разных местах кластера. Тома emptyDir, напротив, недоступны за пределами модулей Pod, объявивших их;
- emptyDir можно использовать с пустыми контейнерами для созда- ния каталогов, включая /var/log и /etc/, когда приложению требу- ется выполнять запись данных в файлы журналов или в конфигурационные файлы;
- для большей производительности или поддержки определенной функциональности желательно добавить в контейнер каталог /tmp или /var/log.

### Тома PersistentVolume
PersistentVolume – это ссылка Kubernetes на том хранилища, который можно подключить к модулю Pod. Хранилище монтируется агентом kubelet, который создает и монтирует различные типы томов и/или вызывает драйвер CSI (о нем мы поговорим далее). Соответственно, запрос PersistentVolumeClaim (PVC) является именованной ссылкой на PersistentVolume. Когда создается Pod, которому требуется постоянное хранилище, обычно возникает следующая цепочка событий:
- запрашивается создание Pod, которому требуется PVC;
- планировщик Kubernetes начинает искать место для него – узел с соответствующей топологией хранилища, количеством процессоров и объемом памяти;
- создается действующий PVC, к которому Pod может получить доступ;
- запрошенный объем выделяется плоскостью управления Kubernetes. Контроллер динамического хранилища создает PersistentVolume. В большинстве промышленных кластеров Kubernetes имеется как минимум один такой контроллер, а чаще несколько (различающихся именем StorageClass). Контроллеры просто наблюдают за созданием стандартных объектов PVC на сервере API, а затем создают тома, соответствующие им;
- после проверки требований к хранилищу планировщик решает, что Pod готов;
- Pod, зависящий от этого запроса PVC, планируется и запускается;
- во время запуска Pod агент kubelet монтирует локальные каталоги, соответствующие PVC;
- локально смонтированный том становится доступным для записи в Pod;
- модуль Pod начинает работу и выполняет чтение или запись в хранилище, существующее внутри PersistentVolume.

Зачем планировщику информация о хранилище - гарантии производительности и предсказуемости. Например, вам может понадобиться ограничить количество томов на узле. Кроме того, если какие-то узлы имеют ограничения, касающиеся хранилища, планировщик может отказаться от размещения модулей Pod на этих узлах, чтобы не создавать «зомби-Pod», которые, хоть и запланированы, но не могут запуститься должным образом из-за отсутствия доступа к ресурсам хранилища.

### Интерфейс контейнерного хранилища (CSI)
**CSI** – это интерфейс контейнерного хранилища (container storage interface), который разработан так, что код поддержки разных типов томов больше не нужно компилировать в ядро Kubernetes. Модель хранилища CSI требует только реализовать некоторые концепции Kubernetes (DaemonSet и контроллер хранилища), чтобы с их помощью kubelet мог предоставить любое хранилище. CSI не зависит от Kubernetes.

**Динамическая подготовка (dynamic provisioning)**, как возможность волшебным образом создать PersistentVolume при появлении PVC, никак не связана с интерфейсом CSI, позволяющим динамически монтировать в контейнер любые хранилища. Однако эти две технологии прекрасно дополняют друг друга. Комбинируя их, разработчик может продолжать использовать одни и те же объявления классов StorageClass (описываются ниже) для монтирования потенциально разных типов хранилищ, но предоставляющих одинаковую высокоуровневую семантику.

### Классы хранилищ (StorageClass)
Классы StorageClass позволяют определять сложную семантику хранения декларативным способом. Несмотря на наличие уникальных параметров, которые можно определять для хранилищ разных типов, общим для всех является режим привязки. Именно здесь создание пользовательского динамического средства подготовки может быть чрезвычайно важным. 

Что происходит в средстве динамической подготовки:
- Мы пишем цикл управления, который отслеживает запросы на получение томов.
- Обнаружив запрос, мы выделяем на жестком диске том размером 100 Гбайт, выполняя вызов API (например, провайдера хранилища на нашем NAS). Обратите внимание, что также можно было бы заранее создать множество каталогов в NAS или NFS.
- Затем определяем объект PV для поддержки PVC. Тип этого тома может быть любым, например NFS или hostPath.

С этого момента в работу вступает Kubernetes, и, как только запрос PVC будет выполнен и создан PersistentVolume, наши модули Pod смогут планироваться планировщиком.

### Варианты организации хранилищ в Kubernetes

**Создание простого Pod с пустым томом для быстрой записи**

Каноническим примером модуля Pod с томом emptyDir может служить приложение, хранящее временные файлы в /var/tmp. Эфемерное хранилище обычно монтируется в Pod как:
- том с одним или несколькими файлами, что характерно для ConfigMap, содержащих конфигурационные данные (например, с различными настройками приложения);
- переменные окружения со значениями из секретов Secret.

Для приложения, использующего файл в роли блокировки или семафора для синхронизации нескольких контейнеров или для внедрения в приложение какой-то эфемерной конфигурации (например, через ConfigMap), достаточно локальных томов хранилища, управляемых агентом kubelet. Секреты могут использовать том emptyDir для внедрения в контейнер пароля (например, в виде файла). Точно так же том emptyDir может одновременно использоваться двумя модулями Pod, благодаря чему можно создать простую очередь заданий или сигналов между двумя контейнерами.

**emptyDir** – это самый простой в реализации тип хранилищ. Ему не требуется физический том на диске, и он гарантированно будет работать в любом кластере. emptyDirs имеет более высокую производительность, чем контейнеризованный каталог.

### hostPath для управления системой и/или доступа к данным
Тома hostPath в Kubernetes похожи на тома Docker, позволяя контейнерам записывать данные непосредственно в файловую систему хоста. Тип томов hostPath имеет широкий спектр примене- ний. Обычно их делят на две категории:
- для реализации вспомогательных функций, предоставляемых контейнерами, которые можно реализовать, только имея доступ к файловой системе хоста (мы рассмотрим это на примере);
- для долговременного хранения файлов, чтобы, когда Pod исчезнет, его данные сохранялись в предсказуемом месте. Обратите внимание, что это применение – почти всегда антишаблон, потому что может приводить к изменению поведения приложения, когда Pod останавливается и затем переназначается на новый узел.
### hostPath, CSI и CNI: канонический вариант использования
Провайдеры CNI в Linux устанавливаются на узел с kubelet, буквально записывая свои двоичные файлы из контейнера в файловую систему узла, обычно в каталог /opt/cni/bin. Это один из самых популярных вариантов применения hostPath – использование контейнеров для выполнения административных действий на узле Linux. Эту возможность используют многие приложения, имеющие административный характер, в том числе:
- Prometheus – решение для сбора метрик и мониторинга, монтирующее /proc и другие системные ресурсы с целью получения параметров потребления ресурсов;
- Logstash – решение для интеграции журналов, подключающее различные каталоги с журналами к контейнерам;
- провайдеры CNI, которые, как уже упоминалось, самостоятельно устанавливают двоичные файлы в /opt/cni/bin;
- провайдеры CSI, использующие hostPath для монтирования своих хранилищ.

Провайдер Calico CNI – один из многих таких низкоуровневых системных процессов Kubernetes, которые не могли бы выполняться без возможности монтировать устройства или каталоги хоста в контейнер напрямую.

**Когда следует использовать тома hostPath?**

hostPath позволяет, реализовать простое и быстрое резервное копирование, осуществление политик соответствия и предоставление высокопроизводительных хранилищ без опоры на глубокую облачную интеграцию. В общем случае, рассматривая пути реализации хранилищ для каждого конкретного случая, учитывайте следующее:
- существует ли встроенный провайдер томов Kubernetes? Если да, то его применение может оказаться самым простым решением, требующим наименьших усилий по автоматизации с вашей стороны;
- если нет, то предоставляет ли ваш поставщик реализацию CSI? Если предоставляет, то можно использовать его, и, скорее всего, он будет иметь в своем составе средство динамической подготовки томов.

Если ни один из этих вариантов не подходит, попробуйте использовать такие инструменты, как тома hostPath или Flex, чтобы настроить хранилище как том, который можно привязать к любому модулю Pod.

# Запуск модулей Pod: как работает kubelet

kubelet – это рабочая лошадка кластера Kubernetes; в центрах обработки данных могут работать тысячи агентов kubelet, потому что он запускается на каждом узле. Одной из задач kubelet является запуск и остановка контейнеров, а CRI – это интерфейс, посредством которого kubelet взаимодействует со средой выполнения контейнеров. Например, containerd классифицируется как среда выполнения контейнеров, которая получает образ и создает действующий контейнер. Движок Docker – это среда выполнения контейнеров, но в настоящее время сообщество Kubernetes отказывается от него в пользу containerd, runC и других окружений.

### kubelet и узел
kubelet – это двоичная программа, запускаемая демоном systemd на каждом узле. Она играет роль планировщика модулей Pod и агента локального узла. kubelet хранит и поддерживает информацию о сервере, на котором выполняется, для своего узла и при обнаружении изменений обновляет объект Node с помощью сервера API.
```
kubectl get nodes -o yaml
annotations:
  kubeadm.alpha.kubernetes.io/cri-socket:
    /run/containerd/containerd.sock   - Kubelet использует этот сокет для связи со средой выполнения контейнеров
```
Все объекты Kubernetes API имеют поля spec и status:
- spec – определяет характеристики объекта (каким он должен быть);
- status – представляет текущее состояние объекта.

Раздел status – это данные, которые kubelet поддерживает для кластера. Помимо всего прочего, он включает список условий, среди которых можно увидеть время последнего обмена контрольными сообщениями (heartbeat) с сервером API. В момент запуска узел автоматически получает всю необходимую системную информацию. Информация о состоянии отправляется на сервер Kubernetes API и постоянно обновляется. Количество узлов может варьироваться от 0 до 15 000 (15 000 считаются текущим пределом из-за чрезмерного роста накладных расходов на поддержку конечных точек и выполнение других операций, интенсивно использующих метаданные). Информация в объекте Node имеет решающее значение, например, для планирования модулей Pod.

**Основы kubelet** - Узлы и агенты kubelet бесполезны без среды выполнения контейнеров, на которую они полагаются, запуская контейнерные процессы.

### Среда выполнения контейнеров: стандарты и соглашения
Для развертывания образов контейнеров, которые являются tar- архивами, агент kubelet использует четко определенный API, позво- ляющий распаковать эти архивы и запустить двоичные файлы в них. Существует две стандартные спецификации, CRI и OCI, определяю- щие, как и что должен делать kubelet, чтобы запустить контейнер:
- интерфейс CRI определяет, как. Он предлагает ряд удаленных вызовов для запуска, остановки и управления контейнерами и образами.
- интерфейс OCI определяет, что. Это стандарт форматов образов контейнеров. Запуская или останавливая контейнер с помощью CRI, вы полагаетесь на то, что формат образа этого контейнера будет определенным образом стандартизирован.

Находясь в командной оболочке, работающей внутри контейнера kind, можно выполнить следующую команду:
```
ps axu | grep /usr/bin/kubelet
```
Она выведет список конфигурационных параметров и флагов командной строки, полученных агентом kubelet, работающим внутри контейнера kind.

### Конфигурационные параметры и API агента kubelet
kubelet – это точка интеграции для широкого спектра примитивов в ОС Linux. kubelet поддерживает более 100 различных параметров ко- мандной строки в двух разных категориях:
- параметры – управляют поведением низкоуровневых функций Linux, используемых с Kubernetes, таких как правила, связанные с использованием iptables, или конфигурация DNS;
- варианты – определяют жизненный цикл и работоспособность kubelet.

Все API-значения для kubelet определены в файле types.go. В этом файле определяется структура данных API, содержащая входные конфигурационные параметры для kubelet, в том числе многие настраиваемые аспекты kubelet, на которые ссылается код в файле kubelet.go. Kubernetes API – это механизм или стандарт определения объектов API в Kubernetes и в его исходном коде.

### Запуск kubelet
Порядок событий может меняться в зависимости от изменений в кодовой базе kubelet и из-за асинхронной природы Kubernetes в целом. Глядя на шаги, можно заметить, что:
- выполняется несколько простых проверок работоспособности, помогающих убедиться, что kubelet сможет запускать модули Pod (контейнеры), – проверяются значения в NodeAllocatable, определяющие, сколько процессоров и памяти доступно;
- запускается процедура containerManager – основной цикл обра- ботки событий в kubelet;
- добавляется контрольная группа. При необходимости она созда- ется с помощью функции setupNode. Планировщик и диспетчер контроллеров «замечают», что в системе появился новый узел. Посредством сервера API они «наблюдают» за его состоянием – способностью запускать процессы и периодичностью отправки контрольных сигналов серверу API. Если kubelet пропустит отправку очередного контрольного сообщения, то в какой-то момент диспетчер контроллеров исключит узел из кластера;
- запускается цикл обработки событий диспетчера устройств deviceManager, который подключает внешние устройства к kubelet. Затем информация об этих устройствах передается как часть непрерывных обновлений (упомянутых на предыдущем шаге);
- к kubelet подключаются и регистрируются плагины журналирования, CSI и устройств.

### Механизм аренды и блокировки в etcd, эволюция аренды узла

Для оптимизации производительности больших кластеров и уменьшения сетевого трафика в Kubernetes 1.17 была реализована конечная точка сервера API для управления узлами через механизм аренды в etcd. В etcd реализована концепция аренды, чтобы компоненты с высокой доступностью (Highly Available, HA), которым может потребоваться аварийное переключение, могли положиться на центральный механизм аренды и блокировки вместо реализации собственного механизма.

Состояние kubelet поддерживается двумя независимыми циклами управления:
- агент kubelet обновляет объект NodeStatus каждые 5 мин, чтобы сообщить серверу API о своем состоянии.
- дополнительно каждые 10 с kubelet обновляет объект Lease. Эти обновления позволяют контроллерам в плоскости управления Kubernetes вытеснить узел в течение нескольких секунд, если он отключился, без больших затрат на отправку большого объема информации о состоянии.

### Управление жизненным циклом Pod в kubelet
После завершения всех предварительных проверок kubelet запускает большой цикл синхронизации: процедуру containerManager. Жизненный цикл модуля Pod и шаги, связанные с управлением им.
- Начало жизненного цикла модуля Pod.
- Проверка возможности запуска Pod на узле.
- Настройка хранилища и сети (CNI).
- Запуск контейнеров посредством CRI.
- Мониторинг модуля Pod.
- Перезапуск.
- Остановка.

### kubelet не запускает контейнеры: это делает CRI
Среда выполнения контейнеров (CRI) предоставляет средства управления контейнерами, которые kubelet должен запускать. Сам агент kubelet не может запускать контейнеры: в этом он полагается на среду выполнения, такую как containerd или runC.

### Приостановленный контейнер: момент истины
Приостановленный контейнер, являющийся прародителем всех контейнеров. Приостановленный контейнер:
- ждет, пока станет доступно сетевое пространство имен, чтобы все контейнеры в модуле Pod могли использовать один IP-адрес и взаимодействовать друг с другом, используя IP-адрес 127.0.0.1;
- ждет, пока станет доступна файловая система, чтобы все контейнеры в модуле Pod могли обмениваться данными через emptyDir.

После настройки Pod каждый вызов runC принимает одни и те же параметры из пространства имен. Несмотря на то что kubelet не запускает контейнеры, в нем сосредоточено довольно много логики, связанной с созданием модулей Pod, которыми kubelet должен управлять. Например, он проверяет готовность сети и хранилища для контейнеров.

### Интерфейс времени выполнения контейнеров (CRI)
Основная магия заключена в интерфейсе CRI, который абстрагирует runC вместе с другими функциями и обеспечивает планирование на более высоком уровне, управление образами и деятельность среды выполнения контейнеров. Как сообщить Kubernetes, где находится сервис CRI? kubelet запускается со следующими двумя параметрами:
```
--container-runtime=remote
--container-runtime-endpoint=/run/containerd/containerd.sock
```
Для взаимодействий с конечной точкой среды выполнения контей- неров kubelet использует gRPC, фреймворк вызова удаленных процедур (Remote Procedure Call, RPC); сам containerd имеет встроенный плагин CRI. Значение remote подразумевает, что Kubernetes может использовать сокет containerd в качестве минимальной реализации интерфейса для создания и управления модулями Pod и их жизненными циклами.

### Процедуры CRI
CRI состоит из четырех высокоуровневых интерфейсов Go, включающих все основные функции, необходимые Kubernetes для запуска контейнеров. Вот эти интерфейсы:
- PodSandBoxManager – создает окружение установки для модулей Pod;
- ContainerRuntime – запускает, выполняет и останавливает контейнеры;
- ImageService – извлекает, перечисляет и удаляет образы;
- ContainerMetricsGetter – сообщает количественную информацию о запущенных контейнерах.

Эти интерфейсы обеспечивают функции приостановки, извлечения, а также создания изолированного окружения 

### Интерфейсы kubelet
###Внутренний интерфейс среды выполнения
CRI в Kubernetes делится на три части: Runtime, StreamingRuntime и CommandRunner. Интерфейс KubeGenericRuntime используется внутри Kubernetes и служит оберткой для основных функций
среды выполнения CRI. Внутри RuntimeService имеется ContainerManager, реализующий главное волшебство. Этот интерфейс является частью фактического определения CRI.

**Как kubelet извлекает образы: интерфейс ImageService** - За подпрограммами среды выполнения контейнеров скрывается интерфейс ImageService, определяющий несколько основных методов: PullImage, GetImage, ListImages и RemoveImage. Идея извлечения образа, исходящая из семантики Docker, является частью спецификации CRI.

# DNS в Kubernetes
Определения, которые желательно знать:
- ответы NXDOMAIN – ответы DNS, которые возвращаются, если для заданного доменного имени не найден IP-адрес;
- отображения A и AAAA – получают имя хоста на входе и возвра- щают адрес IPv4 или IPv6 (например, они получают имя google. com и возвращают 142.250.72.4);
- отображения CNAME – возвращают псевдоним для некоторых доменных имен.

В собственных окружениях CNAME имеют решающее значение для обратной совместимости клиентов API и других приложений, зависящих от сервисов. В следующем фрагменте показан пример, как смешиваются имена A и записи CNAME. Эти записи находятся в так называемых файлах зон.

Вы почти наверняка будете использовать CoreDNS в своих кластерах, и тому есть веские причины. Это единственный общедоступный сервис DNS с открытым исходным кодом и встроенной поддержкой Kubernetes. Он способен:
- подключаться к серверу Kubernetes API и получать IP-адреса для модулей Pod и сервисов Service;
- преобразовывать записи DNS в IP-адреса модулей Pod и сервисов внутри кластера;
- кешировать записи DNS для эффективной работы больших кластеров, где работают сотни модулей Pod, которым требуется разрешать имена сервисов;
- подключать плагины с новыми возможностями во время компиляции (не во время выполнения);
- масштабироваться и гарантировать чрезвычайно низкие задержки даже в окружениях с высокой нагрузкой;
- перенаправлять запросы вышестоящим серверам DNS для разрешения внешних имен в кластере.

CoreDNS позволяет разрешать IP-адреса сервисов, находящихся в сети кластера, а также модулей Pod.

Объекты StatefulSet обладают интересными свойствами, когда речь заходит о DNS, поэтому мы используем этот Pod для исследования возможностей и ограничений Kubernetes в отношении запуска процессов высокой доступности с надежными конечными точками DNS. Объекты StatefulSet чрезвычайно важны для приложений с четко установленной идентичностью, таких как:
- Apache ZooKeeper;
- MinIO или другие приложения, связанные с хранением данных;
- Apache Hadoop;
- Apache Cassandra;
- приложения для майнинга биткоинов.

Объекты StatefulSet (наборы модулей Pod с состоянием) тесно связаны с использованием DNS в Kubernetes, потому что оба обычно используются в сценариях, когда каноническая модель микросервисов начинает разрушаться, а внешние объекты (сервисы, приложения, устаревшие системы) начинают влиять на способ развертывания приложений.

### DNS и автономные сервисы
Автономный сервис – это сервис, не имеющий поля ClusterIP и напрямую возвращающий запись A DNS-сервера. Это решение имеет некоторые важные последствия для DNS. Когда используются автономные сервисы? Как оказывается, многие приложения создают кворумы и реализуют другое поведение, зависящее от сети, напрямую подключаясь друг к другу через IP-адрес, не полагаясь на kube-proxy, обеспечивающий балансировку нагрузки. В общем случае старайтесь использовать сервисы с полем ClusterIP, когда это возможно, потому что с ними гораздо проще работать с точки зрения DNS, если только вам действительно не нужно какое-то особое поведение, связанное с сохранением IP, принятием решений кворумом или конкретными гарантиями в отношении IP-адресов.

### Файл resolv.conf
Файл **resolv.conf** – это стандартный способ настройки DNS для контейнера. В любой ситуации, когда вы пытаетесь выяснить настройки DNS в вашем модуле Pod, это первое место, куда следует заглянуть. Если вы используете современный сервер Linux, то можете исполь- зовать resolvctl, но суть та же.

# Плоскость упарвления
Один из самых простых способов запустить и настроить плоскость управления – использовать kind, кластер Kubernetes в контейнере
```
kind create cluster
kubectl cluster-info --context kind-kind $ kubectl -n kube-system get po -o custom-columns=":metadata.name"
```
### Особенности сервера API
Сервер API обслуживает не только объекты плоскости управления, но и пользовательские объекты. 

Kubernetes – открытая платформа, т. е. открытый API. Открытость платформы обеспечивает появление инноваций и открывает путь для творчества. Ниже перечислены некоторые ресурсы API, связан- ные с кластером Kubernetes. В них вы увидите часть этих объектов API (например, Deployment и Pod):
```
kubectl api-resources -o name | head -n 20
```
Когда определяется манифест YAML с ClusterRoleBinding, частью определения является версия API.
```
apiVersion: rbac.authorization.k8s.io/v1beta1
```
Раздел apiVersion в предыдущем фрагменте YAML определяет версию API. Версионирование API – сложная задача. Объекты API имеют следующие уровни: alpha, beta и GA (general availability – доступность для всех). Объекты с уровнем alpha никогда не должны использоваться в промышленном окружении, потому что могут вызвать серьезные проблемы с обновлением. Объекты API с уровнем alpha обязательно будут изменяться и предназначены только для разработки и экспериментов. Уровень beta в действительности не обозначает бета-версию! Бета-версии программного обеспечения часто считаются нестабильными и не предназначены для промышленного использования, но объекты Kubernetes API с уровнем beta готовы к эксплуатации, и их поддержка гарантирована в отличие от объектов на уровне alpha. Например, наборы DaemonSet находились на уровне beta в течение многих лет, и практически все использовали их в промышленных окружениях.
Префикс v1 позволяет разработчикам Kubernetes нумеровать версии объектов API. Например, в Kubernetes v1.17.0 API автоматического масштабирования включает:
- /apis/autoscaling/v1;
- /apis/autoscaling/v2beta1;
- /apis/autoscaling/v2beta2.
Обратите внимание, что элементы этого списка имеют вид URI. Вы можете просматривать объекты API в формате URI, предварительно запустив кластер kind локально:
```
kind cluster start
kubectl proxy --port=8181
```
Затем откройте страницу http://127.0.0.1:8181/

### Определения пользовательских ресурсов (CRD)
В Kubernetes v1.17.0 имеется 54 объекта API
```
kubectl api-resources | wc
```
Чтобы отделить второстепенные объекты API от сервера API, были придуманы пользовательские определения ре- сурсов CRD. Они позволяют разработчикам определять свои объекты API, а затем с помощью kubectl внедрять их на сервер API. Следующая команда создает объект CRD на сервере API:
```
kubectl apply -f https://raw.githubusercontent.com/cockroachdb/cockroach-operator/v2.4.0/config/crd/bases/crdb.cockroachlabs.com_crdbclusters.yaml
```
По аналогии с Pod и другими стандартными объектами API объекты CRD расширяют платформу Kubernetes API без участия программиста. Операторы, пользовательские контроллеры допуска, Istio, Envoy и другие технологии теперь используют сервер API, определяя свои CRD.

### Планировщик
Планировщик, подобно другим контроллерам, реализует несколько циклов управления, обрабатывающих разные события. Начиная с версии Kubernetes 1.15.0, планировщик был реорганизован для использования фреймворка планирования и поддержки пользовательских плагинов. Kubernetes позволяет использовать пользовательские планировщики, которые запускаются не в реальном планировщике, а в отдельном модуле Pod. Первый компонент фреймворка планировщика – QueueSort. Он сортирует модули Pod, требующие планирования, и ставит их в очередь. Затем фреймворк разбивается на два цикла: цикл планирования и цикл связывания. Сначала цикл планирования выбирает узлы, доступные для запуска модулей Pod. После завершения цикла планирования в работу включается цикл связывания. Он выбирает конкретный узел и проверяет, можно ли разместить Pod на нем. Это может занять некоторое время. плагины планирования, которые обрабатывают такие конфигурации, как Pod NodeAffinity, влияющие на планирование модулей Pod. На первом этапе этого процесса вы- полняются плагины из списка QueueSort, но обратите внимание, что QueueSort можно расширить и даже заменить:
```
func getDefaultConfig() *schedulerapi.Plugins {
  return &schedulerapi.Plugins{
    QueueSort: &schedulerapi.PluginSet{
      Enabled: []schedulerapi.Plugin{
        {Name: queuesort.Name},
      },
    },
```
Приватная функция getDefaultConfig() вызывается функцией NewRegistry, которая определена в том же файле Go. Она возвращает экземпляр реестра с провайдерами алгоритмов. Следующие возвращаемые элементы определяют цикл планирования. Первый из них, Prefilter, – это список плагинов, выполняемых последовательно:
```
PreFilter: &schedulerapi.PluginSet { Проверяетналичие
  Enabled: []schedulerapi.Plugin {
    {Name: noderesources.FitName}, - Проверяет, достаточно ли ресурсов на узле
    {Name: nodeports.Name}, - Проверяет на узле наличие на узле свободных портов для размещения Pod
    {Name: podtopologyspread.Name}, - Проверяет соответствие PodTopologySpread, что позволяет равномерно распределять модули Pod по зонам
    {Name: interpodaffinity.Name}, - Обрабатывает совместимость модулей Pod. Если на узле выполняется несовместимый модуль (согласно правилам, определенным пользователем), то планируемый модуль «отталкивается» от этого узла
    {Name: volumebinding.Name}, - На самом деле это не фильтр, этот плагин создает кеш, используемый позже на этапах резервирования и предварительной привязки
  },
},
```
Следующий этап – фильтрация. Обратите внимание, что Filter – это список плагинов, определяющих возможность запуска Pod на определенном узле:
```
Filter: &schedulerapi.PluginSet {
  Enabled: []schedulerapi.Plugin {
    {Name: nodeunschedulable.Name}, - Гарантирует невозможность запланировать Pod на узле, отмеченном как непредназначенный для планирования (например, на узле в плоскости управления)
    {Name: noderesources.FitName}, - Плагин запускается повторно
    {Name: nodename.Name}, - PodSpec API позволяет установить поле nodeName, идентифицирующее узел, на котором должен размещаться Pod
    {Name: nodeports.Name}, - Этот плагин тоже запускается повторно
    {Name: nodeaffinity.Name} - Проверяет, соответствует ли селектор узла
в определении Pod метке в определении узла
...
```
Пользователь может назначить модулю Pod класс приоритета. В таком случае плагин defaultpreemption позволяет планировщику определить, можно ли вытеснить другой модуль Pod, чтобы освободить место для более приоритетного планируемого модуля Pod. Обратите внимание, что эти плагины повторно выполняют всю фильтрацию, чтобы определить, сможет ли Pod выполняться на определенном узле. Далее выполняется ранжирование. Планировщик составляет список узлов, на которых можно разместить Pod, и теперь он должен их упорядочить путем оценки, чтобы выбрать наиболее подходящий.

Планировщик определяет вес, влияющий на планирование. Все узлы, получившие оценку, прошли различные этапы фильтрации. При приоритизации узлов со сбалансированным использованием ресурсов планировщик учитывает количество доступных процессоров, памяти и томов. Последний шаг в процессе фильтрации – этап резервирования. На этом этапе резервируется том для модуля Pod, который будет использоваться в цикле привязки.

Цикл планирования, выполняя в основном фильтрацию, определя- ет подходящий узел. Но подготовка всех ресурсов на узле, необходимых модулю Pod, – это гораздо более длительный процесс, в течение которого Pod пребывает в очереди планирования. Давайте теперь по- смотрим на цикл привязки, начав с этапа предварительной привязки.
```
PreBind: &schedulerapi.PluginSet
  { Enabled: []schedulerapi.Plugin{
    {Name: volumebinding.Name}, - Привязывает том к модулю Pod
  },
},

Bind: &schedulerapi.PluginSet{
  Enabled: []schedulerapi.Plugin{
    {Name: defaultbinder.Name}, - Сохраняет объект Bind, обращаясь к серверу API, и обновляет информацию об узле, где должен запуститься Pod
  },
},
```
У планировщика есть несколько очередей: активная очередь, куда помещаются модули Pod, планируемые для запуска, и очередь простоя, содержащая модули Pod, не предназначенные для планирования.

**Краткий обзор фреймворка планирования**
- построитель очереди – поддерживает очередь модулей Pod;
- цикл планирования – фильтрует и отыскивает узлы для запуска модулей Pod;
- цикл привязки – сохраняет данные и информацию о привязке на сервере API.

### Диспетчер контроллеров
Значительная часть функциональности диспетчера контроллеров (Kubernetes Controller Manager, KCM) была перемещена в облачный диспетчер контроллеров (Cloud Controller Manager, CCM).
### Хранилище
Давайте пройдемся по контроллерам, составляющим KCM.
Контроллер узла наблюдает за работоспособностью узла и своевременно обновляет его статус в объекте Nodes API. Контроллер репликации поддерживает заданное количество модулей Pod для каждого объекта контроллера репликации в системе. Контроллер конечной точки – это последний контроллер, управля- ющий объектами Endpoint, которые определяются в Kubernetes API. Эти объекты обычно обслуживаются автоматически и создаются для передачи прокси-серверу kube-proxy информации, необходимой для подключения модуля Pod к сервису. Сервис Service может иметь один или несколько модулей Pod, обрабатывающих трафик от указанного сервиса.

### Облачные диспетчеры контроллеров Kubernetes (CCM)
Если у вас появится желание написать новый облачный контроллер, то вам потребуется включить функции для следующих компонентов:
- узлов – для обслуживания виртуальных экземпляров;
- маршрутизации – для обслуживания трафика между узлами;
- внешних балансировщиков нагрузки – для создания балансировщика нагрузки, внешнего по отношению к узлам в кластере.

Код взаимодействия с этими компонентами внутри облачного провайдера зависит от API провайдера.

# etcd и плоскость управления
etcd – это хранилище ключей/значений с надежными гарантиями согласованности. Когда узлы, контроллеры или сервер API выходят из строя, возникает необходимость согласовать приложения в центре обработки данных, чтобы получить возможность запланировать контейнер для выполнения на другом узле, привязать тома к этому контейнеру и т. д. Все изменения состояния, сделанные через Kubernetes API, на самом деле сохраняются в etcd.

Особенности etcd в Kubernetes с практической точки зрения.
- Если данные, хранящиеся в etcd, потеряются, то кластер выйдет из строя. Делайте резервные копии etcd!
- Для запуска etcd v3 в промышленом окружении желательно использовать быстрые твердотельные накопители и высокопроизводительную сеть.

В любой момент времени может выполняться множество операций записи, и это подразумевает жесткие требования к сети и диску – сеть с пропускной способностью не ниже 10 Гбит/с и твердотельные накопители. Обычно должно выполняться не менее 50 последовательных операций ввода/вывода в секунду, что требует применения произво- дительных дисков (например, со скоростью вращения 7200 об/мин). Но нередко etcd требуется выполнять гораздо больше операций.
- В большинстве центров обработки данных и облачных окружений неизбежно возникают сбои, затрагивающие данный узел, поэтому в запасе должны иметься резервные узлы с etcd. Это означает, что в развертывании желательно иметь три или больше узлов etcd.
- Использующие etcd в кластерном окружении должны пони- мать, как работает реализация Raft, почему дисковый ввод/вы- вод важен для консенсуса Raft и как etcd использует процессор и память.
- В etcd сохраняется не только состояние кластера, но и все собы- тия. Поэтому следует подумать о хранении событий (которых много) в другой конечной точке etcd, чтобы операции с данными ядра кластера не конкурировали с менее важными операциями с метаданными событий.
- Инструмент командной строки etcdctl, предназначенный для взаимодействия с сервером etcd, имеет свой тест, позволяющий быстро проверить производительность etcd: etcdctl check perf.
- Если потребуется восстановить экземпляр etcd, следуйте инструкциям на http://mng.bz/6Ze5, где описывается, как вручную восстановить моментальный снимок etcd.

### Мониторинг производительности etcd с помощью Prometheus
Все операции записи в конечном итоге завершаются кворумом нескольких серверов etcd, согласившихся, что запись завершена. Каждое действие сервера API приводит к синхронной записи в etcd. Это гарантирует, что запрос на создание Pod будет храниться на диске, если вдруг сервер API в какой-то момент выйдет из строя. Сервер API отправляет информацию серверу etcd, играющему роль «лидера», после чего начинается волшебство распределенного консенсуса, чтобы запечатлеть это в камне.
- Обычно используется три, пять или семь экземпляров. Количество экземпляров etcd всегда нечетное, чтобы всегда была возможность голосования по выбору нового лидера
- единственный сервер API может получить запрос на выполнение записи, после чего он сохранит данные в указанной конеч- ной точке etcd, назначенной серверу etcd при запуске как --etcd-servers;
- скорость операции записи зависит от быстродействия самого медленного узла etcd. Если на одном из узлов вывод на диск про- исходит медленно, то это время доминирует над общим временем транзакции.

### Когда нужно настраивать etcd
**Вложенная виртуализация** - Как нетрудно догадаться, вложение одной виртуальной машины в другую приводит к огромным потерям производительности и не рекомендуется для промышленного использования. Основная причина, почему этот подход считается таким опасным, – из-за наличия нескольких уровней виртуализации добавляется задержка к операциям записи на жесткий диск. Эта задержка может сделать etcd крайне ненадежным. Вложенная виртуализация ограничивает количество операций ввода/вывода в единицу времени и приводит к частым ошибкам записи. Сам кластер Kubernetes восстанавливается после них, но многие модули Pod будут постоянно терять статус лидера, если в Kubernetes используется Lease API, что становится все более распространенным явлением.

### Пример: быстрая проверка работоспособности etcd
Один из самых простых способов быстро убедиться, что etcd работает, – использовать инструмент командной строки etcdctl, включающий встроенный тест производительности. Для примера зайдите по ssh на узел, где работает etcd. Запустите find, чтобы отыскать местоположение двоичного файла etcdctl:
```
find / -name etcdctl
```
Используйте найденный двоичный файл и передайте ему необходимые сертификаты cacert, которые, скорее всего, будут храниться в /etc/kubernetes/pki/, если используется кластер kind или Cluster API:
```
/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/13/fs/usr/local/bin/etcdctl \
--endpoints="https://localhost:2379" \
--cacert="/etc/kubernetes/pki/etcd/ca.crt" \
--cert="/etc/kubernetes/pki/etcd/server.crt" \
--key="/etc/kubernetes/pki/etcd/server.key" \
check perf
```
Этот вывод сообщит, что etcd достаточно быстр для промышленного использования. 

### etcd как хранилище данных
Алгоритмы консенсуса всегда были ключевой частью распределенных систем, с самых первых дней. Архитектурные решения в плоскости данных, реализованной через etcd, и в плоскости управления (планировщик, диспетчеры контроллеров и сервер API) основаны на одном и том же принципе согласованности любой ценой. Поэтому etcd решает общую проблему согласования глобальных знаний. К числу базовых функций, лежащих в основе сервера Kubernetes API, относятся:
- создание пар ключ/значение;
- удаление пар ключ/значение;
- ключи наблюдения

### Можно ли запустить Kubernetes в других базах данных?
Фреймворк Kubernetes – это всего лишь набор контроллеров, поддерживающих баланс в распределенной группе компьютеров, необходим механизм мониторинга изменений. Вот некоторые базы данных, поддерживающие возможность наблюдения - Apache ZooKeeper; Redis; etcd.

Протокол Raft - определяет надежный и масштабируемый способ согласования состояния базы данных ключей/значений в распределенной группе компьютеров. Проще говоря, Raft можно определить так:
- в базе данных имеется лидер и несколько подчиненных узлов. Общее число узлов – нечетное;
- клиент запрашивает операцию записи в базу данных;
- сервер получает запрос и передает его нескольким подчиненным узлам;
- после того как половина подчиненных узлов получит и подтвердит запрос на запись, сервер фиксирует его;
- клиент получает ответ с признаком успешной записи;
- если лидер выходит из строя, подчиненные узлы выбирают нового лидера и процесс продолжается, при этом старый лидер исключается из кластера.

Из вышеупомянутых баз данных только etcd поддерживает модель строгой согласованности, основанную на протоколе Raft, и конкретно создана для координации центров обработки данных. Именно поэтому она была выбрана для использования в Kubernetes. Тем не менее Kubernetes можно запустить с другой базой данных. Основным требованием Kubernetes является возможность наблюдения за источником данных, чтобы выполнять такие задачи, как планирование модулей Pod, создание балансировщиков нагрузки, предоставление хранилища. etcd v3 имеет возможность организовать наблюдение за множеством различных ключей по одному TCP-соединению. Это делает etcd v3 мощным компаньоном для больших класте ов Kubernetes. Таким образом, в Kubernetes есть второе требование к своей базе данных: согласованность.

### Строгая согласованность
Такая согласованность достигается с помощью ключевых архитектурных констант etcd:
- в кластере etcd есть только один лидер, и точка зрения лидера на 100 % верна;
- в случае потери узла лидера нечетный кворум экземпляров etcd всегда может проголосовать и выбрать нового лидера;
- операция записи не считается осуществленной, пока она не будет подтверждена кворумом;
- узел etcd, не имеющий актуальной информации обо всех транзакциях, никогда не будет передавать данные. Это обеспечива- ется протоколом согласования под названием Raft;
- кластер etcd в любой момент имеет одного и только одного лидера, которому передаются все запросы на запись;
- все операции записи блокируются в etcd до каскадной передачи их как минимум половине узлов в кворуме.

### Согласованность в etcd обеспечивают операции fsync
Операции fsync блокируют операции записи на диск, что гарантирует согласованность etcd и запись данных на диск до возврата ответа. Это обстоятельство также может замедлить некоторые операции API, зато вы никогда не потеряете данные о состоянии кластера Kubernetes в случае сбоя. Чем быстрее ваши диски, тем быстрее будет выполняться операция fsync:
- в промышленных кластерах обычно наблюдается снижение производительности (или сбои), если продолжительность выполнения операции fsync превышает 1с;
- в типичном облаке можно ожидать, что эта операция завершится в течение 250 мс или около того.

Самый простой способ оценить работу etcd – посмотреть на производительность fsync. Метрики Prometheus, извлекаемые из etcd, можно получить с помощью curl или отобра- зить в виде графиков с помощью Grafana. Эти метрики сообщают продолжительность в секундах выполнения блокирующих операций fsync. В локальном кластере на SSD вы увидите, что они выполняются быстро. Например, в локальном кластере, работающем на ноутбуке с твердотельным накопителем, можно увидеть что-то вроде этого:
```
curl localhost:2381/metrics|grep fsync
etcd_disk_wal_fsync_duration_seconds_bucket{le="0.001"} 1239 etcd_disk_wal_fsync_duration_seconds_bucket{le="0.002"} 2365
...
```
- 1 239 из 2 588 операций записи на диск продолжались менее 0,001 с;
- 2 587 операций вывода или 2 588 операций записи на диск продолжались менее 0,008 с;

### Задача etcd – надежное хранение фактов
Определим несколько концепций с конкретными особенностями Kubernetes, которые в конечном итоге зависят от способности etcd поддерживать согласованное представление о кластере. Например:
- за один раз можно принять только один новый факт, и эти факты должны передаваться на один узел, где работает плоскость управления;
- состояние системы в любой момент времени есть сумма всех текущих фактов;
- сервер Kubernetes API обеспечивает 100%-ную достоверность операций чтения и записи в отношении существующего потока фактов;
- поскольку в любой базе данных сущности могут меняться со временем, то могут быть доступны более старые их версии, и etcd поддерживает понятие версионирования.

Строгая согласованность обеспечивается в два этапа: установление лидерства в определенное время, чтобы каждый факт в потоке был принят всеми членами системы, а затем запись этого факта членам. Это (грубое) представление так называемого алгоритма консенсуса Paxos. etcd гарантирует, что недоступность лидера не приведет к несогласованному состоянию базы данных, прерывая операцию записи, если лидер пропадает во время транзакции, до того как запись будет произведена на 50 % узлов в кластере etcd. 

### Журнал упреждающей записи etcd
Надежность etcd обеспечивается еще и сохранением всех транзакций в журнале упреждающей записи (Write-Ahead Log, WAL). Чтобы понять важность WAL, посмотрим, что происходит, когда выполняется запись.
- Клиент отправляет запрос на сервер etcd.
- Сервер etcd использует протокол консенсуса Raft для записи транзакции.
- Raft окончательно подтверждает, что все узлы etcd, являющиеся членами кластера Raft, имеют синхронизированные файлы WAL.

Как клиент может отправлять запросы на запись множеству серверов, не вызывая некоторую несогласованность хотя бы на короткие периоды времени. Причина в том, что реализация Raft в etcd пересылает запросы на запись лидеру независимо от источника. Запись считается незавершенной, пока лидер и половина других узлов в кластере не обновят свое состояние.

### Теорема CAP
Теорема CAP (Consistency, Availability, Partition tolerance – согласованность, доступность, устойчивость к разделению) – основополагающая теория в информатике. etcd выбирает согласованность как наиболее важный аспект. Как результат, если один лидер в кластере etcd выходит из строя, то база данных оказывается недоступной, пока не будет выбран новый лидер. Почему невозможно обеспечить идеальную запись данных в рас- пределенной системе? Потому что узлы могут выходить из строя, и, когда это происходит, требуется некоторое время на восстановление. Тот факт, что это время не равно нулю, означает, что базы данных с несколькими узлами, которые всегда должны быть согласованы друг с другом, иногда могут оказываться недоступными. Например, транзакции должны блокироваться, пока другие хранилища данных не смогут их выполнить.

### Балансировка нагрузки на уровне клиента и etcd
etcd должен иметь доступ ко всем членам кластера etcd, чтобы рас- пределять нагрузку между узлами:
- клиент etcd пытается установить соединения со всеми конечными точками, и первое ответившее соединение остается открытым;
- etcd поддерживает TCP-соединение с конечной точкой, выбранной клиентом;
- в случае сбоев может произойти переход к другим конечным точкам.

### Ограничения по размеру: о чем (не) следует беспокоиться
Сама база данных etcd накладывает некоторые ограничения по размеру и не предназначена для хранения терабайтов и петабайтов данных. В рабочем кластере Kubernetes чрезвычайно грубой, но надежной начальной оценкой объема памяти и диска, приходящегося на одно пространство имен, является 10 Кбайт. Это означает, что кластер с 1000 пространств имен, вероятно, будет достаточно хорошо работать с 1 Гбайт ОЗУ. Однако, поскольку etcd использует большой объем памяти для управления механизмом наблюдения, что является доминирующим фактором в его требованиях к ОЗУ, эта минимальная оценка теряет смысл. Отдельные пары ключ/значение обычно занимают меньше 1,5 Мбайт. Это значение можно настроить параметром max-request-bytes. Kubernetes явно не запрещает хранить произвольно большие объекты (например, ConfigMap с объемом данных > 2 Мбайт), но, в зависимости от настроек etcd, это может быть или не быть возможным. Имейте в виду, это очень важно, особенно учитывая, что каждый член кластера etcd хранит полную копию всех данных, поэтому распределение данных по сегментам невозможно.

### Размеры ограничений
etcd имеет некоторые благонамеренные ограничения:
- запросы большего размера будут обрабатываться, но могут вы- звать задержку обработки других запросов;
- по умолчанию максимальный размер любого запроса равен 1,5 Мбайт;
- это ограничение настраивается с помощью флага --max-request- bytes сервера etcd;
- общий размер базы данных ограничен:
  - максимальный размер хранилища по умолчанию равен 2 Гбайт, при этом желательно, чтобы размер базы данных etcd не превышал 8 Гбайт;
  - максимальный размер полезной нагрузки в etcd по умолчанию равен 1,5 Мбайт. Объем текста, описывающего модуль Pod, обычно составляет менее одного килобайта, поэтому, если вы не создаете CRD или другие объекты, размер которых в тысячу раз больше размера обычного YAML-файла в Kubernetes, это ограничение не должно повлиять на вас.
 
### Шифрование хранимых данных в etcd
В etcd хранится много информации, компрометация которой может привести к катастрофе в масштабе предприятия. etcd является самой ценной добычей для любого хакера в кластере Kubernetes. Давайте посмотрим, как Kubernetes API решает проблему шифрования:
- сам сервер Kubernetes API поддерживает шифрование; он принимает аргумент --encryption-provider-config, описывающий типы объектов API, которые должны шифроваться
- значение --encryption-provider-config представляет файл YAML, в котором перечисляются типы объектов API (например, Secret) и провайдеры шифрования. Их три: AES-GCM, AES-CBC и Secret Box;
- ранее перечисленные провайдеры применяются для расшифровывания в порядке убывания по алфавиту, а для шифрования используется первый элемент в списке провайдеров.

Возможно, в будущем технология шифрования будет внедрена в саму базу данных etcd, но на данный момент хранение данных на зашифрованном диске и шифрование непосредственно на стороне клиента – лучший способ защитить свои данные.

### Производительность и отказоустойчивость etcd в глобальном масштабе
Под глобальным развертыванием etcd подразумевается возможность запускать etcd в режиме георепликации. По умолчанию etcd предназначена для поддержки локального, а не глобального раз- вертывания, а это означает, что вам придется определить дополни- тельные настройки etcd для развертывания в глобальном масштабе. Таким образом, если у вас узлы etcd распределены по разным сетям, вам придется настроить несколько параметров, чтобы:
- смягчить процедуру выбора лидера;
- контрольные сообщения для мониторинга работоспособности посылались реже.

Согласно документации etcd, «разумное время прохождения запроса/ответа в пределах континентальной части США составляет 130 мс, а между США и Японией – около 350–400 мс». Основываясь на этих данных, следует для начала увеличить интервал отправки контрольных сообщений, а также тайм-аут выбора лидера. При коротком интервале тратятся дополнительные такты процессора на отправку по сети избыточных данных. При слишком длинном интервале повышается вероятность, что может потребоваться избрание нового лидера. Ниже приводится пример, как настроить параметры, управляющие выборами лидера, для геораспре- деленного развертывания etcd:
```
$ etcd --heartbeat-interval=100 --election-timeout=500
```
