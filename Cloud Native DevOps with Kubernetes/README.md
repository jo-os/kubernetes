# Kubernetes

**Дирижирование оркестром контейнеров**

Оркестратор контейнеров: программный компонент, предназначенный для объединения множества разных серверов в кластер Кластер — это своего рода унифицированная вычислительная подложка, которая, с точки зрения пользователя, выглядит как компьютер очень высокой мощности, способный работать с контейнерами.

Термин «оркестратор контейнеров» обычно относится к одному сервису, который занимается планированием и оркестрацией кластера, а также управлением им.
- Оркестрация означает координацию и выстраивание цепочки из разных действий для достижения общей цели. 
- Планирование же означает управление доступными ресурсами и направление рабочих заданий туда, где они могут быть выполнены наиболее эффективно.
- Управление кластером является объединение нескольких физических или виртуальных серверов в унифицированную, надежную, отказоустойчивую и довольно цельную группу

В 2014 году компания Google основала открытый проект под названием Kubernetes (от греческого слова — «рулевой, пилот»).

**Что делает платформу Kubernetes такой ценной**

Kubernetes занимается тем же, чем и самые лучшие системные администраторы: автоматизацией, централизованным ведением журнала, мониторингом, обеспечивает отказоустойчивость. Балансировка нагрузки и автомасштабирование, встроены в ядро Kubernetes, Kubernetes отличается обширной и постоянно растущей экосистемой.

**Kubernetes облегчает развертывание**
- Благодаря тому что Kubernetes выполняет плавающие обновления по умолчанию, приобрели популярность развертывания с нулевым временем простоя
- Kubernetes поддерживает автомасштабирование
- Избыточность и отказоустойчивость встроены в Kubernetes

**Kubernetes не решает все проблемы**
- Kubernetes просто не очень хорошо подходит для некоторых структур (таких как базы данных).
- некоторые задачи не требуют Kubernetes и могут работать на платформах, которые иногда называют бессерверными (функции как сервис (functions as a service, FaaS))

**Облачная ориентированность**

Термин «облачно-ориентированный» (cloud native) становится все более популярным сокращением, которое описывает современные приложения и сервисы, пользующиеся преимуществами облаков, контейнеров и оркестрации

Характеристики облачно-ориентированных систем:
- Автоматизируемость
- Универсальность и гибкость
- Устойчивость и масштабируемость
- Динамичность
- Наблюдаемость
- Распределенность

# Первые шаги с Kubernetes

Запуск приложения
```
kubectl run demo --image=cloudnatived/demo:hello --port=9999 --labels app=demo deployment.apps "demo" created
```
Перенаправление портов
```
kubectl port-forward deploy/demo 9999:8888
```
Просмотр pods
```
kubectl get pods --selector app=demo
```
# Размещение Kubernetes

**Архитектура кластера**

«Мозг» кластера называется **управляющим уровнем**. Управляющий уровень на самом деле состоит из нескольких компонентов.
- kube-apiserver — это внешний сервер для управляющего уровня, который обрабатывает API-запросы
- etcd — база данных, в которой Kubernetes хранит всю информацию о существующих узлах, ресурсах кластера
- kube-scheduler определяет, где будут запущены свежесозданные pod-оболочки.
- kube-controller-manager отвечает за запуск контроллеров ресурсов, таких как развертывания.
- cloud-controller-manager взаимодействует с облачным провайдером (в облачных кластерах), управляя такими ресурсами, как балансировщики нагрузки и дисковые тома.

Участники кластера, которые **выполняют компоненты управляющего уровня**, называются **ведущими узлами**.

Участники кластера, которые **выполняют пользовательские рабочие задания**, называются **рабочими узлами**.

Каждый рабочий узел в кластере Kubernetes отвечает за следующие компоненты.
- kubelet отвечает за управление средой выполнения контейнера, в которой запускаются рабочие задания, запланированные для узла, а также за мониторинг их состояния.
- kube-proxy занимается сетевой магией, которая распределяет запросы между pod-оболочками на разных узлах, а также между pod-оболочками и Интернетом.
- Среда выполнения контейнеров запускает и останавливает контейнеры, а также отвечает за их взаимодействие.

**Высокая доступность**

У правильно сконфигурированного управляющего уровня Kubernetes есть несколько ведущих узлов, что делает его высокодоступным. База данных etcd реплицируется между несколькими узлами и может пережить отказ отдельных копий при условии наличия кворума из более чем половины реплик etcd.

**Отказ управляющего уровня**

Если вы остановите все ведущие узлы в своем кластере, pod-оболочки на рабочих узлах продолжат функционировать — по крайней мере какое-то время. Однако вы не сможете развертывать новые контейнеры или менять какие-либо ресурсы Kubernetes, а такие контроллеры, как развертывания, перестанут действовать.

Вам необходимо запастись достаточным количеством ведущих узлов, чтобы кластер мог поддерживать кворум, даже если какой-то из узлов откажет. Для промышленных кластеров реалистичным минимумом является три узла.

**Отказ рабочего узла**

Отказ любого рабочего узла, не влечет за собой никаких существенных последствий: Kubernetes обнаружит сбой и перераспределит pod-оболочки этого узла. Главное, чтобы работал управляющий уровень.

**Установщики Kubernetes**

**kops** (kubernetes.io/docs/setup/production-environment/tools/kops) — это утилита командной строки для автоматического выделения кластеров, служит инструментом, предназначенным специально для AWS.

**Kubespray** фокусируется на установке Kubernetes на существующие компьютеры, особенно на локальные и физические серверы. Это инструмент для простого развертывания промышленных кластеров. Он предлагает множество возможностей, включая высокую доступность и поддержку нескольких платформ.

**TK8** (github.com/kubernauts/tk8) — утилита командной строки для создания кластеров Kubernetes, которая использует как Terraform (для создания облачных серверов), так и Kubespray (для установки на них Kubernetes). Она написана на Go и поддерживает установку на AWS, OpenStack и «чистые серверы». 

**Kubernetes: трудный путь**

**kubeadm** - Утилита kubeadm (kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm) входит в состав дистрибутива Kubernetes и должна помочь вам устанавливать и обслуживать кластеры в соответствии с лучшими рекомендациями. Многие инструменты и сервисы, используют kubeadm для выполнения административных операций,

**Tarmak** (blog.jetstack.io/blog/introducing-tarmak) — это инструмент для управления жизненным циклом кластеров Kubernetes, который должен упростить и сделать более надежными модификацию и обновления кластерных узлов. Tarmak использует Terraform для выделения узлов кластера и Puppet для управления конфигурацией на самих узлах.

**Rancher Kubernetes Engine** - RKE (github.com/rancher/rke) стремится быть простым и быстрым установщиком Kubernetes.

# Работа с объектами Kubernetes
**Надзор и планирование**

Для каждой программы, за которой нужно следить, Kubernetes создает соответствующий объект Deployment, записывающий некоторую связанную с ней информацию: имя образа контейнера, количество реплик (копий), которые вы хотите выполнять, и любые другие параметры, необходимые для запуска контейнера. В связке с ресурсом Deployment работает некий объект Kubernetes под названием контроллер. Контроллеры отслеживают ресурсы, за которые отвечают, убеждаясь в том, что те присутствуют и выполняются, а если заданное развертывание по какой-либо причине не имеет достаточного количества реплик, дополнительно их создают. На самом деле развертывание не управляет репликами напрямую: вместо этого оно автоматически создает сопутствующий объект под названием ReplicaSet, который сам этим занимается.

**Перезапуск контейнеров**

Большинство приложений Kubernetes должны работать долго и надежно, поэтому
подобное поведение имеет смысл: контейнер может остановиться по разным причинам, и в большинстве случаев реакцией живого оператора будет перезапуск — именно так по умолчанию и ведет себя Kubernetes. Задача развертывания состоит в том, чтобы отслеживать связанные с ним контейнеры и постоянно поддерживать определенное их количество.

**Обращение к развертываниям**

Просмотреть все активные развертывания в вашем текущем пространстве:
```
kubectl get deployments
kubectl describe deployments/demo - подробной информации о конкретном развертывании
```
**Pod-оболочки**

**Pod** — это объект Kubernetes, который представляет группу из одного или нескольких контейнеров

**Объекты ReplicaSet**

**ReplicaSet отвечает** за группу идентичных pod-оболочек (или реплик). Развертывания, в свою очередь, управляют объектами ReplicaSet и контролируют поведение реплик в момент их обновления — например, при выкатывании новой версии вашего приложения.

**Поддержание желаемого состояния**

Контроллеры Kubernetes непрерывно сравнивают желаемое состояние, указанное каждым ресурсом, с реальным состоянием кластера и вносят необходимые корректировки. Этот процесс называют циклом согласования, поскольку он все время повторяется в попытке согласовать текущее состояние с желаемым. Создав развертывание, вы сообщили Kubernetes о том, что pod-оболочка должна работать **всегда**.

**Планировщик Kubernetes**

Развертывание создаст pod-оболочки, а Kubernetes при необходимости их запустит. За эту часть процесса отвечает компонент Kubernetes под названием «планировщик». Задача планировщика — следить за этой очередью, взять из нее следующую запланированную pod-оболочку и найти узел, на котором ее можно запустить. При выборе подходящего узла планировщик будет исходить из нескольких разных критериев, включая ресурсы, запрашиваемые pod-оболочкой. Как только выполнение pod-оболочки было запланировано, утилита kubelet, работающая на соответствующем узле, подхватывает ее и производит запуск ее контейнера.

**Ресурсы являются данными**

Все ресурсы Kubernetes, такие как развертывания и pod-оболочки, представлены записями во внутренней базе данных. Цикл согласования следит за любыми изменениями в записях и предпринимает соответствующие действия. На самом деле команда kubectl run лишь добавляет в базу данных новую запись о развертывании, а система делает все остальное. Но для взаимодействия с Kubernetes вовсе не обязательно использовать команду kubectl run — вы можете создавать манифесты ресурсов.

**Манифесты развертываний**

deployment.yaml
```yml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: demo
  labels:
    app: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo
template:
  metadata:
    labels:
      app: demo
spec:
  containers:
    - name: demo
    image: cloudnatived/demo:hello
    ports:
    - containerPort: 8888
```
**Использование команды kubectl apply**

Передавать YAML-манифесты вашему кластеру, используя команду kubectl apply.
```
kubectl apply -f k8s/deployment.yaml
```
Для подключения к pod-оболочке с помощью веб-браузера надо создать сервис — ресурс Kubernetes, который позволяет подключаться к развернутым pod-оболочкам.

**Ресурсы типа «сервис»**

Ресурс типа «сервис» предоставляет один несменяемый IP-адрес или такое же доменное имя, которые автоматически перена- правляют на любую подходящую pod-оболочку. Сервис можно считать веб-прокси или балансировщиком нагрузки, который направляет запросы к группе внутренних pod-оболочек.

service.yaml
```yaml
apiVersion: v1
kind: Service
metadata:
  name: demo
  labels:
    app: demo
spec:
ports:
  - port: 9999
  protocol: TCP
    targetPort: 8888
  selector:
    app: demo
  type: ClusterIP
```
Параметр selector говорит сервису, как перенаправлять запросы к конкретным pod-оболочкам. Сервис предоставляет запросам единую точку входа в эти pod-оболочки.
```
kubectl apply -f k8s/service.yaml
kubectl port-forward service/demo 9999
```
**Обращение к кластеру с помощью kubectl**

С помощью kubectl get можно запрашивать pod-оболочки и развертывания. Эту команду также можно использовать для просмотра узлов вашего кластера:
```
kubectl get nodes
```
Вывести ресурсы всех типов, выполните команду - kubectl get all

Получить исчерпывающую информацию об отдельной pod-оболочке (или любом другом ресурсе), выполните kubectl describe:
```
kubectl describe pod/demo-dev-6c96484c48-69vss
```
## Helm: диспетчер пакетов для Kubernetes

Один из популярных диспетчеров пакетов для Kubernetes называется Helm. helm для установки и конфигурации приложений
(собственных или чужих) и создавать пакеты, которые полностью описывают все необходимые для работы приложения ресурсы, включая их зависимости и настройки. Пакеты в Helm называются чартами (charts).

Установка Helm - Следуйте инструкциям по установке Helm (helm.sh/docs/using_helm/#installing-helm) для вашей операционной системы.

Установка чарта Helm
```
helm install --name demo ./k8s/demo
```
Helm создал ресурс Deployment и Service, команда helm install создает объект Kubernetes под названием «выпуск» (release).

**Чарты, репозитории и выпуски**
- Чарт — пакет Helm с определениями всех ресурсов, необходимых для выполнения приложения в Kubernetes.
- Репозиторий — место, где можно делиться своими чартами и загружать чужие.
- Выпуск — конкретный экземпляр чарта, запущенный в кластере Kubernetes

У каждого выпуска есть уникальное имя, которое можно указать в команде helm install с помощью флага **-name**

**Вывод списка выпусков Helm**

Проверить, какие выпуски запущены на данный момент:
```
helm list
```
Вывести состояние конкретного выпуска, передайте его имя команде **helm status**.

## Управление ресурсами

**Запросы ресурсов**

Запрос ресурса в Kubernetes определяет минимальный объем этого ресурса, который необходим для работы pod-оболочки. Например, запрос 100m (100 миллипроцессоров) и 250Mi (250 МиБ памяти) означает, что pod-оболочка не может быть назначена узлу с меньшим количеством доступных ресурсов. Если в кластере нет ни одного узла с достаточной мощностью, pod-оболочка будет оставаться в состоянии pending, пока такой узел не появится.
```
resources:
  requests:
    memory: "10Mi"
    cpu: "100m"
```
**Лимит на ресурс** определяет максимальное количество этого ресурса, которое pod-оболочке позволено использовать. Если pod-оболочка попытается занять больше выделенного ей лимита на процессор, производительность будет снижена. Pod-оболочка, пытающаяся использовать больше, чем указано в лимите на память, будет принудительно остановлена, и ее выполнение, если это возможно, снова окажется запланировано.
```
resources:
  limits:
    memory: "20Mi"
    cpu: "250m"
```
Kubernetes допускает **отрицательный баланс ресурсов**, когда сумма всех лимитов у контейнеров одного узла превышает общее количество ресурсов, которыми узел обладает. Общее количество потребляемых ресурсов начнет приближаться к максимальной мощности узла и Kubernetes начнет удалять контейнеры более агрессивно. В условиях нехватки ресурсов могут быть остановлены даже те контейнеры, которые исчерпали запрошенные ресурсы, а не лимиты. При прочих равных, когда возникает необходимость в удалении pod-оболочек, Kubernetes начинает с тех, которые больше всего превысили запрошенные ресурсы.

**Управление жизненным циклом контейнера**

Контейнеризированные приложения довольно часто входят в ступор: их процесс все еще выполняется, но больше не обслуживает никакие запросы. Kubernetes нужен какой-то способ для обнаружения подобных ситуаций, чтобы решить проблему за счет перезапуска контейнера.

**Проверки работоспособности**

Если приложение отвечает с помощью HTTP-кода вида 2xx или 3xx, Kubernetes считает его активным.
Для контейнера с HTTP-сервером определение проверки работоспособности обычно выглядит примерно так:
```
livenessProbe:
  httpGet:
    path: /healthz
    port: 8888
  initialDelaySeconds: 3
  periodSeconds: 3
```
**Задержка и частота проверки**

Поле initialDelaySeconds позволяет указать время ожидания перед первой проверкой работоспособности, чтобы избежать убийственного цикла (loop of the death). Поле periodSeconds определяет, как часто следует выполнять проверку работоспособности: в данном примере это делается каждые три секунды.

**Проверки готовности**

Проверки готовности и работоспособности имеют общее происхождение, но разную семантику. Если ваше приложение не начинает прослушивать HTTP, пока не будет готово к обработке запросов, проверки готовности и работоспособности могут выглядеть одинаково:
```
readinessProbe:
  httpGet:
    path: /healthz
    port: 8888
  initialDelaySeconds: 3
  periodSeconds: 3
```
Контейнер, не прошедший проверку готовности, удаляется из любых сервисов, совпавших с заданной pod-оболочкой. Это похоже на удаление неисправного узла из пула балансировщика нагрузки: к pod-оболочке не будет направляться трафик, пока она опять не начнет успешно проходить проверку готовности.

**Проверки готовности на основе файла** - Проверка готовности такого рода может быть полезной. Например, если нужно временно выключить контейнер с целью отладки, к нему можно подключиться и удалить файл /tmp/healthy.

**minReadySeconds** - для контейнера можно установить поле minReadySeconds. Контейнеры или pod-оболочки не будут считаться готовыми, пока с момента успешной проверки готовности не пройдет minReadySeconds секунд (по умолчанию 0).

**PodDisruptionBudget** - Иногда Kubernetes нужно остановить ваши pod-оболочки, даже если они в полном порядке и готовы к работе (этот процесс называется выселением). Возможно, узел, на котором они размещены, очищается перед обновлением и pod-оболочки необходимо переместить на другой узел. Ресурс PodDisruptionBudget позволяет указать, сколько pod-оболочек заданного приложения допустимо к потере в любой момент времени.

**minAvailable** - для определения минимального количества рабочих pod.

**maxUnavailable** - это относится лишь к так называемому добровольному выселению, то есть инициированному системой Kubernetes. Если, к примеру, узел испытывает аппаратные неполадки или удаляется, его pod-оболочки выселяются принудительно, даже если это нарушает параметры PodDisruptionBudget.

**Использование пространств имен** 

Механизмом контроля за потреблением ресурсов в вашем кластере является использование пространств имен. Пространство имен в Kubernetes предоставляет способ разделения кластера на отдельные части.

Чтобы увидеть, какие пространства имен существуют в вашем кластере:
```
kubectl get namespaces
```
**Работа с пространствами имен**
Ваша команда будет использовать то пространство имен, которое вы укажете с помощью флага --namespace (сокращенно -n).
```
kubectl get pods --namespace prod
```
В Kubernetes пространства имен можно создавать с помощью ресурса Namespace:
```
apiVersion: v1
kind: Namespace
metadata:
  name: demo
```
Пространство имен можно использовать в качестве временного виртуального кластера, который, став ненужным, удаляется.

**Квоты на ресурсы**

Вы можете ограничить потребление процессорного времени, памяти и для заданных пространств имен
```
apiVersion: v1
kind: ResourceQuota
metadata:
  name: demo-resourcequota
spec:
  hard:
    pods: "100"
```
Применение этого манифеста к конкретному пространству имен (например, к demo) устанавливает жесткий лимит на запуск не более чем 100 pod-оболочек в этом пространстве.
```
kubectl create namespace demo
kubectl apply --namespace demo -f k8s/resourcequota.yaml
```
Проверить, активирован ли ресурс ResourceQuotas в конкретном пространстве имен:
```
kubectl get resourcequotas -n demo
```
**Оптимизация стоимости кластера**

**Оптимизация узлов**

Более крупные узлы могут быть более рентабельными, поскольку доля их ресурсов, доступных для ваших рабочих заданий, оказывается выше. Но, с другой стороны, потеря отдельного узла будет иметь более заметные последствия для доступной мощности вашего кластера. У мелких узлов также более высокая доля заблокированных ресурсов — порций памяти и процессорного времени, которые не используются и которые слишком малы для того, чтобы pod-оболочка могла их занять. Узлы лучше делать достаточно большими для выполнения как минимум пяти ваших типичных pod-оболочек. По умолчанию в Kubernetes действует лимит 110 pod-оболочек на узел, можете поднять его с помощью параметра --max-pods утилиты kubelet. Стремитесь к показателю 10–100 pod-оболочек на узел.

**Оптимизация хранилища** - Консоль вашего облачного провайдера или сервиса Kubernetes обычно умеет по-
казывать количество IOPS, используемое вашими узлами.

**Избавление от неиспользуемых ресурсов** - По мере увеличения вашего кластера Kubernetes вы будете обнаруживать множество неиспользуемых или потерянных ресурсов.

**Использование метаданных владельца** - Чтобы минимизировать неиспользуемые ресурсы, на уровне организации стоит
ввести правило, согласно которому каждый ресурс должен иметь информацию о своем владельце. Для этого можно использовать аннотации Kubernetes.
```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: my-brilliant-app
  annotations:
    example.com/owner: "Customer Apps Team"   - информацию о владельце
```
Метаданные владельца должны указывать на человека или команду, с которыми можно связаться при возникновении вопросов относительно конкретного ресурса.

**Поиск малоиспользуемых ресурсов**

Каждая pod-оболочка должна предоставлять в виде показателя количество полученных запросов. Вы также можете проверить показатели загруженности процессора и памяти у каждой pod-оболочки в вашей веб-консоли, чтобы найти наименее загруженные.

**Удаление завершенных запланированных заданий**

Запланированные задания в Kubernetes — это pod-оболочки, которые выполняются единожды и не перезапускаюся после завершения работы. Однако в базе данных Kubernetes остаются Job-объекты, и если у вас наберется значительное количество завершенных заданий, это может повлиять на производительность API. Для удаления таких объектов предусмотрен
удобный инструмент **kube-job-cleaner**

**Использование принадлежности к узлам для управления планированием**

В Kubernetes можно использовать концепцию принадлежности узлов (node affinities), чтобы pod-оболочки, отказ которых недопустим, не размещались на прерываемых узлах. Чтобы планировщик Kubernetes никогда не разместил pod-оболочку на одном из таких узлов, добавьте следующий фрагмент в ее спецификацию.
```
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
          - key: cloud.google.com/gke-preemptible
            operator: DoesNotExist
```
Принадлежность requiredDuringScheduling... является обязательной: pod-оболочка, в которой она указана, никогда не будет запущена на узле, соответствующем выражению селектора (это называется жесткой принадлежностью

Мягкая принадлежность с противоположным смыслом
```
preferredDuringSchedulingIgnoredDuringExecution:
- preference:
  matchExpressions:
  - key: cloud.google.com/gke-preemptible
    operator: Exists
weight: 100
```
Означает следующее: «Если можешь — пожалуйста, размещай данную pod-оболочку на прерываемом узле; если нет — ничего страшного.

**Балансировка вашей рабочей нагрузки**

Суть проблемы в том, что планировщик перемещает pod-оболочки с одного узла на другой, только если они по какой-либо причине перезапускаются. К тому же задача планировщика — распределить рабочую нагрузку равномерно между узлами — иногда конфликтует с поддержанием высокой доступности отдельных сервисов. Одним из решений является использование инструмента под названием **Descheduler**. Для Descheduler можно задавать различные стратегии и линии поведения. Например, одна из его политик ищет малонагруженные узлы и переносит на них pod-оболочки с других узлов. Другая политика ищет pod-оболочки, две и более реплики которых размещены на одном узле, и выселяет их.

## Работа с кластерами

**Максимальный кластер**

1.12 официально поддерживает кластеры до 5000 узлов. В документации к Kubernetes говорится о том, что поддерживаемая конфигурация кластера не должна превышать 5000 узлов, 150 000 pod-оболочек, 300 000 контейнеров, а на одном узле не должно быть больше 100 pod-оболочек.

**Федеративные кластеры** - Федерация позволяет синхронизировать два и более кластера, запуская на них идентичные задания.

**Типы облачных серверов** - Ведущие узлы в небольших кластерах (до пяти узлов) должны иметь как минимум один виртуальный процессор (vCPU) и 3–4 ГиБ памяти. 

**Обратное масштабирование**

В принципе, у Kubernetes нет никаких проблем и с обратным масштабированием. Вы можете приказать системе очистить узлы, подлежащие удалению, и она постепенно выключит pod-оболочки на этих узлах или куда-нибудь их переместит.

Большинство инструментов для управления кластером выполняют очищение узлов автоматически, но также можно использовать команду **kubectl drain**, чтобы сделать это вручную. Если у кластера достаточно свободной мощности для перераспределения «обреченных» pod-оболочек, вы сможете удалить их сразу после успешного очищения узлов. Очистка позволяет pod-оболочкам корректно завершить свою работу, убрать за собой, а также, если необходимо, сохранить какое-либо состояние.

**Автомасштабирование** - автомасштабирование — автоматическое увеличение или уменьшение количества серверов в группе на основе какого-то показателя или графика.

**Проверка на соответствие** - платформа Kubernetes содержит набор тестов, позволяющий подтвердить, что кластер соответствует спецификации, то есть удовлетворяет основному набору требований для заданной версии Kubernetes. Соответствие стандартам — это лишь начальное условие, которое должно выполняться любым промышленным кластером. Однако в Kubernetes существует много распространенных проблем с конфигурацией и рабочей нагрузкой:
- использование слишком больших образов контейнеров может привести к потере значительного количества времени и ресурсов кластера;
- развертывания с единственной репликой pod-оболочки не являются высокодоступными;
- выполнение процессов в контейнерах от имени администратора представляет потенциальный риск для безопасности

Стандартным инструментом для выполнения проверок является система **Sonobuoy** от Heptio

**K8Guard** - Утилита K8Guard, разработанная компанией Target, умеет искать распространенные проблемы в кластерах

**Copper** (copper.sh) — это инструмент для проверки манифестов Kubernetes перед их развертыванием; он отмечает распространенные проблемы или применяет отдельные политики.

**kube-bench** — это инструмент для аудита кластеров Kubernetes с помощью набора тестов производительности, разработанных Центром интернет-безопасности.

**Ведение журнала аудита для Kubernetes** - Если включить ведение этого журнала, все запросы к API кластера будут записываться с пометкой о том, когда и кто их сделал.

**Хаотическое тестирование** - Такой вид автоматического, произвольного вмешательства в промышленные сервисы иногда называют тестом обезьяны в честь инструмента Chaos Monkey («хаотическая обезьяна»), разработанного компанией Netflix для тестирования своей инфраструктуры.

**chaoskube** (github.com/linki/chaoskube) случайным образом удаляет pod-оболочки вашего кластера.

**kube-monkey** (github.com/asobti/kube-monkey) запускается в заранее установленное время и формирует график развертываний, которые будут объектами тестирования на протяжении всего дня.

**PowerfulSeal** (github.com/bloomberg/powerfulseal) — это открытый инструмент для хаотического тестирования Kubernetes, который работает в двух режимах: интерактивном и автономном.

## Продвинутые инструменты для работы с Kubernetes
